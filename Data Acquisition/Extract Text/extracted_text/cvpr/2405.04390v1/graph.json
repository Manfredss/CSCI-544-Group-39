{
  "summary": "The figure illustrates the Memory State-Space Model, distinguishing deterministic and stochastic states with solid an...",
  "caption": "Graphical model of Memory State-Space Model. Deterministic states are denoted by squares, while stochastic states are represented by circles. The observed states are highlighted in grey for clarity. Solid lines represent the generative model, while dotted lines depict variational inference.",
  "label": "fig:graph",
  "extracted_context": "### Supplementary Material: Pre-training Objective and Methodology\n\n#### **1. World Model Components**  \nDriveWorld is built upon a **Memory State-Space Model (MSSM)** that jointly learns spatio-temporal representations for 4D scene understanding. The model comprises five core components:  \n- **BEV Representation Model**: Encodes multi-camera observations $ o_t $ into a bird’s-eye view (BEV) representation $ b_t \\sim q_\\phi(b_t \\mid o_t) $.  \n- **Stochastic State Model**: Captures latent dynamics $ s_t \\sim q_\\phi(s_t \\mid h_{t}, a_{t-1}, o_t) $, where $ h_t $ is a history context, $ a_{t-1} $ is the previous action, and $ o_t $ is the current observation.  \n- **Dynamic Transition Model**: Predicts future states $ s_t \\sim p_\\theta(s_t \\mid h_t, \\hat{a}_{t-1}) $ based on historical context $ h_t $ and previous actions $ \\hat{a}_{t-1} $.  \n- **Static Propagation Model**: Transmits spatial information across time $ \\hat{b} \\sim p_\\theta(\\hat{b} \\mid b^{\\prime}) $, ensuring spatial coherence in the BEV representation.  \n- **Action Decoder**: Generates predicted actions $ \\hat{a}_t \\sim p_\\theta(\\hat{a}_t \\mid h_t, s_t) $ from latent states and history.  \n- **3D Occupancy Decoder**: Reconstructs 3D occupancy $ \\hat{y}_t \\sim p_\\theta(\\hat{y}_t \\mid h_t, s_t, \\hat{b}) $, enabling explicit spatial-temporal scene understanding.  \n\nThese components form a unified framework for learning compact, interpretable representations of 4D scenes.  \n\n---\n\n#### **2. Joint Probability Distribution and Variational Inference**  \nThe joint distribution of DriveWorld is defined as:  \n$$\np(h_{1:T}, s_{1:T}, y_{1:T+L}, a_{1:T+L}) = \\prod_{t=1}^{T} p(h_t, s_t \\mid h_{t-1}, s_{t-1}, a_{t-1}) \\cdot p(y_t, a_t \\mid h_t, s_t, \\hat{b}) \\cdot \\prod_{k=1}^{L} p(h_k, s_k \\mid h_T, s_T, a_{k-1}) \\cdot p(y_k, a_k \\mid h_T, s_T, \\hat{b}),\n$$  \nwhere $ h_t $ is a deterministic history context, and $ s_t $ represents stochastic latent states. The model factorizes the joint distribution into temporal and spatial sub-models, enabling scalable learning of long-horizon dynamics.  \n\nTo maximize the marginal likelihood of observed actions $ a_{1:T+L} $ and future predictions $ y_{1:T+L} $, DriveWorld employs **deep variational inference**. A variational distribution $ q_{H,S} $ is introduced:  \n$$\nq_{H,S} \\triangleq q(h_{1:T}, s_{1:T} \\mid o_{1:T}, y_{1:T+L}, a_{1:T+L}) = \\prod_{t=1}^{T} q(h_t \\mid h_{t-1}, s_{t-1}) \\cdot q(s_t \\mid o_{\\le t}, a_{< t}),\n$$  \nwhich is parameterized by a neural network with weights $ \\phi $. This allows the model to approximate the true posterior distribution $ p(h_{1:T}, s_{1:T} \\mid o_{1:T}, y_{1:T+L}, a_{1:T+L}) $, enabling efficient learning of the underlying dynamics.  \n\n---\n\n#### **3. Graphical Model and Key Modules**  \nThe **Memory State-Space Model** (MSSM) is visualized in **Figure 1** (see caption below). Deterministic states (e.g., history context $ h_t $) are represented by squares, while stochastic states (e.g., latent states $ s_t $) are denoted by circles. Observed states (e.g., BEV representations $ b_t $, actions $ a_t $, and 3D occupancy $ y_t $) are highlighted in grey.  \n\n- **Dynamic Memory Bank**: Learns temporal-aware representations by aggregating historical context and action sequences.  \n- **Static Scene Propagation**: Captures spatial relationships through explicit 3D occupancy decoding, ensuring spatial coherence across time.  \n- **Task Prompt**: Guides the model to adaptively acquire task-specific representations for downstream tasks (e.g., detection, prediction, planning).  \n\nThis modular design enables DriveWorld to simultaneously learn spatio-temporal dynamics and task-specific knowledge, achieving robust performance across autonomous driving tasks.  \n\n---\n\n#### **4. Key Contributions and Implications**  \n- **4D Scene Reconstruction**: By integrating 3D occupancy decoding, DriveWorld provides a comprehensive representation of the 4D world, enabling accurate prediction of future states and spatial relationships.  \n- **End-to-End Learning**: The unified framework eliminates the need for manual feature engineering, allowing the model to learn from raw multi-camera inputs and actions.  \n- **Generalization to New Tasks**: The Task Prompt mechanism enables the model to adapt to diverse tasks (e.g., object detection, motion forecasting, planning) with minimal fine-tuning.  \n\nThese innovations position DriveWorld as a foundational model for vision-centric autonomous driving, bridging the gap between perception and decision-making in complex, dynamic environments.  \n\n---\n\n**Figure Caption:**  \n**Figure 1:** Graphical model of the Memory State-Space Model. Deterministic states (e.g., history context $ h_t $) are denoted by squares, while stochastic states (e.g., latent states $ s_t $) are represented by circles. Observed states (e.g., BEV representations $ b_t $, actions $ a_t $, and 3D occupancy $ y_t $) are highlighted in grey. Solid lines represent the generative model, while dotted lines depict variational inference.  \n\n**Figure Label:** fig:graph  \n\n--- \n\nThis supplementary material provides a rigorous foundation for understanding DriveWorld’s pre-training objective, architectural design, and its implications for autonomous driving.",
  "extraction_method": "LLM-based"
}