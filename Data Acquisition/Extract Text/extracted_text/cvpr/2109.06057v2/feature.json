{
  "summary": "This figure illustrates aligning mid-level attributes between source and target domains in unsupervised domain adapta...",
  "caption": "Illustration of feature alignment approach for unsupervised domain adaptation. Mid-level attribute features are aligned between source and target domains in a joint learning pipeline. $_{attr}$ represents the attribute alignment loss between the source attributes and the target attributes.",
  "label": "fig:fa",
  "extracted_context": "Hereâ€™s the extracted context from the provided text, organized by key sections and relevant paragraphs:\n\n---\n\n### **1. Unsupervised Clustering & Feature Learning**  \n**Key Concepts:**  \n- **Clustering Methods:** Unsupervised person Re-ID relies on clustering techniques to group similar samples. Each sample is assigned a label based on its nearest neighbors.  \n  - Example:  \n    > \"In unsupervised clustering, each sample is allocated a label the same as its nearest neighbours. Utilizing a memory bank (e.g. lookup table), a sample's probability sharing the same identity with its neighbours can be calculated...\"  \n- **Hard-Batch Triplet Loss:** HCT introduces a hard-batch triplet loss to reduce outliers' impact in clustering.  \n  - Equation:  \n    $$ L_{htri}=\\sum_{i=1}^P \\sum_{a=1}^K \\left[m+\\max_{p=1...K}D(x_a^i,x_p^j)-\\min_{\\substack{j=1...P\\\\ n=1...N}}D(x_a^i,x_n^j) \\right] $$  \n  - Purpose: Ensures anchors are closer to positive samples than negative ones.  \n\n**Video-Based Re-ID:**  \n- **Temporal Consistency:** JVTC uses temporal information between cameras to improve feature learning.  \n  - Example:  \n    > \"When a person appears in camera $A$ at time $t_a$, the person would be likely recorded by camera $B$ at time $t_b$, and less likely by camera $C$ at time $t_c$.\"  \n- **Tracklets:** Tracklets (multi-frame sequences) are used to learn invariant features.  \n  - Example:  \n    > \"Tracklets are useful in learning invariant feature representation simply because that same tracklet frames with various person appearances represent the same person.\"  \n\n---\n\n### **2. Camera-Aware Invariance Learning**  \n**Key Methods:**  \n- **SCCT:** Adjusts color statistics across cameras using linear transformations.  \n- **Cross-Domain Mixup:** Interpolates data manifolds to simulate cross-camera scenarios.  \n- **MetaCam (DSCE):** Simulates cross-camera Re-ID during training by splitting data into meta-train/meta-test subsets.  \n- **UGA:** Reduces cross-camera noise by separating intra-camera and inter-camera learning stages.  \n- **UCDA:** Leverages camera-specific image characteristics for invariance.  \n\n**Probability Calculations:**  \n- **Intra-Camera Probability:**  \n  $$ p_{ij}^{intra} = \\frac{\\exp(s.m_j^Tx_i^t)}{\\sum_{k \\in O_i^{intra}}\\exp(s.m_k^Tx_i^t)} $$  \n- **Inter-Camera Probability:**  \n  $$ p_{ij}^{inter} = \\frac{\\exp(s.m_j^Tx_i^t)}{\\sum_{k \\in O_i^{inter}}\\exp(s.m_k^x_i^t)} $$  \n- **Combined Loss:**  \n  $$ \\mathcal{L} = -\\sum w_{i,j}\\log(p_{i,j}^{in,tra}) - \\sum w_{i,t}\\log(p_{i,t}^{inter}) $$  \n\n**Video-Based Invariance (TAUDL):**  \n- Learns intra-camera tracklet features first, then propagates learning to inter-camera features.  \n\n---\n\n### **3. Camera-Aware Invariance in Unsupervised Domain Adaptation**  \n**Incomplete Section:**  \nThe text truncates here, but the following context is implied:  \n- **Domain Adaptation (UDA):** Unsupervised domain adaptation aims to align features between source and target domains.  \n  - Example:  \n    > \"Unsupervised domain adaptation (UDA) focuses on aligning features between source and target domains without labeled target data.\"  \n- **Attribute Alignment:** Mid-level attributes (e.g., color, texture) are aligned between domains.  \n  - Figure:  \n    > \"Figure Caption: Illustration of feature alignment approach for unsupervised domain adaptation. Mid-level attribute features are aligned between source and target domains in a joint learning pipeline. $_{attr}$ represents the attribute alignment loss between the source attributes and the target attributes.\"  \n\n---\n\n### **4. Key Equations & Methods**  \n- **OIM Loss (Online Instance Mining):**  \n  $$ \\mathcal{L} = -\\log(p_t) \\quad \\text{where} \\quad p_{ij} = \\frac{\\exp(s.m_j^Tx_i^t)}{\\sum_{k=1}^{N_t}\\exp(s.m_k^Tx_i^t)} $$  \n- **Triplet Loss:**  \n  $$ L = \\sum_{i=1}^P \\left[m + D(x_a^i, x_p^j) - D(x_a^i, x_n^j) \\right] $$  \n\n---\n\n### **5. Challenges & Solutions**  \n- **Hard Samples:** Methods like TSSL (soft labels) and HCT (hard-batch triplet loss) mitigate hard sample bias.  \n- **Cross-Camera Variability:** Techniques like Mixup, MetaCam, and UGA address domain shifts and noisy labels.  \n- **Temporal Information:** Video-based methods (e.g., JVTC, TAUDL) exploit temporal consistency for robustness.  \n\n---\n\n### **Figure & Label**  \n- **Caption:** *Illustration of feature alignment approach for unsupervised domain adaptation. Mid-level attribute features are aligned between source and target domains in a joint learning pipeline.*  \n- **Label:** `fig:fa`  \n\n---\n\nThis summary captures the core concepts, equations, and methods from the provided text, focusing on clustering, camera invariance, and domain adaptation. Let me know if you need further details on specific sections!",
  "extraction_method": "LLM-based"
}