{
  "summary": "Memory-based adapters enable single-view 3D models to handle multi-view data without task-specific changes.",
  "caption": "Details about the architectures of image and point cloud backbones and how to insert the adapters into them.",
  "label": "arch",
  "extracted_context": "**Summary of Key Contributions and Findings from the Paper and Supplementary Material**\n\n### **1. Core Contributions**\n- **Memory-Based Adapters**: Propose a generic framework to adapt single-view 3D perception models (e.g., semantic segmentation, object detection, instance segmentation) to handle multi-view data without task-specific modifications.\n- **Plug-and-Play Design**: Insert adapters into existing backbones (e.g., U-Net, Minkowski-UNet, FCAF3D, TD3D) to retain global context from previous frames while processing current frames.\n- **Cross-Task Generalization**: Achieve state-of-the-art performance on **ScanNet** and **SceneNN** without task-specific designs, demonstrating versatility across 3D perception tasks.\n\n---\n\n### **2. Methodology Overview**\n- **Adapters for Temporal Context**:\n  - **Image-to-3D Memory**: For image features, a 3D-to-2D adapter enhances features using 3D memory (e.g., scene representations from ROI-wise point clouds).\n  - **Point Cloud Memory**: For point cloud tasks, a memory buffer caches high-resolution scene representations (e.g., in TD3D) to ensure completeness within each ROI.\n- **3D NMS for Instance Segmentation**: Simplifies mask fusion by leveraging complete point clouds from memory, avoiding complex strategies like mask fusion.\n\n---\n\n### **3. Architecture Integration (Supplementary Details)**\n- **Semantic Segmentation**:\n  - **Image Backbone**: U-Net (Figure 1, D).\n  - **Point Cloud Backbone**: Minkowski-UNet (Figure 1, C).\n- **Object Detection**:\n  - **Image Backbone**: ResNet + FPN (Figure 1, E).\n  - **Point Cloud Backbone**: FCAF3D (Figure 1, B).\n- **Instance Segmentation**:\n  - **Image Backbone**: Same as object detection (Figure 1, E).\n  - **Point Cloud Backbone**: TD3D (Figure 1, A).\n- **Adapter Insertion**: Adapters are integrated into the backbone's feature extraction pipeline to maintain temporal context without altering the original model structure.\n\n---\n\n### **4. Training Hyperparameters (Supplementary Details)**\n- **Stage 1 (Single-View Training)**:\n  - Semantic Segmentation: 250 epochs, AdamW optimizer, OneCycleLR scheduler.\n  - Object Detection: 12 epochs, stepwise scheduler.\n  - Instance Segmentation: 33 epochs, stepwise scheduler.\n- **Stage 2 (Adapter Finetuning)**:\n  - Learning rates and weights decay adjusted for each task (e.g., 0.0008â€“0.008 for semantic segmentation, 0.001 for detection/segmentation).\n\n---\n\n### **5. Class-Specific Results (Supplementary Tables)**\n- **3D Semantic Segmentation**:\n  - **ScanNet**: High performance on common categories (e.g., **walls (85.7%)**, **floors (97.1%)**, **beds (80.7%)**), but lower on rare categories like **curtains (58.9%)**.\n  - **SceneNN**: Strong performance on walls (75.3%) and floors (82.6%), but lower on less frequent objects (e.g., **curtains (50.2%)**).\n- **3D Object Detection**:\n  - **ScanNet**: Strong AP scores for **beds (85.4%)**, **chairs (88.7%)**, and **sofas (87.2%)**, but lower for **curtains (29.4%)** and **toilets (95.2%)**.\n- **Key Insight**: The model excels on frequent, semantically rich categories but faces challenges with ambiguous or sparse objects (e.g., curtains, small furniture).\n\n---\n\n### **6. Key Takeaways**\n- **Efficiency**: Adapters enable existing models to handle multi-view data with minimal modifications, reducing the need for task-specific retraining.\n- **Performance**: Outperforms state-of-the-art online methods on ScanNet and SceneNN, even without task-specific designs.\n- **Scalability**: The approach is adaptable to various 3D perception tasks and backbones, making it a versatile solution for real-world applications.\n\nThis work bridges the gap between single-view and multi-view 3D perception by leveraging memory-based adapters, offering a unified framework for improving temporal consistency and robustness in scene understanding.",
  "extraction_method": "LLM-based"
}