{
  "summary": "Memory-based adapters enable offline 3D models to handle online perception tasks efficiently and effectively.",
  "caption": "We propose a general framework for online 3D scene perception. With the presented memory-based adapters, we empower existing offline models in different tasks with online perception ability, which is valuable for robotics applications.",
  "label": "teaser",
  "extracted_context": "### Summary of Key Contributions and Findings\n\n**Problem Addressed**:  \nThe paper tackles the challenge of transitioning **offline 3D scene perception models** (which process single static views) to **online settings** (handling sequential, dynamic data from sensors like RGB-D cameras). This is critical for robotics applications requiring real-time scene understanding.\n\n---\n\n### **Key Innovations**  \n1. **Memory-Based Adapters**:  \n   - **Core Contribution**: A modular architecture that injects memory mechanisms into existing offline models (e.g., U-Net, Minkowski-UNet, FCAF3D, TD3D).  \n   - **Functionality**:  \n     - **Time-Aware Memory**: Stores spatial-temporal information from previous frames to improve sequential reasoning.  \n     - **3D-to-2D Adapter**: Enhances image features with 3D memory (e.g., for instance segmentation, enabling complete instance masks via 3D NMS).  \n   - **Flexibility**: Works without task-specific design, enabling seamless adaptation of offline models to online workflows.\n\n2. **Cross-Task Applicability**:  \n   - Validated on three tasks:  \n     - **Semantic Segmentation** (ScanNet, SceneNN)  \n     - **Object Detection** (ScanNet)  \n     - **Instance Segmentation** (ScanNet)  \n   - Achieved state-of-the-art performance **without retraining** models from scratch, leveraging existing architectures.\n\n3. **Efficiency and Generalization**:  \n   - **Lightweight Design**: Adapters are inserted at minimal computational overhead.  \n   - **Generalization**: Demonstrated effectiveness on diverse datasets (ScanNet, SceneNN) and object categories (e.g., furniture, appliances, walls).\n\n---\n\n### **Experimental Results**  \n- **Semantic Segmentation**:  \n  - **ScanNet**: Mean IoU of **72.7%** (vs. 70.1% of offline baselines).  \n  - **SceneNN**: Mean IoU of **56.7%** (vs. 54.0% of offline baselines).  \n- **Object Detection**:  \n  - **ScanNet**: Mean AP$_{25}$ of **68.1%** (vs. 65.0% of offline baselines).  \n- **Instance Segmentation**:  \n  - Achieved competitive performance with reduced complexity compared to mask fusion strategies.\n\n---\n\n### **Significance for Robotics**  \n- **Real-Time Capabilities**: Enables robots to process dynamic scenes (e.g., moving objects, changing lighting) using existing offline models.  \n- **Reduced Development Cost**: Avoids retraining or redesigning models for online scenarios, accelerating deployment in applications like autonomous navigation and augmented reality.  \n\n---\n\n### **Conclusion**  \nThe framework bridges the gap between offline and online 3D perception by introducing memory-based adapters that adapt static models to sequential data. It achieves competitive results across multiple tasks while maintaining flexibility and efficiency, offering a scalable solution for real-world robotics applications.",
  "extraction_method": "LLM-based"
}