{
  "summary": "The MMC framework certifies majority voting via e-processes, balancing theoretical guarantees with efficient sampling...",
  "caption": "Given a prompt, the model generates multiple reasoning rollouts from the reference distribution $_{}(|{pr})$. The resulting terminal answers are aggregated via majority voting, viewed as mode estimation under sampling uncertainty. The Martingale Majority Certificate (MMC) monitors the empirical margin and provides an stopping rule for certification. Test-time training with SNR or entropy-based adaptation sharpens the terminal distribution, thereby increasing the signal-to-noise ratio (SNR) and reducing the number of samples required for certification.",
  "label": "fig:framework",
  "extracted_context": "**Relevant Context Paragraphs on the Martingale Majority Certificate (MMC) Framework:**\n\n---\n\n### **1. Core Framework Overview**  \nThe MMC framework leverages **e-processes** to monitor the empirical margin of majority voting under sampling uncertainty. It generates multiple reasoning rollouts from a reference distribution $ P(\\cdot | \\text{pr}) $, aggregates terminal answers via majority voting (mode estimation), and employs a **stopping rule** to certify the correct majority answer. The stopping rule is based on **supermartingale properties** of the $ e $-values, ensuring validity under composite null hypotheses (e.g., tied votes).  \n\n**Key components**:  \n- **Dynamic label counts** $ (s_n, f_n, o_n) $ for top-2 candidates and others.  \n- **Two e-processes** ($ e^{\\text{run}}_n $, $ e^{\\text{oth}}_n $) for testing:  \n  - $ e^{\\text{run}}_n $: Competing top-2 candidates.  \n  - $ e^{\\text{oth}}_n $: Aggregated others vs. top-2.  \n- **Stopping condition**: When both $ e^{\\text{run}}_n \\geq 1/\\varepsilon $ and $ e^{\\text{oth}}_n \\geq 1/\\varepsilon $, the majority answer is certified.  \n- **Budget constraint**: If the sample limit $ N_{\\text{budget}} $ is reached, the model abstains.  \n\n---\n\n### **2. Practical Priors for Computing $ e $-Values**  \n**A. Truncated Beta Prior**  \n- **Definition**: A single latent parameter shared across informative rounds, truncated to $ (\\tfrac{1}{2}, 1] $ to ensure support under the one-sided alternative.  \n- **Closed-form updates**:  \n  $$\n  e^{\\text{run}}_n = 2^{M_n} \\cdot \\frac{\\mathsf{B}_{>1/2}(a+s_n, b+f_n)}{\\mathsf{B}_{>1/2}(a,b)}, \\quad \n  e^{\\text{oth}}_n = 2^{T_n} \\cdot \\frac{\\mathsf{B}_{>1/2}(a+s_n, b+o_n)}{\\mathsf{B}_{>1/2}(a,b)}.\n  $$  \n- **Hyperparameters**: Defaults include Jeffreys ($ a = b = \\tfrac{1}{2} $) or Laplace ($ a = b = 1 $).  \n\n**B. Updating Plug-in Point Prior**  \n- **Shared smoothing**: Uses hyperparameters $ (\\alpha_A, \\alpha_B, \\alpha_O) $ to estimate multinomial probabilities $ \\hat{p}_{A,n}, \\hat{p}_{B,n}, \\hat{p}_{O,n} $.  \n- **Informative-round parameters**:  \n  $$\n  \\theta^\\star_n = \\operatorname{clip}\\left( \\frac{\\hat{p}_{A,n}}{\\hat{p}_{A,n} + \\hat{p}_{B,n}} \\right), \\quad \\lambda^\\star_n = \\operatorname{clip}\\left( \\frac{\\hat{p}_{A,n}}{\\hat{p}_{A,n} + \\hat{p}_{O,n}} \\right).\n  $$  \n- **Dynamic updates**: Adjusts $ e $-values based on observed votes (A, B, or others).  \n\n---\n\n### **3. Algorithm (Algorithm 1)**  \n- **Steps**:  \n  1. **Track top-2 candidates** ($ A_{n-1}, B_{n-1} $) and others.  \n  2. **Compute $ \\rho_{\\text{run}} $** and $ \\rho_{\\text{oth}} $ ratios for updating $ e $-values.  \n  3. **Update counts** $ (s_n, f_n, o_n) $ based on the observed vote.  \n  4. **Check stopping conditions** or budget limits.  \n- **Key innovation**: Online computation of $ e $-values using **ratios** of Beta functions, enabling real-time monitoring.  \n\n---\n\n### **4. Figure Description (fig:framework)**  \nThe framework integrates:  \n- **Prompt-driven rollouts**: Generating multiple reasoning paths from $ P(\\cdot | \\text{pr}) $.  \n- **Majority voting**: Aggregating terminal answers to estimate the mode.  \n- **MMC certification**: Monitoring the empirical margin via $ e $-values to stop sampling.  \n- **Adaptation**: Test-time training with **SNR/entropy-based adaptation** sharpens terminal distributions, improving certification efficiency by reducing required samples.  \n\n---\n\n**Summary**: The MMC framework combines **Bayesian inference**, **online learning**, and **stochastic stopping rules** to certify majority voting under uncertainty. Its design balances theoretical guarantees (supermartingale properties) with practical efficiency (closed-form updates, adaptive priors).",
  "extraction_method": "LLM-based"
}