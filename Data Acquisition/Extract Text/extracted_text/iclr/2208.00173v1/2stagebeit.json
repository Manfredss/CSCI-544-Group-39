{
  "summary": "BEIT's pre-training uses self-attention to capture semantic regions without task-specific supervision.",
  "caption": "Overview of BEIT pre-training. The figure is edited from~.",
  "label": "fig:beit",
  "extracted_context": "**Relevant Context Paragraphs:**\n\n1. **BEIT Pre-Training Overview**  \n   \"To explain why BEIT~\\cite{bao2022beit} helps the finetuning on downstream tasks, its authors analyze the self-attention map and show that BEIT distinguishes semantic regions using self-attention heads without any task-specific supervision.\"  \n   *Figure Caption:* \"Overview of BEIT pre-training. The figure is edited from~.\"  \n   *Figure Label:* `fig:beit`  \n\n2. **MAE's Reconstruction Capabilities**  \n   \"Moreover,~\\cite{he2022masked} shows that an MAE, pretrained with a masking ratio of 75\\%, infers complex and holistic reconstructions even when 95\\% of pixels are masked, suggesting it learns various concepts, \\textit{i.e.}, \\ semantics.\"  \n\n3. **Rich Hidden Representation Hypothesis**  \n   \"The authors of MAE~\\cite{he2022masked} ``hypothesize that this behavior occurs through a rich hidden representation inside the MAE''. Given that the masked and reconstructed visual patches are not semantic entities as words in languages, this behavior is somewhat unexpected and is hypothesized to occur ``by way of a rich hidden representation''~\\cite{he2022masked}.\"  \n\n4. **Backbone Compatibility with CNN**  \n   \"With ViT~\\cite{dosovitskiy2021an} as the default backbone in MAE~\\cite{he2022masked}, a natural question is whether masked autoencoder works only with a transformer backbone instead of CNN. Since CNN cannot tackle the masked inputs and positional embedding directly, multiple works~\\cite{gao2022convmae,fang2022unleashing,li2022architecture,fang2022corrupted} have attempted to unify ViT and CNN in a compatible masked autoencoder framework.\"  \n\n5. **Data Requirements for Pretraining**  \n   \"A popular belief regarding the benefit of transfer learning comes from pretraining on a much larger dataset than the target dataset. Challenging this belief,~\\cite{el2021large} investigates whether self-supervised pretraining on a smaller dataset can yield the same benefit. The fact that their investigation is performed with ViT-based masked autoencoder makes it more interesting because, compared with its CNN, ViT is found to require much more samples~\\cite{dosovitskiy2021an}.\"  \n\n6. **Denoising Perspective and Corruption Methods**  \n   \"Given that masked autoencoder is a class of denoising autoencoder, \\cite{tian2022beyond} investigates a general question: are there other effective image degradation methods beyond masking for effective visual pretraining? Five methods, namely zoom-in, zoom-out, distortion, blurring, and de-colorizing, have been investigated, and they are found to perform better than None (\\textit{i.e.}, \\ no pretraining), suggesting a unified denoising perspective on the success of masked autoencoder.\"  \n\n---  \n**Key Focus:** The BEIT pre-training explanation, MAE's reconstruction capabilities, and the denoising perspective are central to understanding the success of masked autoencoders in vision tasks. The figure caption and label (`fig:beit`) provide context for visualizing BEIT's pre-training process.",
  "extraction_method": "LLM-based"
}