{
  "summary": "A masked autoencoder reconstructs most of an image from heavily masked inputs, capturing semantic understanding witho...",
  "caption": "Overview of a masked autoencoder with the figure borrowed from the original work MAE~.",
  "label": "fig:mae",
  "extracted_context": "**Relevant Context Paragraphs:**\n\n1. **Introduction to Masked Autoencoders (MAEs) and Their Success:**  \n   \"To explain why BEIT... helps the finetuning on downstream tasks, its authors analyze the self-attention map and show that BEiT distinguishes semantic regions using self-attention heads without any task-specific supervision. Moreover, [MAE] shows that an MAE, pretrained with a masking ratio of 75%, infers complex and holistic reconstructions even when 95% of pixels are masked, suggesting it learns various concepts, i.e., semantics.\"\n\n2. **Compatibility with CNN Architectures:**  \n   \"A natural question is whether masked autoencoder works only with a transformer backbone instead of CNN. Since CNN cannot tackle the masked inputs and positional embedding directly, multiple works... have attempted to unify ViT and CNN in a compatible masked autoencoder framework. ConvMAE... utilizes hybrid convolution-transformer architectures... Early attempts of CNN-based inpainting... focuses on reconstruction task with low-level interactions, which causes higher feature uncertainty.\"\n\n3. **Data Requirements and Efficiency:**  \n   \"A popular belief... pretraining on a much larger dataset than the target dataset. Challenging this belief, [research] investigates whether self-supervised pretraining on a smaller dataset can yield the same benefit... pretraining masked autoencoder... on 1% of ImageNet dataset achieves comparable transfer performance... More recently, [research] performed a comprehensive study on data scaling... MIM is also demanding on larger data, especially for larger models with longer training epochs.\"\n\n4. **Denoising Perspective and Degradation Methods:**  \n   \"Given that masked autoencoder is a class of denoising autoencoder, [research] investigates a general question: are there other effective image degradation methods beyond masking for effective visual pretraining? Five methods... are found to perform better than None... Among them, zoom-in performs the best and is complementary with masking to further boost the performance. [Research] also investigates frequency masking by predicting masked high-frequency from the unmasked low-frequency content, or vice versa.\"\n\n5. **Efficiency Improvements via Input Manipulation:**  \n   \"Local masked reconstruction (LoMaR)... reduces the required computation while achieving comparable or better downstream task performance. ObjMAE... reduces the pre-training compute cost by 72%... MixMIM... replaces an image's masked tokens with tokens from another image... achieves stronger results efficiently compared to existing MIM works.\"\n\n6. **Hierarchical Architectures and Masking Strategies:**  \n   \"HiViT... substitutes window attention layers in Swin with MLP layers... Uniform Masking MAE (UM-MAE)... uniformly samples patches... avoids shortcuts for pixel reconstruction... achieves comparable performance to baselines while requiring less training time and GPU memory.\"\n\n7. **Unification of ViT and CNN in MIM:**  \n   \"CIM... replaces the input images artificially masked in MIM with a corrupted image generated by a trainable generator... yields compelling results in vision benchmarks. AÂ²MIM... is compatible with ViT and CNN in a unified way... success lies in learning middle-level patch interaction, agnostic to architecture choices.\"  \n\nThese paragraphs highlight the key innovations, efficiency gains, and theoretical insights into the success of masked autoencoders in vision tasks.",
  "extraction_method": "LLM-based"
}