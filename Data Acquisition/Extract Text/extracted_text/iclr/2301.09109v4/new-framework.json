{
  "summary": "FedRAP combines local and global embeddings with sparsity to create personalized item representations for efficient, ...",
  "caption": ". For each user-$i$, FedRAP utilizes its ratings $r_i$ as labels to locally train a user embedding $_i$ and a local item embedding $^{(i)}$. By adding $^{(i)}$ to the global item embedding $$ from the server, i.e., , FedRAP creates a personalized item embedding based on both shared knowledge and personal perspective and thus produces better recommendations. Since clients and server only communicate $$, FedRAP enforce its sparsity to reduce the communication cost and encourage its difference to $^{(i)}$ so they are complementary.-1",
  "label": "fig:framework",
  "extracted_context": "**Relevant Context Paragraphs:**\n\n1. **Framework Description (Fig. 1):**  \n   \"For each user-$i$, FedRAP utilizes its ratings $r_i$ as labels to locally train a user embedding $_i$ and a local item embedding $^{(i)}$. By adding $^{(i)}$ to the global item embedding $$ from the server, i.e., , FedRAP creates a personalized item embedding based on both shared knowledge and personal perspective and thus produces better recommendations. Since clients and server only communicate $$, FedRAP enforces its sparsity to reduce the communication cost and encourage its difference to $^{(i)}$ so they are complementary.\"  \n   *This explains the core architecture of FedRAP, emphasizing local-global embedding fusion, sparsity enforcement, and privacy-preserving communication.*\n\n2. **Differential Privacy Mechanism:**  \n   \"To protect user privacy, we introduce noise from the [Gaussian distribution] to the gradients during model training, ensuring differential privacy (DP) guarantees with a specified $\\epsilon$-$\\delta$ privacy budget.\"  \n   *Highlights the use of noise injection for privacy protection, though the full text is truncated here.*\n\n3. **Ablation Study on Sparsity of $\\mathbf{C}$:**  \n   \"Since FedRAP only exchanges $\\mathbf{C}$ between server and clients, retaining user-related information on the client side, we further explore the impact of sparsity constraints on $\\mathbf{C}$ in FedRAP by comparing the data distribution of $\\mathbf{C}$ and $\\{\\mathbf{D}^{(i)}\\}^n_{i=1}$ learned in the final iteration when training both FedRAP and FedRAP-L2 on the ML-100K dataset.\"  \n   *Analyzes how sparsity in $\\mathbf{C}$ reduces communication overhead and supports personalized learning.*\n\n4. **Effectiveness of Personalization:**  \n   \"FedRAP-C performs the worst, followed by FedRAP-D and PFedRec. These indicate the importance of not only personalizing the item information that is user-related, but also keeping the parts of the item information that are non-user-related.\"  \n   *Demonstrates the necessity of balancing personalization and shared knowledge for effective recommendations.*\n\n5. **Weight Curriculum Analysis:**  \n   \"FedRAP-fixed, with constant weights, underperforms FedRAP, which adjusts weights from small to large. This confirms that early-stage additive personalization leads to performance degradation, as item embeddings have not yet captured enough useful information.\"  \n   *Explains how dynamic weight schedules (e.g., tanh) improve model performance compared to static settings.*\n\n6. **Communication Efficiency:**  \n   \"A sparse $\\mathbf{C}$ helps reduce the communication overhead between the server and clients.\"  \n   *Emphasizes the practical benefit of sparsity in federated settings for resource efficiency.*\n\n7. **Model Comparison:**  \n   \"The results of FedRAP-No, FedRAP-L2 and FedRAP are very close, where the performance of FedRAP is slightly better than the other two as seen in Fig. \\ref{fig:ablation_effective}(b). Thus, we choose to use $L_1$ norm to induce $\\mathbf{C}$ to become sparse.\"  \n   *Validates the superiority of $L_1$-based sparsity constraints over alternatives like Frobenius norm.*\n\nThese paragraphs summarize the key technical contributions, design choices, and experimental findings of FedRAP, focusing on its framework, privacy mechanisms, and performance validation.",
  "extraction_method": "LLM-based"
}