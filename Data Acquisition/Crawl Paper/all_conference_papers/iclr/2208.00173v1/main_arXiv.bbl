% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{100}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{he2022masked}
K.~He, X.~Chen, S.~Xie, Y.~Li, P.~Doll{\'a}r, and R.~Girshick, ``Masked
  autoencoders are scalable vision learners,'' in \emph{CVPR}, 2022.

\bibitem{lecun2015deep}
Y.~LeCun, Y.~Bengio, and G.~Hinton, ``Deep learning,'' \emph{nature}, 2015.

\bibitem{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``{ImageNet} classification with
  deep convolutional neural networks,'' in \emph{NeurIPS}, 2012.

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman, ``Very deep convolutional networks for
  large-scale image recognition,'' in \emph{ICLR}, 2015.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{CVPR}, 2016.

\bibitem{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei, ``{ImageNet}: A
  large-scale hierarchical image database,'' in \emph{CVPR}, 2009.

\bibitem{dosovitskiy2021an}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit,
  and N.~Houlsby, ``An image is worth 16x16 words: Transformers for image
  recognition at scale,'' in \emph{ICLR}, 2021.

\bibitem{he2020momentum}
K.~He, H.~Fan, Y.~Wu, S.~Xie, and R.~Girshick, ``Momentum contrast for
  unsupervised visual representation learning,'' in \emph{CVPR}, 2020.

\bibitem{chen2020simple}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton, ``A simple framework for
  contrastive learning of visual representations,'' in \emph{ICML}, 2020.

\bibitem{bao2022beit}
H.~Bao, L.~Dong, and F.~Wei, ``Beit: Bert pre-training of image transformers,''
  \emph{ICLR}, 2022.

\bibitem{ramesh2021zero}
A.~Ramesh, M.~Pavlov, G.~Goh, S.~Gray, C.~Voss, A.~Radford, M.~Chen, and
  I.~Sutskever, ``Zero-shot text-to-image generation,'' in \emph{ICML}, 2021.

\bibitem{vincent2008extracting}
P.~Vincent, H.~Larochelle, Y.~Bengio, and P.-A. Manzagol, ``Extracting and
  composing robust features with denoising autoencoders,'' in \emph{ICML},
  2008.

\bibitem{vincent2010stacked}
P.~Vincent, H.~Larochelle, I.~Lajoie, Y.~Bengio, P.-A. Manzagol, and L.~Bottou,
  ``Stacked denoising autoencoders: Learning useful representations in a deep
  network with a local denoising criterion.'' \emph{Journal of machine learning
  research}, 2010.

\bibitem{pathak2016context}
D.~Pathak, P.~Krahenbuhl, J.~Donahue, T.~Darrell, and A.~A. Efros, ``Context
  encoders: Feature learning by inpainting,'' in \emph{CVPR}, 2016.

\bibitem{devlin2019bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' \emph{NAACL}, 2019.

\bibitem{ng2011sparse}
A.~Ng \emph{et~al.}, ``Sparse autoencoder,'' \emph{CS294A Lecture notes}, 2011.

\bibitem{noroozi2016unsupervised}
M.~Noroozi and P.~Favaro, ``Unsupervised learning of visual representations by
  solving jigsaw puzzles,'' in \emph{ECCV}, 2016.

\bibitem{gidaris2018unsupervised}
S.~Gidaris, P.~Singh, and N.~Komodakis, ``Unsupervised representation learning
  by predicting image rotations,'' \emph{ICLR}, 2018.

\bibitem{zhang2022how}
C.~Zhang, K.~Zhang, C.~Zhang, T.~X. Pham, C.~D. Yoo, and I.~S. Kweon, ``How
  does simsiam avoid collapse without negative samples? a unified understanding
  with self-supervised contrastive learning,'' in \emph{ICLR}, 2022.

\bibitem{zhang2022dual}
C.~Zhang, K.~Zhang, T.~X. Pham, C.~Yoo, and I.-S. Kweon, ``Dual temperature
  helps contrastive learning without many negative samples: Towards
  understanding and simplifying moco,'' in \emph{CVPR}, 2022.

\bibitem{jing2021understanding}
L.~Jing, P.~Vincent, Y.~LeCun, and Y.~Tian, ``Understanding dimensional
  collapse in contrastive self-supervised learning,'' \emph{arXiv preprint
  arXiv:2110.09348}, 2021.

\bibitem{yi2022masked}
K.~Yi, Y.~Ge, X.~Li, S.~Yang, D.~Li, J.~Wu, Y.~Shan, and X.~Qie, ``Masked image
  modeling with denoising contrast,'' \emph{arXiv preprint arXiv:2205.09616},
  2022.

\bibitem{wei2022masked}
C.~Wei, H.~Fan, S.~Xie, C.-Y. Wu, A.~Yuille, and C.~Feichtenhofer, ``Masked
  feature prediction for self-supervised visual pre-training,'' in \emph{CVPR},
  2022.

\bibitem{baevski2022data2vec}
A.~Baevski, W.-N. Hsu, Q.~Xu, A.~Babu, J.~Gu, and M.~Auli, ``Data2vec: A
  general framework for self-supervised learning in speech, vision and
  language,'' \emph{arXiv preprint arXiv:2202.03555}, 2022.

\bibitem{xie2022simmim}
Z.~Xie, Z.~Zhang, Y.~Cao, Y.~Lin, J.~Bao, Z.~Yao, Q.~Dai, and H.~Hu, ``Simmim:
  A simple framework for masked image modeling,'' in \emph{CVPR}, 2022.

\bibitem{yu2022point}
X.~Yu, L.~Tang, Y.~Rao, T.~Huang, J.~Zhou, and J.~Lu, ``Point-bert:
  Pre-training 3d point cloud transformers with masked point modeling,'' in
  \emph{CVPR}, 2022.

\bibitem{assran2022masked}
M.~Assran, M.~Caron, I.~Misra, P.~Bojanowski, F.~Bordes, P.~Vincent, A.~Joulin,
  M.~Rabbat, and N.~Ballas, ``Masked siamese networks for label-efficient
  learning,'' \emph{arXiv preprint arXiv:2204.07141}, 2022.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in
  \emph{NeurIPS}, 2017.

\bibitem{radford2018improving}
A.~Radford, K.~Narasimhan, T.~Salimans, I.~Sutskever \emph{et~al.}, ``Improving
  language understanding by generative pre-training,'' 2018.

\bibitem{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever \emph{et~al.},
  ``Language models are unsupervised multitask learners,'' \emph{OpenAI blog},
  2019.

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell \emph{et~al.}, ``Language
  models are few-shot learners,'' \emph{Advances in neural information
  processing systems}, 2020.

\bibitem{bilogur2020notes}
A.~Bilogur, ``Notes on gpt-2 and bert models,'' \emph{Kaggle blog}, 2019.

\bibitem{wu2018unsupervised}
Z.~Wu, Y.~Xiong, S.~X. Yu, and D.~Lin, ``Unsupervised feature learning via
  non-parametric instance discrimination,'' in \emph{CVPR}, 2018.

\bibitem{oord2018representation}
A.~v.~d. Oord, Y.~Li, and O.~Vinyals, ``Representation learning with
  contrastive predictive coding,'' \emph{arXiv preprint arXiv:1807.03748},
  2018.

\bibitem{zbontar2021barlow}
J.~Zbontar, L.~Jing, I.~Misra, Y.~LeCun, and S.~Deny, ``Barlow twins:
  Self-supervised learning via redundancy reduction,'' \emph{ICML}, 2021.

\bibitem{chen2021exploring}
X.~Chen and K.~He, ``Exploring simple siamese representation learning,'' in
  \emph{CVPR}, 2021.

\bibitem{larsson2016learning}
G.~Larsson, M.~Maire, and G.~Shakhnarovich, ``Learning representations for
  automatic colorization,'' in \emph{ECCV}, 2016.

\bibitem{larsson2017colorization}
------, ``Colorization as a proxy task for visual understanding,'' in
  \emph{CVPR}, 2017.

\bibitem{zhang2016colorful}
R.~Zhang, P.~Isola, and A.~A. Efros, ``Colorful image colorization,'' in
  \emph{ECCV}, 2016.

\bibitem{zhang2017split}
------, ``Split-brain autoencoders: Unsupervised learning by cross-channel
  prediction,'' in \emph{CVPR}, 2017.

\bibitem{chen2020generative}
M.~Chen, A.~Radford, R.~Child, J.~Wu, H.~Jun, D.~Luan, and I.~Sutskever,
  ``Generative pretraining from pixels,'' in \emph{ICML}, 2020.

\bibitem{chen2020igpt_blog}
------, ``Generative pretraining from pixels,'' in \emph{OpenAI blog}, 2020.

\bibitem{caron2021emerging}
M.~Caron, H.~Touvron, I.~Misra, H.~J{\'e}gou, J.~Mairal, P.~Bojanowski, and
  A.~Joulin, ``Emerging properties in self-supervised vision transformers,''
  \emph{arXiv preprint arXiv:2104.14294}, 2021.

\bibitem{dong2021peco}
X.~Dong, J.~Bao, T.~Zhang, D.~Chen, W.~Zhang, L.~Yuan, D.~Chen, F.~Wen, and
  N.~Yu, ``Peco: Perceptual codebook for bert pre-training of vision
  transformers,'' \emph{arXiv preprint arXiv:2111.12710}, 2021.

\bibitem{li2022mc}
X.~Li, Y.~Ge, K.~Yi, Z.~Hu, Y.~Shan, and L.-Y. Duan, ``mc-beit: Multi-choice
  discretization for image bert pre-training,'' \emph{arXiv preprint
  arXiv:2203.15371}, 2022.

\bibitem{chen2022context}
X.~Chen, M.~Ding, X.~Wang, Y.~Xin, S.~Mo, Y.~Wang, S.~Han, P.~Luo, G.~Zeng, and
  J.~Wang, ``Context autoencoder for self-supervised representation learning,''
  \emph{arXiv preprint arXiv:2202.03026}, 2022.

\bibitem{dalal2005histograms}
N.~Dalal and B.~Triggs, ``Histograms of oriented gradients for human
  detection,'' in \emph{CVPR}, 2005.

\bibitem{wettig2022should}
A.~Wettig, T.~Gao, Z.~Zhong, and D.~Chen, ``Should you mask 15\% in masked
  language modeling?'' \emph{arXiv preprint arXiv:2202.08005}, 2022.

\bibitem{wang2021pvt}
W.~Wang, E.~Xie, X.~Li, D.-P. Fan, K.~Song, D.~Liang, T.~Lu, P.~Luo, and
  L.~Shao, ``Pyramid vision transformer: A versatile backbone for dense
  prediction without convolutions,'' 2021.

\bibitem{liu2021swin}
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo, ``Swin
  transformer: Hierarchical vision transformer using shifted windows,'' 2021.

\bibitem{huang2022green}
L.~Huang, S.~You, M.~Zheng, F.~Wang, C.~Qian, and T.~Yamasaki, ``Green
  hierarchical vision transformer for masked image modeling,'' \emph{arXiv
  preprint arXiv:2205.13515}, 2022.

\bibitem{li2022uniform}
X.~Li, W.~Wang, L.~Yang, and J.~Yang, ``Uniform masking: Enabling mae
  pre-training for pyramid-based vision transformers with locality,''
  \emph{arXiv preprint arXiv:2205.10063}, 2022.

\bibitem{zhang2022hivit}
X.~Zhang, Y.~Tian, W.~Huang, Q.~Ye, Q.~Dai, L.~Xie, and Q.~Tian, ``Hivit:
  Hierarchical vision transformer meets masked image modeling,'' \emph{arXiv
  preprint arXiv:2205.14949}, 2022.

\bibitem{chen2022efficient}
J.~Chen, M.~Hu, B.~Li, and M.~Elhoseiny, ``Efficient self-supervised vision
  pretraining with local masked reconstruction,'' \emph{arXiv preprint
  arXiv:2206.00790}, 2022.

\bibitem{wu2022object}
J.~Wu and S.~Mo, ``Object-wise masked autoencoders for fast pre-training,''
  \emph{arXiv preprint arXiv:2205.14338}, 2022.

\bibitem{zhou2016learning}
B.~Zhou, A.~Khosla, A.~Lapedriza, A.~Oliva, and A.~Torralba, ``Learning deep
  features for discriminative localization,'' in \emph{CVPR}, 2016.

\bibitem{liu2022mixmim}
J.~Liu, X.~Huang, Y.~Liu, and H.~Li, ``Mixmim: Mixed and masked image modeling
  for efficient visual representation learning,'' \emph{arXiv preprint
  arXiv:2205.13137}, 2022.

\bibitem{gao2022convmae}
P.~Gao, T.~Ma, H.~Li, J.~Dai, and Y.~Qiao, ``Convmae: Masked convolution meets
  masked autoencoders,'' \emph{arXiv preprint arXiv:2205.03892}, 2022.

\bibitem{fang2022unleashing}
Y.~Fang, S.~Yang, S.~Wang, Y.~Ge, Y.~Shan, and X.~Wang, ``Unleashing vanilla
  vision transformer with masked image modeling for object detection,''
  \emph{arXiv preprint arXiv:2204.02964}, 2022.

\bibitem{li2022architecture}
S.~Li, D.~Wu, F.~Wu, Z.~Zang, K.~Wang, L.~Shang, B.~Sun, H.~Li, S.~Li
  \emph{et~al.}, ``Architecture-agnostic masked image modeling--from vit back
  to cnn,'' \emph{arXiv preprint arXiv:2205.13943}, 2022.

\bibitem{fang2022corrupted}
Y.~Fang, L.~Dong, H.~Bao, X.~Wang, and F.~Wei, ``Corrupted image modeling for
  self-supervised visual pre-training,'' \emph{arXiv preprint
  arXiv:2202.03382}, 2022.

\bibitem{xiao2021early}
T.~Xiao, M.~Singh, E.~Mintun, T.~Darrell, P.~Doll{\'a}r, and R.~Girshick,
  ``Early convolutions help transformers see better,'' \emph{NeurIPS}, 2021.

\bibitem{el2021large}
A.~El-Nouby, G.~Izacard, H.~Touvron, I.~Laptev, H.~Jegou, and E.~Grave, ``Are
  large-scale datasets necessary for self-supervised pre-training?''
  \emph{arXiv preprint arXiv:2112.10740}, 2021.

\bibitem{xie2022data}
Z.~Xie, Z.~Zhang, Y.~Cao, Y.~Lin, Y.~Wei, Q.~Dai, and H.~Hu, ``On data scaling
  in masked image modeling,'' \emph{arXiv preprint arXiv:2206.04664}, 2022.

\bibitem{yang2022domain}
H.~Yang, M.~Chen, Y.~Wang, S.~Tang, F.~Zhu, L.~Bai, R.~Zhao, and W.~Ouyang,
  ``Domain invariant masked autoencoders for self-supervised learning from
  multi-domains,'' \emph{arXiv preprint arXiv:2205.04771}, 2022.

\bibitem{bachmann2022multimae}
R.~Bachmann, D.~Mizrahi, A.~Atanov, and A.~Zamir, ``Multimae: Multi-modal
  multi-task masked autoencoders,'' \emph{arXiv preprint arXiv:2204.01678},
  2022.

\bibitem{tian2022beyond}
Y.~Tian, L.~Xie, J.~Fang, M.~Shi, J.~Peng, X.~Zhang, J.~Jiao, Q.~Tian, and
  Q.~Ye, ``Beyond masking: Demystifying token-based pre-training for vision
  transformers,'' \emph{arXiv preprint arXiv:2203.14313}, 2022.

\bibitem{xie2022masked}
J.~Xie, W.~Li, X.~Zhan, Z.~Liu, Y.~S. Ong, and C.~C. Loy, ``Masked frequency
  modeling for self-supervised visual pre-training,'' \emph{arXiv preprint
  arXiv:2206.07706}, 2022.

\bibitem{cao2022understand}
S.~Cao, P.~Xu, and D.~A. Clifton, ``How to understand masked autoencoders,''
  \emph{arXiv preprint arXiv:2202.03670}, 2022.

\bibitem{pan2022towards}
J.~Pan, P.~Zhou, and S.~Yan, ``Towards understanding why mask-reconstruction
  pretraining helps in downstream tasks,'' \emph{arXiv preprint
  arXiv:2206.03826}, 2022.

\bibitem{chen2021mocov3}
X.~Chen, S.~Xie, and K.~He, ``An empirical study of training self-supervised
  vision transformers,'' \emph{ICCV}, 2021.

\bibitem{grill2020bootstrap}
J.-B. Grill, F.~Strub, F.~Altch{\'e}, C.~Tallec, P.~Richemond, E.~Buchatskaya,
  C.~Doersch, B.~Avila~Pires, Z.~Guo, M.~Gheshlaghi~Azar \emph{et~al.},
  ``Bootstrap your own latent-a new approach to self-supervised learning,''
  \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem{misra2020self}
I.~Misra and L.~v.~d. Maaten, ``Self-supervised learning of pretext-invariant
  representations,'' in \emph{CVPR}, 2020.

\bibitem{tao2022siamese}
C.~Tao, X.~Zhu, G.~Huang, Y.~Qiao, X.~Wang, and J.~Dai, ``Siamese image
  modeling for self-supervised vision representation learning,'' \emph{arXiv
  preprint arXiv:2206.01204}, 2022.

\bibitem{li2021mst}
Z.~Li, Z.~Chen, F.~Yang, W.~Li, Y.~Zhu, C.~Zhao, R.~Deng, L.~Wu, R.~Zhao,
  M.~Tang \emph{et~al.}, ``Mst: Masked self-supervised transformer for visual
  representation,'' \emph{NeurIPS}, 2021.

\bibitem{wang2022repre}
L.~Wang, F.~Liang, Y.~Li, W.~Ouyang, H.~Zhang, and J.~Shao, ``Repre: Improving
  self-supervised vision transformer with reconstructive pre-training,''
  \emph{arXiv preprint arXiv:2201.06857}, 2022.

\bibitem{wei2022contrastive}
Y.~Wei, H.~Hu, Z.~Xie, Z.~Zhang, Y.~Cao, J.~Bao, D.~Chen, and B.~Guo,
  ``Contrastive learning rivals masked image modeling in fine-tuning via
  feature distillation,'' \emph{arXiv preprint arXiv:2205.14141}, 2022.

\bibitem{jing2022masked}
L.~Jing, J.~Zhu, and Y.~LeCun, ``Masked siamese convnets,'' \emph{arXiv
  preprint arXiv:2206.07700}, 2022.

\bibitem{zhou2022ibot}
J.~Zhou, C.~Wei, H.~Wang, W.~Shen, C.~Xie, A.~Yuille, and T.~Kong, ``ibot:
  Image bert pre-training with online tokenizer,'' \emph{ICLR}, 2022.

\bibitem{kakogeorgiou2022hide}
I.~Kakogeorgiou, S.~Gidaris, B.~Psomas, Y.~Avrithis, A.~Bursuc, K.~Karantzalos,
  and N.~Komodakis, ``What to hide from your students: Attention-guided masked
  image modeling,'' \emph{arXiv preprint arXiv:2203.12719}, 2022.

\bibitem{caron2020unsupervised}
M.~Caron, I.~Misra, J.~Mairal, P.~Goyal, P.~Bojanowski, and A.~Joulin,
  ``Unsupervised learning of visual features by contrasting cluster
  assignments,'' \emph{arXiv preprint arXiv:2006.09882}, 2020.

\bibitem{zhou2022self}
L.~Zhou, H.~Liu, J.~Bae, J.~He, D.~Samaras, and P.~Prasanna, ``Self
  pre-training with masked autoencoders for medical image analysis,''
  \emph{arXiv preprint arXiv:2203.05573}, 2022.

\bibitem{chen2022masked}
Z.~Chen, D.~Agarwal, K.~Aggarwal, W.~Safta, M.~M. Balan, V.~Sethuraman, and
  K.~Brown, ``Masked image modeling advances 3d medical image analysis,''
  \emph{arXiv preprint arXiv:2204.11716}, 2022.

\bibitem{ly2022student}
S.~T. Ly, B.~Lin, H.~Q. Vo, D.~Maric, B.~Roysam, and H.~V. Nguyen, ``Student
  collaboration improves self-supervised learning: Dual-loss adaptive masked
  autoencoder for brain cell image analysis,'' \emph{arXiv preprint
  arXiv:2205.05194}, 2022.

\bibitem{quan2022global}
H.~Quan, X.~Li, W.~Chen, M.~Zou, R.~Yang, T.~Zheng, R.~Qi, X.~Gao, and X.~Cui,
  ``Global contrast masked autoencoders are powerful pathological
  representation learners,'' \emph{arXiv preprint arXiv:2205.09048}, 2022.

\bibitem{an2022masked}
J.~An, Y.~Bai, H.~Chen, Z.~Gao, and G.~Litjens, ``Masked autoencoders
  pre-training in multiple instance learning for whole slide image
  classification,'' in \emph{Medical Imaging with Deep Learning}, 2022.

\bibitem{luo2022self}
Y.~Luo, Z.~Chen, and X.~Gao, ``Self-distillation augmented masked autoencoders
  for histopathological image classification,'' \emph{arXiv preprint
  arXiv:2203.16983}, 2022.

\bibitem{wang2022bevt}
R.~Wang, D.~Chen, Z.~Wu, Y.~Chen, X.~Dai, M.~Liu, Y.-G. Jiang, L.~Zhou, and
  L.~Yuan, ``Bevt: Bert pretraining of video transformers,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2022, pp. 14\,733--14\,743.

\bibitem{tan2021vimpac}
H.~Tan, J.~Lei, T.~Wolf, and M.~Bansal, ``Vimpac: Video pre-training via masked
  token prediction and contrastive learning,'' \emph{arXiv preprint
  arXiv:2106.11250}, 2021.

\bibitem{tong2022videomae}
Z.~Tong, Y.~Song, J.~Wang, and L.~Wang, ``Videomae: Masked autoencoders are
  data-efficient learners for self-supervised video pre-training,'' \emph{arXiv
  preprint arXiv:2203.12602}, 2022.

\bibitem{girdhar2022omnimae}
R.~Girdhar, A.~El-Nouby, M.~Singh, K.~V. Alwala, A.~Joulin, and I.~Misra,
  ``Omnimae: Single model masked pretraining on images and videos,''
  \emph{arXiv preprint arXiv:2206.08356}, 2022.

\bibitem{feichtenhofer2022masked}
C.~Feichtenhofer, H.~Fan, Y.~Li, and K.~He, ``Masked autoencoders as
  spatiotemporal learners,'' \emph{arXiv preprint arXiv:2205.09113}, 2022.

\bibitem{gupta2022maskvit}
A.~Gupta, S.~Tian, Y.~Zhang, J.~Wu, R.~Mart{\'\i}n-Mart{\'\i}n, and L.~Fei-Fei,
  ``Maskvit: Masked visual pre-training for video prediction,'' \emph{arXiv
  preprint arXiv:2206.11894}, 2022.

\bibitem{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark \emph{et~al.}, ``Learning transferable visual
  models from natural language supervision,'' in \emph{ICML}, 2021.

\bibitem{mu2021slip}
N.~Mu, A.~Kirillov, D.~Wagner, and S.~Xie, ``Slip: Self-supervision meets
  language-image pre-training,'' \emph{arXiv preprint arXiv:2112.12750}, 2021.

\bibitem{jia2021scaling}
C.~Jia, Y.~Yang, Y.~Xia, Y.-T. Chen, Z.~Parekh, H.~Pham, Q.~Le, Y.-H. Sung,
  Z.~Li, and T.~Duerig, ``Scaling up visual and vision-language representation
  learning with noisy text supervision,'' in \emph{ICML}, 2021.

\bibitem{bao2022vlmo}
H.~Bao, W.~Wang, L.~Dong, Q.~Liu, O.~K. Mohammed, K.~Aggarwal, S.~Som, and
  F.~Wei, ``Vlmo: Unified vision-language pre-training with
  mixture-of-modality-experts,'' \emph{arXiv preprint arXiv:2111.02358}, 2022.

\bibitem{geng2022multimodal}
X.~Geng, H.~Liu, L.~Lee, D.~Schuurams, S.~Levine, and P.~Abbeel, ``Multimodal
  masked autoencoders learn transferable representations,'' \emph{arXiv
  preprint arXiv:2205.14204}, 2022.

\bibitem{dou2022empirical}
Z.-Y. Dou, Y.~Xu, Z.~Gan, J.~Wang, S.~Wang, L.~Wang, C.~Zhu, P.~Zhang, L.~Yuan,
  N.~Peng \emph{et~al.}, ``An empirical study of training end-to-end
  vision-and-language transformers,'' in \emph{CVPR}, 2022.

\bibitem{bitton2021data}
Y.~Bitton, G.~Stanovsky, M.~Elhadad, and R.~Schwartz, ``Data efficient masked
  language modeling for vision and language,'' \emph{arXiv preprint
  arXiv:2109.02040}, 2021.

\bibitem{lu2022unified}
J.~Lu, C.~Clark, R.~Zellers, R.~Mottaghi, and A.~Kembhavi, ``Unified-io: A
  unified model for vision, language, and multi-modal tasks,'' \emph{arXiv
  preprint arXiv:2206.08916}, 2022.

\bibitem{bao2022vl}
H.~Bao, W.~Wang, L.~Dong, and F.~Wei, ``Vl-beit: Generative vision-language
  pretraining,'' \emph{arXiv preprint arXiv:2206.01127}, 2022.

\bibitem{pang2022masked}
Y.~Pang, W.~Wang, F.~E. Tay, W.~Liu, Y.~Tian, and L.~Yuan, ``Masked
  autoencoders for point cloud self-supervised learning,'' \emph{arXiv preprint
  arXiv:2203.06604}, 2022.

\bibitem{liu2022masked}
H.~Liu, M.~Cai, and Y.~J. Lee, ``Masked discrimination for self-supervised
  learning on point clouds,'' \emph{arXiv preprint arXiv:2203.11183}, 2022.

\bibitem{zhang2022point}
R.~Zhang, Z.~Guo, P.~Gao, R.~Fang, B.~Zhao, D.~Wang, Y.~Qiao, and H.~Li,
  ``Point-m2ae: Multi-scale masked autoencoders for hierarchical point cloud
  pre-training,'' \emph{arXiv preprint arXiv:2205.14401}, 2022.

\bibitem{min2022voxel}
C.~Min, D.~Zhao, L.~Xiao, Y.~Nie, and B.~Dai, ``Voxel-mae: Masked autoencoders
  for pre-training large-scale point clouds,'' \emph{arXiv e-prints}, pp.
  arXiv--2206, 2022.

\bibitem{tan2022mgae}
Q.~Tan, N.~Liu, X.~Huang, R.~Chen, S.-H. Choi, and X.~Hu, ``Mgae: Masked
  autoencoders for self-supervised learning on graphs,'' \emph{arXiv preprint
  arXiv:2201.02534}, 2022.

\bibitem{chen2022graph}
H.~Chen, S.~Zhang, and G.~Xu, ``Graph masked autoencoder,'' \emph{arXiv
  preprint arXiv:2202.08391}, 2022.

\bibitem{hou2022graphmae}
Z.~Hou, X.~Liu, Y.~Dong, C.~Wang, J.~Tang \emph{et~al.}, ``Graphmae:
  Self-supervised masked graph autoencoders,'' \emph{arXiv preprint
  arXiv:2205.10803}, 2022.

\bibitem{li2022maskgae}
J.~Li, R.~Wu, W.~Sun, L.~Chen, S.~Tian, L.~Zhu, C.~Meng, Z.~Zheng, and W.~Wang,
  ``Maskgae: Masked graph modeling meets graph autoencoders,'' \emph{arXiv
  preprint arXiv:2205.10053}, 2022.

\bibitem{haarnoja2018soft}
T.~Haarnoja, A.~Zhou, K.~Hartikainen, G.~Tucker, S.~Ha, J.~Tan, V.~Kumar,
  H.~Zhu, A.~Gupta, P.~Abbeel \emph{et~al.}, ``Soft actor-critic algorithms and
  applications,'' \emph{arXiv preprint arXiv:1812.05905}, 2018.

\bibitem{tao2022evaluating}
T.~Tao, D.~Reda, and M.~van~de Panne, ``Evaluating vision transformer methods
  for deep reinforcement learning from pixels,'' \emph{arXiv preprint
  arXiv:2204.04905}, 2022.

\bibitem{laskin2020reinforcement}
M.~Laskin, K.~Lee, A.~Stooke, L.~Pinto, P.~Abbeel, and A.~Srinivas,
  ``Reinforcement learning with augmented data,'' \emph{Advances in neural
  information processing systems}, vol.~33, pp. 19\,884--19\,895, 2020.

\bibitem{xiao2022masked}
T.~Xiao, I.~Radosavovic, T.~Darrell, and J.~Malik, ``Masked visual pre-training
  for motor control,'' \emph{arXiv preprint arXiv:2203.06173}, 2022.

\bibitem{schulman2017proximal}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov, ``Proximal
  policy optimization algorithms,'' \emph{arXiv preprint arXiv:1707.06347},
  2017.

\bibitem{seo2022masked}
Y.~Seo, D.~Hafner, H.~Liu, F.~Liu, S.~James, K.~Lee, and P.~Abbeel, ``Masked
  world models for visual control,'' \emph{arXiv preprint arXiv:2206.14244},
  2022.

\bibitem{niizumi2022masked}
D.~Niizumi, D.~Takeuchi, Y.~Ohishi, N.~Harada, and K.~Kashino, ``Masked
  spectrogram modeling using masked autoencoders for learning general-purpose
  audio representation,'' \emph{arXiv preprint arXiv:2204.12260}, 2022.

\bibitem{baade2022mae}
A.~Baade, P.~Peng, and D.~Harwath, ``Mae-ast: Masked autoencoding audio
  spectrogram transformer,'' \emph{arXiv preprint arXiv:2203.16691}, 2022.

\bibitem{zha2022time}
M.~Zha, ``Time series generation with masked autoencoder,'' \emph{arXiv
  preprint arXiv:2201.07006}, 2022.

\bibitem{majmundar2022met}
K.~Majmundar, S.~Goyal, P.~Netrapalli, and P.~Jain, ``Met: Masked encoding for
  tabular data,'' \emph{arXiv preprint arXiv:2206.08564}, 2022.

\bibitem{xu2022masked}
H.~Xu, S.~Ding, X.~Zhang, H.~Xiong, and Q.~Tian, ``Masked autoencoders are
  robust data augmentors,'' \emph{arXiv preprint arXiv:2206.04846}, 2022.

\bibitem{qi2017pointnet}
C.~R. Qi, H.~Su, K.~Mo, and L.~J. Guibas, ``Pointnet: Deep learning on point
  sets for 3d classification and segmentation,'' in \emph{CVPR}, 2017.

\bibitem{hamilton2017graphsage}
W.~L. Hamilton, R.~Ying, and J.~Leskovec, ``Inductive representation learning
  on large graphs,'' in \emph{NeurIPS}, 2017.

\bibitem{kipf2017gcn}
T.~N. Kipf and M.~Welling, ``Semi-supervised classification with graph
  convolutional networks,'' in \emph{ICLR}, 2017.

\bibitem{velivckovic2017gan}
P.~Veli{\v{c}}kovi{\'c}, G.~Cucurull, A.~Casanova, A.~Romero, P.~Lio, and
  Y.~Bengio, ``Graph attention networks,'' \emph{ICLR}, 2018.

\bibitem{xu2018gin}
K.~Xu, W.~Hu, J.~Leskovec, and S.~Jegelka, ``How powerful are graph neural
  networks?'' \emph{ICLR}, 2019.

\end{thebibliography}
