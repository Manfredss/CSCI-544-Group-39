In this paper, we have presented memory-based adapters for online 3D scene perception. Mainstream 3D scene perception methods are offline, which is hard to be applied in most real-time applications where only streaming RGB-D video is accessible. Existing online perception methods design model and task-specific temporal learning approaches, but most of them only focus on temporal aggregation for single modality and thus cannot fully exploit temporal relations between image and point cloud features. 
To this end, we propose plug-and-play temporal learning modules, which can empower offline methods with online perception ability by simply inserting memory-based adapters and finetuning on RGB-D videos. Specifically, given point cloud and image features extracted from the backbones, we first devise a queued memory mechanism to cache these information over time and maintaining a reasonable storage overhead. Then we devise aggregation modules which directly operate on the memory and pass temporal information from the cached features to current frame. As the global context of image features is limited due to the short queue, we further propose 3D-to-2D adapter to enhance image features with 3D memory.
We conduct extensive experiments on ScanNet and SceneNN. By equipping offline models with our modules, we achieve leading performance on three scene perception tasks compared with state-of-the-art online methods, even without any model and task-specific designs.