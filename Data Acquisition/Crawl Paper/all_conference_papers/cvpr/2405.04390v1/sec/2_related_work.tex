\section{Related Work}
\subsection{Pre-training for Autonomous Driving}
Based on input modalities, autonomous driving pre-training algorithms can be primarily categorized: pre-training on large-scale LiDAR point clouds~\cite{ad-pt,fei2023self,uni3d} and pre-training on images~\cite{liang2022effective,survey2,chen2021multisiam}. Pre-training algorithms for large-scale LiDAR point clouds can further be classified into contrastive learning methods~\cite{strl,gcc3d,proposalcontrast,bevcontrast,sheng2023contrastive,chen2022co}, masked autoencoder methods~\cite{occupancy-mae,voxel-mae,gd-mae,geomae,mv-jar}, and occupancy-based approaches~\cite{iscc,occupancy-mae,also,spot}. To incorporate 3D spatial structure into vision-centric autonomous driving, pre-training methods involving depth estimation have seen widespread adoption~\cite{dd3d,ppgeo}. OccNet~\cite{occnet}, UniScene~\cite{uniscene}, UniPAD~\cite{unipad}, and PonderV2~\cite{ponderv2} have introduced pre-training via 3D scene reconstruction. BEVDistill~\cite{bevdistill}, DistillBEV~\cite{distillbev}, and GeoMIM~\cite{geomim} employ knowledge distillation to transfer geometric insights from pre-trained LiDAR point clouds detection models. However, autonomous driving presents a 4D scene understanding challenge. We propose the first 4D pre-training approach based on world models for vision-centric autonomous driving.
\subsection{Spatial-Temporal Modeling for Autonomous Driving}
In the domain of autonomous driving, there has been significant research focus on spatio-temporal modeling. BEVFormer~\cite{bevformer} employs spatio-temporal transformers to learn BEV representations from multi-camera images. BEVDet4D~\cite{bevdet4d} extends BEVDet~\cite{bevdet} from spatial-only 3D space to the spatio-temporal 4D space. BEVStereo~\cite{bevstereo}, STS~\cite{sts}, and SOLOFusion~\cite{solofusion} address depth perception challenges in camera-based 3D tasks by leveraging temporal multi-view stereo (MVS)~\cite{mvs}. PETRv2~\cite{petrv2} and StreamPETR~\cite{streampetr} utilize sparse object queries to model moving objects and enable efficient transmission of long-term temporal information. ST-P3~\cite{stp3} and UniAD~\cite{uniad} are dedicated to building end-to-end vision-based autonomous driving systems through spatio-temporal feature learning.

\subsection{World Models}
World models enable intelligent agents to learn a state representation from past experiences and current observations, allowing them to predict future outcomes~\cite{world_models,lecun,dreamerv2}.
World models find extensive applications in reinforcement learning~\cite{dreamerv2,seo2023multi,pan2022iso}, and autonomous driving~\cite{gaia,wavbi,bogdoll2023exploring,gao2022enhance}. In reinforcement learning, \citet{world_models} proposed that the world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. Methods in ~\cite{recurrent_wm,dreamerv1,dreamerv2,dreamerv3} presuppose access to rewards and online interaction with the environment from predictions in the
compact latent space of a world model. ContextWM~\cite{contextwm} and SWIM~\cite{swim} pre-train world models with abundant in-the-wild videos for downstream visual tasks. 
In autonomous driving, \citet{occupancy} proposed the geometric occupancy grid as a world model for robot perception and navigation in 1989. MILE~\cite{mile} proposed to build the world model by predicting the future BEV segmentation from high-resolution videos of expert demonstrations for autonomous driving. GAIA-1~\cite{gaia} and DriveDreamer~\cite{drivedreamer} have constructed generative world models that harness video, text, and action inputs to create realistic driving scenarios using diffusion models. \citet{wavbi} builds unsupervised world models for the point cloud forecasting task in autonomous driving.
In this paper, we imbue the robot with a pre-trained spatio-temporal representation via world models to perceive surroundings and predict the future behaviour of other participants.