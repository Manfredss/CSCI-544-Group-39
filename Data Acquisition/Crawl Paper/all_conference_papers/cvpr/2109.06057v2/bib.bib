@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@article{yaghoubi_sss-pr_2021,
	title = {{SSS}-{PR}: {A} short survey of surveys in person re-identification},
	volume = {143},
	issn = {0167-8655},
	shorttitle = {{SSS}-{PR}},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865521000015},
	doi = {10.1016/j.patrec.2020.12.017},
	abstract = {Person re-identification (re-id) addresses the problem of whether “a query image corresponds to an identity in the database” and is believed to play a fundamental role in security enforcement in the near future, particularly in crowded urban environments. Due to many possibilities in selecting appropriate model architectures, datasets, and settings, the performance reported by the state-of-the-art re-id methods oscillates significantly among the published surveys. Therefore, it is difficult to understand the mainstream trends and emerging research difficulties in person re-id. This paper proposes a multi-dimensional taxonomy to categorize the most relevant researches according to different perspectives and tries to unify the categorization of re-id methods and fill the gap between the recently published surveys. Furthermore, we discuss the open challenges with a focus on privacy concerns and the issues caused by the exponential increase in the number of re-id publications over the recent years. Finally, we discuss several challenging directions for future studies.},
	language = {en},
	urldate = {2021-04-18},
	journal = {Pattern Recognition Letters},
	author = {Yaghoubi, Ehsan and Kumar, Aruna and Proença, Hugo},
	month = mar,
	year = {2021},
	keywords = {Person re-identification, Privacy and security, Visual surveillance},
	pages = {50--57},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\9TAZZA2A\\Yaghoubi et al. - 2021 - SSS-PR A short survey of surveys in person re-ide.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\VFKGKGDR\\S0167865521000015.html:text/html}
}

@article{ye_deep_2021,
	title = {Deep {Learning} for {Person} {Re}-identification: {A} {Survey} and {Outlook}},
	issn = {1939-3539},
	shorttitle = {Deep {Learning} for {Person} {Re}-identification},
	doi = {10.1109/TPAMI.2021.3054775},
	abstract = {Person re-identification (Re-ID) aims at retrieving a person of interest across multiple non-overlapping cameras. With the advancement of deep neural networks and increasing demand of intelligent video surveillance, it has gained significantly increased interest in the computer vision community. By dissecting the involved components in developing a person Re-ID system, we categorize it into the closed-world and open-world settings. We first conduct a comprehensive overview with in-depth analysis for closed-world person Re-ID from three different perspectives, including deep feature representation learning, deep metric learning and ranking optimization. With the performance saturation under closed-world setting, the research focus for person Re-ID has recently shifted to the open-world setting, facing more challenging issues. This setting is closer to practical applications under specific scenarios. We summarize the open-world Re-ID in terms of five different aspects. By analyzing the advantages of existing methods, we design a powerful AGW baseline, achieving state-of-the-art or at least comparable performance on twelve datasets for four different Re-ID tasks. Meanwhile, we introduce a new evaluation metric (mINP) for person Re-ID, indicating the cost for finding all the correct matches, which provides an additional criterion to evaluate the Re-ID system. Finally, some important yet under-investigated open issues are discussed.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ye, M. and Shen, J. and Lin, G. and Xiang, T. and Shao, L. and Hoi, S. C. H.},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Annotations, Cameras, Data models, Deep learning, Deep Learning, Evaluation Metric, Feature extraction, Literature Survey, Pedestrian Retrieval, Person Re-Identification, Training, Training data},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\C49WLXN3\\9336268.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\GWR5L4LB\\Ye et al. - 2021 - Deep Learning for Person Re-identification A Surv.pdf:application/pdf}
}

@article{zheng_person_2016,
	title = {Person {Re}-identification: {Past}, {Present} and {Future}},
	shorttitle = {Person {Re}-identification},
	url = {http://arxiv.org/abs/1610.02984},
	abstract = {Person re-identification (re-ID) has become increasingly popular in the community due to its application and research significance. It aims at spotting a person of interest in other cameras. In the early days, hand-crafted algorithms and small-scale evaluation were predominantly reported. Recent years have witnessed the emergence of large-scale datasets and deep learning systems which make use of large data volumes. Considering different tasks, we classify most current re-ID methods into two classes, i.e., image-based and video-based; in both tasks, hand-crafted and deep learning systems will be reviewed. Moreover, two new re-ID tasks which are much closer to real-world applications are described and discussed, i.e., end-to-end re-ID and fast re-ID in very large galleries. This paper: 1) introduces the history of person re-ID and its relationship with image classification and instance retrieval; 2) surveys a broad selection of the hand-crafted systems and the large-scale methods in both image- and video-based re-ID; 3) describes critical future directions in end-to-end re-ID and fast retrieval in large galleries; and 4) finally briefs some important yet under-developed issues.},
	urldate = {2021-04-18},
	journal = {arXiv:1610.02984 [cs]},
	author = {Zheng, Liang and Yang, Yi and Hauptmann, Alexander G.},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.02984},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\4E5MC3JQ\\Zheng et al. - 2016 - Person Re-identification Past, Present and Future.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\HPDYWSQR\\1610.html:text/html}
}

@inproceedings{liu_person_2012,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Person {Re}-identification: {What} {Features} {Are} {Important}?},
	isbn = {978-3-642-33863-2},
	shorttitle = {Person {Re}-identification},
	doi = {10.1007/978-3-642-33863-2_39},
	abstract = {State-of-the-art person re-identification methods seek robust person matching through combining various feature types. Often, these features are implicitly assigned with a single vector of global weights, which are assumed to be universally good for all individuals, independent to their different appearances. In this study, we show that certain features play more important role than others under different circumstances. Consequently, we propose a novel unsupervised approach for learning a bottom-up feature importance, so features extracted from different individuals are weighted adaptively driven by their unique and inherent appearance attributes. Extensive experiments on two public datasets demonstrate that attribute-sensitive feature importance facilitates more accurate person matching when it is fused together with global weights obtained using existing methods.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2012. {Workshops} and {Demonstrations}},
	publisher = {Springer},
	author = {Liu, Chunxiao and Gong, Shaogang and Loy, Chen Change and Lin, Xinggang},
	editor = {Fusiello, Andrea and Murino, Vittorio and Cucchiara, Rita},
	year = {2012},
	keywords = {Feature Importance, Gabor Feature, Pairwise Constraint, Random Forest, Unsupervised Approach},
	pages = {391--401},
	file = {Springer Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\HAPPJUJ6\\Liu et al. - 2012 - Person Re-identification What Features Are Import.pdf:application/pdf}
}

@incollection{gong_person_2011,
	address = {London},
	title = {Person {Re}-identification},
	isbn = {978-0-85729-670-2},
	url = {https://doi.org/10.1007/978-0-85729-670-2_14},
	abstract = {A fundamental task for a distributed multi-camera system is to associate people across camera views at different locations and times. In a crowded and uncontrolled environment observed by cameras from a distance, person re-identification by biometrics such as face and gait is infeasible due to insufficient image details and arbitrary viewing conditions. Visual appearance features, extracted mainly from clothing, are intrinsically weak for matching people. For instance, most people in public spaces wear dark clothes in winter. A person’s appearance can also change significantly between different camera views if large changes occur in view angle, lighting, background clutter and occlusion. This results in different people appearing more alike than that of the same person across different camera views. In this chapter, we describe a method for learning the optimal matching distance criterion, regardless feature representation. This approach to person re-identification shifts the burden of computation from finding some universally optimal imagery features to discovering a matching mechanism for selecting adaptively different features that are locally optimal for each and every pairs of matches. Moreover, behaviour correlations hold useful spatio-temporal contextual information about expectations on where and when a person may re-appear in a networked visible space. This information is utilised for improving matching accuracy through context-aware search.},
	language = {en},
	urldate = {2021-04-18},
	booktitle = {Visual {Analysis} of {Behaviour}: {From} {Pixels} to {Semantics}},
	publisher = {Springer},
	author = {Gong, Shaogang and Xiang, Tao},
	editor = {Gong, Shaogang and Xiang, Tao},
	year = {2011},
	doi = {10.1007/978-0-85729-670-2_14},
	pages = {301--313},
	file = {Springer Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\CN9CFTBB\\Gong and Xiang - 2011 - Person Re-identification.pdf:application/pdf}
}

@article{leng_survey_2020,
	title = {A {Survey} of {Open}-{World} {Person} {Re}-{Identification}},
	volume = {30},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2019.2898940},
	abstract = {Person re-identification (re-ID) has been a popular topic in computer vision and pattern recognition communities for a decade. Several important milestones such as metric-based and deeply-learned re-ID in recent years have promoted this topic. However, most existing re-ID works are designed for closed-world scenarios rather than realistic open-world settings, which limits the practical application of the re-ID technique. On one hand, the performance of the latest re-ID methods has surpassed the human-level performance on several commonly used benchmarks (e.g., Market1501 and CUHK03), which are collected from closed-world scenarios. On the other hand, open-world tasks that are less developed and more challenging have received increasing attention in the re-ID community. Therefore, this paper starts the first attempt to analyze the trends of open-world re-ID and summarizes them from both narrow and generalized perspectives. In the narrow perspective, open-world re-ID is regarded as person verification (i.e., open-set re-ID) instead of person identification, that is, the query person may not occur in the gallery set. In the generalized perspective, application-driven methods that are designed for specific applications are defined as generalized open-world re-ID. Their settings are usually close to realistic application requirements. Specifically, this survey mainly includes the following four points for open-world re-ID: 1) analyzing the discrepancies between closed- and open-world scenarios; 2) describing the developments of existing open-set re-ID works and their limitations; 3) introducing specific application-driven works from three aspects, namely, raw data, practical procedure, and efficiency; and 4) summarizing the state-of-the-art methods and future directions for open-world re-ID. This survey on open-world re-ID provides a guidance for improving the usability of re-ID technique in practical applications.},
	number = {4},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Leng, Q. and Ye, M. and Tian, Q.},
	month = apr,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {Person re-identification, Cameras, Feature extraction, Benchmark testing, closed-world, Market research, Measurement, open-set, open-world, Probes, specific application-driven, Task analysis},
	pages = {1092--1108},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\JULS657A\\Leng et al. - 2020 - A Survey of Open-World Person Re-Identification.pdf:application/pdf}
}

@article{lavi_survey_2018,
	title = {Survey on {Deep} {Learning} {Techniques} for {Person} {Re}-{Identification} {Task}},
	url = {http://arxiv.org/abs/1807.05284},
	abstract = {Intelligent video-surveillance is currently an active research field in computer vision and machine learning techniques. It provides useful tools for surveillance operators and forensic video investigators. Person re-identification (PReID) is one among these tools. It consists of recognizing whether an individual has already been observed over a camera in a network or not. This tool can also be employed in various possible applications such as off-line retrieval of all the video-sequences showing an individual of interest whose image is given a query, and online pedestrian tracking over multiple camera views. To this aim, many techniques have been proposed to increase the performance of PReID. Among the systems, many researchers utilized deep neural networks (DNNs) because of their better performance and fast execution at test time. Our objective is to provide for future researchers the work being done on PReID to date. Therefore, we summarized state-of-the-art DNN models being used for this task. A brief description of each model along with their evaluation on a set of benchmark datasets is given. Finally, a detailed comparison is provided among these models followed by some limitations that can work as guidelines for future research.},
	urldate = {2021-04-18},
	journal = {arXiv:1807.05284 [cs]},
	author = {Lavi, Bahram and Serj, Mehdi Fatan and Ullah, Ihsan},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.05284},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\CV3GHJA7\\Lavi et al. - 2018 - Survey on Deep Learning Techniques for Person Re-I.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\CYNSGLIS\\1807.html:text/html}
}

@article{wang_beyond_2020,
	title = {Beyond {Intra}-modality: {A} {Survey} of {Heterogeneous} {Person} {Re}-identification},
	shorttitle = {Beyond {Intra}-modality},
	url = {http://arxiv.org/abs/1905.10048},
	abstract = {An efficient and effective person re-identification (ReID) system relieves the users from painful and boring video watching and accelerates the process of video analysis. Recently, with the explosive demands of practical applications, a lot of research efforts have been dedicated to heterogeneous person re-identification (Hetero-ReID). In this paper, we provide a comprehensive review of state-of-the-art Hetero-ReID methods that address the challenge of inter-modality discrepancies. According to the application scenario, we classify the methods into four categories -- low-resolution, infrared, sketch, and text. We begin with an introduction of ReID, and make a comparison between Homogeneous ReID (Homo-ReID) and Hetero-ReID tasks. Then, we describe and compare existing datasets for performing evaluations, and survey the models that have been widely employed in Hetero-ReID. We also summarize and compare the representative approaches from two perspectives, i.e., the application scenario and the learning pipeline. We conclude by a discussion of some future research directions. Follow-up updates are avaible at: https://github.com/lightChaserX/Awesome-Hetero-reID},
	urldate = {2021-04-18},
	journal = {arXiv:1905.10048 [cs]},
	author = {Wang, Zheng and Wang, Zhixiang and Zheng, Yinqiang and Wu, Yang and Zeng, Wenjun and Satoh, Shin'ichi},
	month = apr,
	year = {2020},
	note = {arXiv: 1905.10048},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\NF3UCWPD\\Wang et al. - 2020 - Beyond Intra-modality A Survey of Heterogeneous P.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\KW3VHF2L\\1905.html:text/html}
}

@article{almasawa_survey_2019,
	title = {A {Survey} on {Deep} {Learning}-{Based} {Person} {Re}-{Identification} {Systems}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2957336},
	abstract = {Person re-identification systems (person Re-ID) have recently gained more attention between computer vision researchers. They are playing a key role in intelligent visual surveillance systems and have widespread applications like applications for public security. The person Re-ID systems can identify if a person has been seen by a non-overlapping camera over large camera network in an unconstrained environment. It is a challenging issue since a person appears differently under different camera views and faces many challenges such as pose variation, occlusion and illumination changes. Many methods had been introduced for generating handcrafted features aimed to handle the person Re-ID problem. In recent years, many studies have started to apply deep learning methods to enhance the person Re-ID performance due the deep learning yielded significant results in computer vision issues. Therefore, this paper is a survey of the recent studies that proposed to improve the person Re-ID systems using deep learning. The public datasets that are used for evaluating these systems are discussed. Finally, the paper addresses future directions and current issues that must be considered toward improving the person Re-ID systems.},
	journal = {IEEE Access},
	author = {Almasawa, M. O. and Elrefaei, L. A. and Moria, K.},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Cameras, Deep learning, Feature extraction, Measurement, Probes, Task analysis, Machine learning, person re-identification, Robustness, video surveillance},
	pages = {175228--175247},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\JBHQWQSA\\8920082.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\ABJMSQW8\\Almasawa et al. - 2019 - A Survey on Deep Learning-Based Person Re-Identifi.pdf:application/pdf}
}

@inproceedings{wang_non-local_2018,
	title = {Non-{Local} {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
	year = {2018},
	pages = {7794--7803},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\H56HQIPF\\Wang et al. - 2018 - Non-Local Neural Networks.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\7EZK6G4S\\Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html:text/html}
}

@article{radenovic_fine-tuning_2019,
	title = {Fine-{Tuning} {CNN} {Image} {Retrieval} with {No} {Human} {Annotation}},
	volume = {41},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2018.2846566},
	abstract = {Image descriptors based on activations of Convolutional Neural Networks (CNNs) have become dominant in image retrieval due to their discriminative power, compactness of representation, and search efficiency. Training of CNNs, either from scratch or fine-tuning, requires a large amount of annotated data, where a high quality of annotation is often crucial. In this work, we propose to fine-tune CNNs for image retrieval on a large collection of unordered images in a fully automated manner. Reconstructed 3D models obtained by the state-of-the-art retrieval and structure-from-motion methods guide the selection of the training data. We show that both hard-positive and hard-negative examples, selected by exploiting the geometry and the camera positions available from the 3D models, enhance the performance of particular-object retrieval. CNN descriptor whitening discriminatively learned from the same training data outperforms commonly used PCA whitening. We propose a novel trainable Generalized-Mean (GeM) pooling layer that generalizes max and average pooling and show that it boosts retrieval performance. Applying the proposed method to the VGG network achieves state-of-the-art performance on the standard benchmarks: Oxford Buildings, Paris, and Holidays datasets.},
	number = {7},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Radenović, F. and Tolias, G. and Chum, O.},
	month = jul,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Image retrieval, Training, Training data, Task analysis, Image representation, processing and computer vision, computing methodologies, neural nets, pattern recognition, applications, Solid modeling, Standards, Three-dimensional displays},
	pages = {1655--1668},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\ZTC5B5ZE\\8382272.html:text/html;Submitted Version:C\:\\Users\\lxt76\\Zotero\\storage\\FUL33N5R\\Radenović et al. - 2019 - Fine-Tuning CNN Image Retrieval with No Human Anno.pdf:application/pdf}
}

@inproceedings{zheng_person_2017,
	title = {Person {Re}-{Identification} in the {Wild}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Zheng_Person_Re-Identification_in_CVPR_2017_paper.html},
	urldate = {2021-04-20},
	author = {Zheng, Liang and Zhang, Hengheng and Sun, Shaoyan and Chandraker, Manmohan and Yang, Yi and Tian, Qi},
	year = {2017},
	pages = {1367--1376},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\QXM8Y47X\\Zheng et al. - 2017 - Person Re-Identification in the Wild.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\W5QMYHEF\\Zheng_Person_Re-Identification_in_CVPR_2017_paper.html:text/html}
}

@inproceedings{zhou_online_2020,
	title = {Online {Joint} {Multi}-{Metric} {Adaptation} {From} {Frequent} {Sharing}-{Subset} {Mining} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Zhou_Online_Joint_Multi-Metric_Adaptation_From_Frequent_Sharing-Subset_Mining_for_Person_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Zhou, Jiahuan and Su, Bing and Wu, Ying},
	year = {2020},
	pages = {2909--2918},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\ZN95ABD2\\Zhou et al. - 2020 - Online Joint Multi-Metric Adaptation From Frequent.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\6B68KC66\\Zhou_Online_Joint_Multi-Metric_Adaptation_From_Frequent_Sharing-Subset_Mining_for_Person_CVPR_2.html:text/html}
}

@inproceedings{jin_style_2020,
	title = {Style {Normalization} and {Restitution} for {Generalizable} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Jin_Style_Normalization_and_Restitution_for_Generalizable_Person_Re-Identification_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Jin, Xin and Lan, Cuiling and Zeng, Wenjun and Chen, Zhibo and Zhang, Li},
	year = {2020},
	pages = {3143--3152},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\4C57SVRG\\Jin et al. - 2020 - Style Normalization and Restitution for Generaliza.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\VYBPVL75\\Jin_Style_Normalization_and_Restitution_for_Generalizable_Person_Re-Identification_CVPR_2020_pa.html:text/html}
}

@inproceedings{lin_unsupervised_2020,
	title = {Unsupervised {Person} {Re}-{Identification} via {Softened} {Similarity} {Learning}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Lin_Unsupervised_Person_Re-Identification_via_Softened_Similarity_Learning_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Lin, Yutian and Xie, Lingxi and Wu, Yu and Yan, Chenggang and Tian, Qi},
	year = {2020},
	pages = {3390--3399},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\7UDERYCD\\Lin et al. - 2020 - Unsupervised Person Re-Identification via Softened.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\ANTQQNJK\\Lin_Unsupervised_Person_Re-Identification_via_Softened_Similarity_Learning_CVPR_2020_paper.html:text/html}
}

@inproceedings{wang_transferable_2020,
	title = {Transferable, {Controllable}, and {Inconspicuous} {Adversarial} {Attacks} on {Person} {Re}-identification {With} {Deep} {Mis}-{Ranking}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Transferable_Controllable_and_Inconspicuous_Adversarial_Attacks_on_Person_Re-identification_With_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Wang, Hongjun and Wang, Guangrun and Li, Ya and Zhang, Dongyu and Lin, Liang},
	year = {2020},
	pages = {342--351},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\HQPAVUPZ\\Wang et al. - 2020 - Transferable, Controllable, and Inconspicuous Adve.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\QIVR6AZL\\Wang_Transferable_Controllable_and_Inconspicuous_Adversarial_Attacks_on_Person_Re-identificatio.html:text/html}
}

@inproceedings{cheng_inter-task_2020,
	title = {Inter-{Task} {Association} {Critic} for {Cross}-{Resolution} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Inter-Task_Association_Critic_for_Cross-Resolution_Person_Re-Identification_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Cheng, Zhiyi and Dong, Qi and Gong, Shaogang and Zhu, Xiatian},
	year = {2020},
	pages = {2605--2615},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\MKYUYWSF\\Cheng et al. - 2020 - Inter-Task Association Critic for Cross-Resolution.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\XXKXE5CN\\Cheng_Inter-Task_Association_Critic_for_Cross-Resolution_Person_Re-Identification_CVPR_2020_pap.html:text/html}
}

@inproceedings{yan_learning_2020,
	title = {Learning {Multi}-{Granular} {Hypergraphs} for {Video}-{Based} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Yan_Learning_Multi-Granular_Hypergraphs_for_Video-Based_Person_Re-Identification_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Yan, Yichao and Qin, Jie and Chen, Jiaxin and Liu, Li and Zhu, Fan and Tai, Ying and Shao, Ling},
	year = {2020},
	pages = {2899--2908},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\8F5PRAQL\\Yan et al. - 2020 - Learning Multi-Granular Hypergraphs for Video-Base.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\5V3BCNZF\\Yan_Learning_Multi-Granular_Hypergraphs_for_Video-Based_Person_Re-Identification_CVPR_2020_pape.html:text/html}
}

@inproceedings{zhang_relation-aware_2020,
	title = {Relation-{Aware} {Global} {Attention} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Relation-Aware_Global_Attention_for_Person_Re-Identification_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Zhang, Zhizheng and Lan, Cuiling and Zeng, Wenjun and Jin, Xin and Chen, Zhibo},
	year = {2020},
	pages = {3186--3195},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\VE2KLN9A\\Zhang et al. - 2020 - Relation-Aware Global Attention for Person Re-Iden.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\RHRUJYT6\\Zhang_Relation-Aware_Global_Attention_for_Person_Re-Identification_CVPR_2020_paper.html:text/html}
}

@inproceedings{yang_spatial-temporal_2020,
	title = {Spatial-{Temporal} {Graph} {Convolutional} {Network} for {Video}-{Based} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Yang_Spatial-Temporal_Graph_Convolutional_Network_for_Video-Based_Person_Re-Identification_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Yang, Jinrui and Zheng, Wei-Shi and Yang, Qize and Chen, Ying-Cong and Tian, Qi},
	year = {2020},
	pages = {3289--3299},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\692U6FDZ\\Yang et al. - 2020 - Spatial-Temporal Graph Convolutional Network for V.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\CLV86FB8\\Yang_Spatial-Temporal_Graph_Convolutional_Network_for_Video-Based_Person_Re-Identification_CVPR.html:text/html}
}

@inproceedings{chen_salience-guided_2020,
	title = {Salience-{Guided} {Cascaded} {Suppression} {Network} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Salience-Guided_Cascaded_Suppression_Network_for_Person_Re-Identification_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Chen, Xuesong and Fu, Canmiao and Zhao, Yong and Zheng, Feng and Song, Jingkuan and Ji, Rongrong and Yang, Yi},
	year = {2020},
	pages = {3300--3310},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\CKN6MBFK\\Chen et al. - 2020 - Salience-Guided Cascaded Suppression Network for P.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\S69PRZNE\\Chen_Salience-Guided_Cascaded_Suppression_Network_for_Person_Re-Identification_CVPR_2020_paper.html:text/html}
}


@inproceedings{zhai_ad-cluster_2020,
	title = {{AD}-{Cluster}: {Augmented} {Discriminative} {Clustering} for {Domain} {Adaptive} {Person} {Re}-{Identification}},
	shorttitle = {{AD}-{Cluster}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Zhai_AD-Cluster_Augmented_Discriminative_Clustering_for_Domain_Adaptive_Person_Re-Identification_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Zhai, Yunpeng and Lu, Shijian and Ye, Qixiang and Shan, Xuebo and Chen, Jie and Ji, Rongrong and Tian, Yonghong},
	year = {2020},
	pages = {9021--9030},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\B2CKISRJ\\Zhai et al. - 2020 - AD-Cluster Augmented Discriminative Clustering fo.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\7UECPSFY\\Zhai_AD-Cluster_Augmented_Discriminative_Clustering_for_Domain_Adaptive_Person_Re-Identificatio.html:text/html}
}

@inproceedings{liu_unity_2020,
	title = {Unity {Style} {Transfer} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Liu_Unity_Style_Transfer_for_Person_Re-Identification_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Liu, Chong and Chang, Xiaojun and Shen, Yi-Dong},
	year = {2020},
	pages = {6887--6896},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\82N8HGEV\\Liu et al. - 2020 - Unity Style Transfer for Person Re-Identification.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\WZCGTPSE\\Liu_Unity_Style_Transfer_for_Person_Re-Identification_CVPR_2020_paper.html:text/html}
}

@inproceedings{wang_high-order_2020,
	title = {High-{Order} {Information} {Matters}: {Learning} {Relation} and {Topology} for {Occluded} {Person} {Re}-{Identification}},
	shorttitle = {High-{Order} {Information} {Matters}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_High-Order_Information_Matters_Learning_Relation_and_Topology_for_Occluded_Person_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Wang, Guan'an and Yang, Shuo and Liu, Huanyu and Wang, Zhicheng and Yang, Yang and Wang, Shuliang and Yu, Gang and Zhou, Erjin and Sun, Jian},
	year = {2020},
	pages = {6449--6458},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\JMTSP6UH\\Wang et al. - 2020 - High-Order Information Matters Learning Relation .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\PVAVV9CA\\Wang_High-Order_Information_Matters_Learning_Relation_and_Topology_for_Occluded_Person_CVPR_202.html:text/html}
}

@inproceedings{choi_hi-cmd_2020,
	title = {Hi-{CMD}: {Hierarchical} {Cross}-{Modality} {Disentanglement} for {Visible}-{Infrared} {Person} {Re}-{Identification}},
	shorttitle = {Hi-{CMD}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Choi_Hi-CMD_Hierarchical_Cross-Modality_Disentanglement_for_Visible-Infrared_Person_Re-Identification_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Choi, Seokeon and Lee, Sumin and Kim, Youngeun and Kim, Taekyung and Kim, Changick},
	year = {2020},
	pages = {10257--10266},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\NZRNMX5S\\Choi et al. - 2020 - Hi-CMD Hierarchical Cross-Modality Disentanglemen.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\D2J24WX7\\Choi_Hi-CMD_Hierarchical_Cross-Modality_Disentanglement_for_Visible-Infrared_Person_Re-Identifi.html:text/html}
}

@inproceedings{fan_learning_2020,
	title = {Learning {Longterm} {Representations} for {Person} {Re}-{Identification} {Using} {Radio} {Signals}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Fan_Learning_Longterm_Representations_for_Person_Re-Identification_Using_Radio_Signals_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Fan, Lijie and Li, Tianhong and Fang, Rongyao and Hristov, Rumen and Yuan, Yuan and Katabi, Dina},
	year = {2020},
	pages = {10699--10709},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\WRLNDYUX\\Fan et al. - 2020 - Learning Longterm Representations for Person Re-Id.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\4M8K74L5\\Fan_Learning_Longterm_Representations_for_Person_Re-Identification_Using_Radio_Signals_CVPR_202.html:text/html}
}

@inproceedings{zhang_multi-granularity_2020,
	title = {Multi-{Granularity} {Reference}-{Aided} {Attentive} {Feature} {Aggregation} for {Video}-{Based} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Multi-Granularity_Reference-Aided_Attentive_Feature_Aggregation_for_Video-Based_Person_Re-Identification_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Zhang, Zhizheng and Lan, Cuiling and Zeng, Wenjun and Chen, Zhibo},
	year = {2020},
	pages = {10407--10416},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\C4RTJ2ZP\\Zhang et al. - 2020 - Multi-Granularity Reference-Aided Attentive Featur.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\AMQW54U4\\Zhang_Multi-Granularity_Reference-Aided_Attentive_Feature_Aggregation_for_Video-Based_Person_Re.html:text/html}
}

@inproceedings{ahmed_camera_2020,
	title = {Camera {On}-{Boarding} for {Person} {Re}-{Identification} {Using} {Hypothesis} {Transfer} {Learning}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Ahmed_Camera_On-Boarding_for_Person_Re-Identification_Using_Hypothesis_Transfer_Learning_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Ahmed, Sk Miraj and Lejbolle, Aske R. and Panda, Rameswar and Roy-Chowdhury, Amit K.},
	year = {2020},
	pages = {12144--12153},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\QTMUZIHJ\\Ahmed et al. - 2020 - Camera On-Boarding for Person Re-Identification Us.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\M2NEJLVV\\Ahmed_Camera_On-Boarding_for_Person_Re-Identification_Using_Hypothesis_Transfer_Learning_CVPR_2.html:text/html}
}

@inproceedings{lu_cross-modality_2020,
	title = {Cross-{Modality} {Person} {Re}-{Identification} {With} {Shared}-{Specific} {Feature} {Transfer}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Lu_Cross-Modality_Person_Re-Identification_With_Shared-Specific_Feature_Transfer_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Lu, Yan and Wu, Yue and Liu, Bin and Zhang, Tianzhu and Li, Baopu and Chu, Qi and Yu, Nenghai},
	year = {2020},
	pages = {13379--13389},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\CJKT5Y7X\\Lu et al. - 2020 - Cross-Modality Person Re-Identification With Share.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\JY5ZWPVY\\Lu_Cross-Modality_Person_Re-Identification_With_Shared-Specific_Feature_Transfer_CVPR_2020_pape.html:text/html}
}

@inproceedings{zeng_hierarchical_2020,
	title = {Hierarchical {Clustering} {With} {Hard}-{Batch} {Triplet} {Loss} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Zeng_Hierarchical_Clustering_With_Hard-Batch_Triplet_Loss_for_Person_Re-Identification_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Zeng, Kaiwei and Ning, Munan and Wang, Yaohua and Guo, Yang},
	year = {2020},
	pages = {13657--13665},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\37YEL7EL\\Zeng et al. - 2020 - Hierarchical Clustering With Hard-Batch Triplet Lo.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\5T3DUGWV\\Zeng_Hierarchical_Clustering_With_Hard-Batch_Triplet_Loss_for_Person_Re-Identification_CVPR_202.html:text/html}
}

@inproceedings{huang_real-world_2020,
	title = {Real-{World} {Person} {Re}-{Identification} via {Degradation} {Invariance} {Learning}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Huang_Real-World_Person_Re-Identification_via_Degradation_Invariance_Learning_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Huang, Yukun and Zha, Zheng-Jun and Fu, Xueyang and Hong, Richang and Li, Liang},
	year = {2020},
	pages = {14084--14094},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\6PPQJ3BC\\Huang et al. - 2020 - Real-World Person Re-Identification via Degradatio.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\WKKEV9G5\\Huang_Real-World_Person_Re-Identification_via_Degradation_Invariance_Learning_CVPR_2020_paper.html:text/html}
}

@inproceedings{wang_unsupervised_2020,
	title = {Unsupervised {Person} {Re}-{Identification} via {Multi}-{Label} {Classification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Unsupervised_Person_Re-Identification_via_Multi-Label_Classification_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Wang, Dongkai and Zhang, Shiliang},
	year = {2020},
	pages = {10981--10990},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\EZIAXNDU\\Wang and Zhang - 2020 - Unsupervised Person Re-Identification via Multi-La.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\DFAL36YC\\Wang_Unsupervised_Person_Re-Identification_via_Multi-Label_Classification_CVPR_2020_paper.html:text/html}
}

@inproceedings{wang_smoothing_2020,
	title = {Smoothing {Adversarial} {Domain} {Attack} and {P}-{Memory} {Reconsolidation} for {Cross}-{Domain} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Smoothing_Adversarial_Domain_Attack_and_P-Memory_Reconsolidation_for_Cross-Domain_Person_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Wang, Guangcong and Lai, Jian-Huang and Liang, Wenqi and Wang, Guangrun},
	year = {2020},
	pages = {10568--10577},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\HXWDAMYQ\\Wang et al. - 2020 - Smoothing Adversarial Domain Attack and P-Memory R.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\CKK3YGIS\\Wang_Smoothing_Adversarial_Domain_Attack_and_P-Memory_Reconsolidation_for_Cross-Domain_Person_C.html:text/html}
}

@inproceedings{gao_pose-guided_2020,
	title = {Pose-{Guided} {Visible} {Part} {Matching} for {Occluded} {Person} {ReID}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Gao_Pose-Guided_Visible_Part_Matching_for_Occluded_Person_ReID_CVPR_2020_paper.html},
	urldate = {2021-04-20},
	author = {Gao, Shang and Wang, Jingya and Lu, Huchuan and Liu, Zimo},
	year = {2020},
	pages = {11744--11752},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\VQDK253T\\Gao et al. - 2020 - Pose-Guided Visible Part Matching for Occluded Per.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\PR8U2MAF\\Gao_Pose-Guided_Visible_Part_Matching_for_Occluded_Person_ReID_CVPR_2020_paper.html:text/html}
}

@inproceedings{chen_instance-guided_2019,
	title = {Instance-{Guided} {Context} {Rendering} for {Cross}-{Domain} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Instance-Guided_Context_Rendering_for_Cross-Domain_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Chen, Yanbei and Zhu, Xiatian and Gong, Shaogang},
	year = {2019},
	pages = {232--242},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\PDXV2UN9\\Chen et al. - 2019 - Instance-Guided Context Rendering for Cross-Domain.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\RRMA5UBL\\Chen_Instance-Guided_Context_Rendering_for_Cross-Domain_Person_Re-Identification_ICCV_2019_pape.html:text/html}
}

@inproceedings{chen_mixed_2019,
	title = {Mixed {High}-{Order} {Attention} {Network} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Mixed_High-Order_Attention_Network_for_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Chen, Binghui and Deng, Weihong and Hu, Jiani},
	year = {2019},
	pages = {371--381},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\4QGKAF7N\\Chen et al. - 2019 - Mixed High-Order Attention Network for Person Re-I.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\69KWIQQS\\Chen_Mixed_High-Order_Attention_Network_for_Person_Re-Identification_ICCV_2019_paper.html:text/html}
}

@inproceedings{miao_pose-guided_2019,
	title = {Pose-{Guided} {Feature} {Alignment} for {Occluded} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Miao_Pose-Guided_Feature_Alignment_for_Occluded_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Miao, Jiaxu and Wu, Yu and Liu, Ping and Ding, Yuhang and Yang, Yi},
	year = {2019},
	pages = {542--551},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\4HQZ49YE\\Miao et al. - 2019 - Pose-Guided Feature Alignment for Occluded Person .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\TSNLRQND\\Miao_Pose-Guided_Feature_Alignment_for_Occluded_Person_Re-Identification_ICCV_2019_paper.html:text/html}
}

@inproceedings{yu_robust_2019,
	title = {Robust {Person} {Re}-{Identification} by {Modelling} {Feature} {Uncertainty}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Robust_Person_Re-Identification_by_Modelling_Feature_Uncertainty_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Yu, Tianyuan and Li, Da and Yang, Yongxin and Hospedales, Timothy M. and Xiang, Tao},
	year = {2019},
	pages = {552--561},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\87B332FI\\Yu et al. - 2019 - Robust Person Re-Identification by Modelling Featu.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\TGXQHA4A\\Yu_Robust_Person_Re-Identification_by_Modelling_Feature_Uncertainty_ICCV_2019_paper.html:text/html}
}

@inproceedings{subramaniam_co-segmentation_2019,
	title = {Co-{Segmentation} {Inspired} {Attention} {Networks} for {Video}-{Based} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Subramaniam_Co-Segmentation_Inspired_Attention_Networks_for_Video-Based_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Subramaniam, Arulkumar and Nambiar, Athira and Mittal, Anurag},
	year = {2019},
	pages = {562--572},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\BJA9TYFH\\Subramaniam et al. - 2019 - Co-Segmentation Inspired Attention Networks for Vi.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\B3QKBUT8\\Subramaniam_Co-Segmentation_Inspired_Attention_Networks_for_Video-Based_Person_Re-Identificatio.html:text/html}
}

@inproceedings{li_recover_2019,
	title = {Recover and {Identify}: {A} {Generative} {Dual} {Model} for {Cross}-{Resolution} {Person} {Re}-{Identification}},
	shorttitle = {Recover and {Identify}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Recover_and_Identify_A_Generative_Dual_Model_for_Cross-Resolution_Person_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Li, Yu-Jhe and Chen, Yun-Chun and Lin, Yen-Yu and Du, Xiaofei and Wang, Yu-Chiang Frank},
	year = {2019},
	pages = {8090--8099},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\8AFRWQQ6\\Li et al. - 2019 - Recover and Identify A Generative Dual Model for .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\V3ZGMGIX\\Li_Recover_and_Identify_A_Generative_Dual_Model_for_Cross-Resolution_Person_ICCV_2019_paper.html:text/html}
}

@inproceedings{wang_rgb-infrared_2019,
	title = {{RGB}-{Infrared} {Cross}-{Modality} {Person} {Re}-{Identification} via {Joint} {Pixel} and {Feature} {Alignment}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_RGB-Infrared_Cross-Modality_Person_Re-Identification_via_Joint_Pixel_and_Feature_Alignment_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Wang, Guan'an and Zhang, Tianzhu and Cheng, Jian and Liu, Si and Yang, Yang and Hou, Zengguang},
	year = {2019},
	pages = {3623--3632},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\H254D7U9\\Wang et al. - 2019 - RGB-Infrared Cross-Modality Person Re-Identificati.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\33UWICVV\\Wang_RGB-Infrared_Cross-Modality_Person_Re-Identification_via_Joint_Pixel_and_Feature_Alignment.html:text/html}
}

@inproceedings{guo_beyond_2019,
	title = {Beyond {Human} {Parts}: {Dual} {Part}-{Aligned} {Representations} for {Person} {Re}-{Identification}},
	shorttitle = {Beyond {Human} {Parts}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Guo_Beyond_Human_Parts_Dual_Part-Aligned_Representations_for_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Guo, Jianyuan and Yuan, Yuhui and Huang, Lang and Zhang, Chao and Yao, Jin-Ge and Han, Kai},
	year = {2019},
	pages = {3642--3651},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\DD29G24F\\Guo et al. - 2019 - Beyond Human Parts Dual Part-Aligned Representati.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\VGQXDSUW\\Guo_Beyond_Human_Parts_Dual_Part-Aligned_Representations_for_Person_Re-Identification_ICCV_2019.html:text/html}
}

@inproceedings{dai_batch_2019,
	title = {Batch {DropBlock} {Network} for {Person} {Re}-{Identification} and {Beyond}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Dai_Batch_DropBlock_Network_for_Person_Re-Identification_and_Beyond_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Dai, Zuozhuo and Chen, Mingqiang and Gu, Xiaodong and Zhu, Siyu and Tan, Ping},
	year = {2019},
	pages = {3691--3701},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\PJZZJALD\\Dai et al. - 2019 - Batch DropBlock Network for Person Re-Identificati.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\I3T3VH6D\\Dai_Batch_DropBlock_Network_for_Person_Re-Identification_and_Beyond_ICCV_2019_paper.html:text/html}
}

@inproceedings{zhou_omni-scale_2019,
	title = {Omni-{Scale} {Feature} {Learning} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Omni-Scale_Feature_Learning_for_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Zhou, Kaiyang and Yang, Yongxin and Cavallaro, Andrea and Xiang, Tao},
	year = {2019},
	pages = {3702--3712},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\JLW3J5A6\\Zhou et al. - 2019 - Omni-Scale Feature Learning for Person Re-Identifi.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\KIE6RB38\\Zhou_Omni-Scale_Feature_Learning_for_Person_Re-Identification_ICCV_2019_paper.html:text/html}
}

@inproceedings{quan_auto-reid_2019,
	title = {Auto-{ReID}: {Searching} for a {Part}-{Aware} {ConvNet} for {Person} {Re}-{Identification}},
	shorttitle = {Auto-{ReID}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Quan_Auto-ReID_Searching_for_a_Part-Aware_ConvNet_for_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Quan, Ruijie and Dong, Xuanyi and Wu, Yu and Zhu, Linchao and Yang, Yi},
	year = {2019},
	pages = {3750--3759},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\28J4GSDM\\Quan et al. - 2019 - Auto-ReID Searching for a Part-Aware ConvNet for .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\9EX9PR5A\\Quan_Auto-ReID_Searching_for_a_Part-Aware_ConvNet_for_Person_Re-Identification_ICCV_2019_paper.html:text/html}
}

@inproceedings{xia_second-order_2019,
	title = {Second-{Order} {Non}-{Local} {Attention} {Networks} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Xia_Second-Order_Non-Local_Attention_Networks_for_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Xia, Bryan (Ning) and Gong, Yuan and Zhang, Yizhe and Poellabauer, Christian},
	year = {2019},
	pages = {3760--3769},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\G5Y937CR\\Xia et al. - 2019 - Second-Order Non-Local Attention Networks for Pers.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\9H3RTG5G\\Xia_Second-Order_Non-Local_Attention_Networks_for_Person_Re-Identification_ICCV_2019_paper.html:text/html}
}

@inproceedings{li_global-local_2019,
	title = {Global-{Local} {Temporal} {Representations} for {Video} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Global-Local_Temporal_Representations_for_Video_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Li, Jianing and Wang, Jingdong and Tian, Qi and Gao, Wen and Zhang, Shiliang},
	year = {2019},
	pages = {3958--3967},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\4IU7ARW5\\Li et al. - 2019 - Global-Local Temporal Representations for Video Pe.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\WTRXJSFI\\Li_Global-Local_Temporal_Representations_for_Video_Person_Re-Identification_ICCV_2019_paper.html:text/html}
}

@inproceedings{luo_spectral_2019,
	title = {Spectral {Feature} {Transformation} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Luo_Spectral_Feature_Transformation_for_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Luo, Chuanchen and Chen, Yuntao and Wang, Naiyan and Zhang, Zhaoxiang},
	year = {2019},
	pages = {4976--4985},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\L5P9BZQF\\Luo et al. - 2019 - Spectral Feature Transformation for Person Re-Iden.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\ZDV38Z9H\\Luo_Spectral_Feature_Transformation_for_Person_Re-Identification_ICCV_2019_paper.html:text/html}
}

@inproceedings{fu_self-similarity_2019,
	title = {Self-{Similarity} {Grouping}: {A} {Simple} {Unsupervised} {Cross} {Domain} {Adaptation} {Approach} for {Person} {Re}-{Identification}},
	shorttitle = {Self-{Similarity} {Grouping}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Fu_Self-Similarity_Grouping_A_Simple_Unsupervised_Cross_Domain_Adaptation_Approach_for_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Fu, Yang and Wei, Yunchao and Wang, Guanshuo and Zhou, Yuqian and Shi, Honghui and Huang, Thomas S.},
	year = {2019},
	pages = {6112--6121},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\6U4ET5IE\\Fu et al. - 2019 - Self-Similarity Grouping A Simple Unsupervised Cr.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\JTG65CGQ\\Fu_Self-Similarity_Grouping_A_Simple_Unsupervised_Cross_Domain_Adaptation_Approach_for_ICCV_201.html:text/html}
}

@inproceedings{liu_deep_2019,
	title = {Deep {Reinforcement} {Active} {Learning} for {Human}-in-the-{Loop} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Deep_Reinforcement_Active_Learning_for_Human-in-the-Loop_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Liu, Zimo and Wang, Jingya and Gong, Shaogang and Lu, Huchuan and Tao, Dacheng},
	year = {2019},
	pages = {6122--6131},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\NGMP6IKB\\Liu et al. - 2019 - Deep Reinforcement Active Learning for Human-in-th.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\4Z8EPAR8\\Liu_Deep_Reinforcement_Active_Learning_for_Human-in-the-Loop_Person_Re-Identification_ICCV_2019.html:text/html}
}

@inproceedings{liu_view_2019,
	title = {View {Confusion} {Feature} {Learning} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_View_Confusion_Feature_Learning_for_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Liu, Fangyi and Zhang, Lei},
	year = {2019},
	pages = {6639--6648},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\W8QZV7ZR\\Liu and Zhang - 2019 - View Confusion Feature Learning for Person Re-Iden.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\UVUIUIZW\\Liu_View_Confusion_Feature_Learning_for_Person_Re-Identification_ICCV_2019_paper.html:text/html}
}

@inproceedings{sun_mvp_2019,
	title = {{MVP} {Matching}: {A} {Maximum}-{Value} {Perfect} {Matching} for {Mining} {Hard} {Samples}, {With} {Application} to {Person} {Re}-{Identification}},
	shorttitle = {{MVP} {Matching}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Sun_MVP_Matching_A_Maximum-Value_Perfect_Matching_for_Mining_Hard_Samples_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Sun, Han and Chen, Zhiyuan and Yan, Shiyang and Xu, Lin},
	year = {2019},
	pages = {6737--6747},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\RJA4C9UC\\Sun et al. - 2019 - MVP Matching A Maximum-Value Perfect Matching for.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\K4XIXS9M\\Sun_MVP_Matching_A_Maximum-Value_Perfect_Matching_for_Mining_Hard_Samples_ICCV_2019_paper.html:text/html}
}

@inproceedings{wu_unsupervised_2019,
	title = {Unsupervised {Person} {Re}-{Identification} by {Camera}-{Aware} {Similarity} {Consistency} {Learning}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Unsupervised_Person_Re-Identification_by_Camera-Aware_Similarity_Consistency_Learning_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Wu, Ancong and Zheng, Wei-Shi and Lai, Jian-Huang},
	year = {2019},
	pages = {6922--6931},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\IBWP4YZ4\\Wu et al. - 2019 - Unsupervised Person Re-Identification by Camera-Aw.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\4PN9EPQA\\Wu_Unsupervised_Person_Re-Identification_by_Camera-Aware_Similarity_Consistency_Learning_ICCV_2.html:text/html}
}

@inproceedings{li_cross-dataset_2019,
	title = {Cross-{Dataset} {Person} {Re}-{Identification} via {Unsupervised} {Pose} {Disentanglement} and {Adaptation}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Cross-Dataset_Person_Re-Identification_via_Unsupervised_Pose_Disentanglement_and_Adaptation_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Li, Yu-Jhe and Lin, Ci-Siang and Lin, Yan-Bo and Wang, Yu-Chiang Frank},
	year = {2019},
	pages = {7919--7929},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\XP9RFJLH\\Li et al. - 2019 - Cross-Dataset Person Re-Identification via Unsuper.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\BKQ22MYG\\Li_Cross-Dataset_Person_Re-Identification_via_Unsupervised_Pose_Disentanglement_and_Adaptation_.html:text/html}
}

@inproceedings{fang_bilinear_2019,
	title = {Bilinear {Attention} {Networks} for {Person} {Retrieval}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Fang_Bilinear_Attention_Networks_for_Person_Retrieval_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Fang, Pengfei and Zhou, Jieming and Roy, Soumava Kumar and Petersson, Lars and Harandi, Mehrtash},
	year = {2019},
	pages = {8030--8039},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\SEA2DTRR\\Fang et al. - 2019 - Bilinear Attention Networks for Person Retrieval.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\JKT5JB7I\\Fang_Bilinear_Attention_Networks_for_Person_Retrieval_ICCV_2019_paper.html:text/html}
}

@inproceedings{zhou_discriminative_2019,
	title = {Discriminative {Feature} {Learning} {With} {Consistent} {Attention} {Regularization} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Discriminative_Feature_Learning_With_Consistent_Attention_Regularization_for_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Zhou, Sanping and Wang, Fei and Huang, Zeyi and Wang, Jinjun},
	year = {2019},
	pages = {8040--8049},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\LLWS3WYQ\\Zhou et al. - 2019 - Discriminative Feature Learning With Consistent At.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\P24S4V2F\\Zhou_Discriminative_Feature_Learning_With_Consistent_Attention_Regularization_for_Person_Re-Ide.html:text/html}
}

@inproceedings{qi_novel_2019,
	title = {A {Novel} {Unsupervised} {Camera}-{Aware} {Domain} {Adaptation} {Framework} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Qi_A_Novel_Unsupervised_Camera-Aware_Domain_Adaptation_Framework_for_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Qi, Lei and Wang, Lei and Huo, Jing and Zhou, Luping and Shi, Yinghuan and Gao, Yang},
	year = {2019},
	pages = {8080--8089},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\6VQVUPZU\\Qi et al. - 2019 - A Novel Unsupervised Camera-Aware Domain Adaptatio.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\Z88PP5A6\\Qi_A_Novel_Unsupervised_Camera-Aware_Domain_Adaptation_Framework_for_Person_Re-Identification_I.html:text/html}
}

@inproceedings{zhang_self-training_2019,
	title = {Self-{Training} {With} {Progressive} {Augmentation} for {Unsupervised} {Cross}-{Domain} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Self-Training_With_Progressive_Augmentation_for_Unsupervised_Cross-Domain_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Zhang, Xinyu and Cao, Jiewei and Shen, Chunhua and You, Mingyu},
	year = {2019},
	pages = {8222--8231},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\LF7646PK\\Zhang et al. - 2019 - Self-Training With Progressive Augmentation for Un.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\RRBPB5KM\\Zhang_Self-Training_With_Progressive_Augmentation_for_Unsupervised_Cross-Domain_Person_Re-Ident.html:text/html}
}

@inproceedings{wu_unsupervised_2019-1,
	title = {Unsupervised {Graph} {Association} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Unsupervised_Graph_Association_for_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Wu, Jinlin and Yang, Yang and Liu, Hao and Liao, Shengcai and Lei, Zhen and Li, Stan Z.},
	year = {2019},
	pages = {8321--8330},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\I63HLNV3\\Wu et al. - 2019 - Unsupervised Graph Association for Person Re-Ident.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\F6DEJZCB\\Wu_Unsupervised_Graph_Association_for_Person_Re-Identification_ICCV_2019_paper.html:text/html}
}

@inproceedings{wang_advpattern_2019,
	title = {{advPattern}: {Physical}-{World} {Attacks} on {Deep} {Person} {Re}-{Identification} via {Adversarially} {Transformable} {Patterns}},
	shorttitle = {{advPattern}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_advPattern_Physical-World_Attacks_on_Deep_Person_Re-Identification_via_Adversarially_Transformable_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Wang, Zhibo and Zheng, Siyan and Song, Mengkai and Wang, Qian and Rahimpour, Alireza and Qi, Hairong},
	year = {2019},
	pages = {8341--8350},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\N5K7LSKJ\\Wang et al. - 2019 - advPattern Physical-World Attacks on Deep Person .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\4P6ANQYN\\Wang_advPattern_Physical-World_Attacks_on_Deep_Person_Re-Identification_via_Adversarially_Trans.html:text/html}
}

@inproceedings{chen_abd-net_2019,
	title = {{ABD}-{Net}: {Attentive} but {Diverse} {Person} {Re}-{Identification}},
	shorttitle = {{ABD}-{Net}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_ABD-Net_Attentive_but_Diverse_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Chen, Tianlong and Ding, Shaojin and Xie, Jingyi and Yuan, Ye and Chen, Wuyang and Yang, Yang and Ren, Zhou and Wang, Zhangyang},
	year = {2019},
	pages = {8351--8361},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\ET6JZAID\\Chen et al. - 2019 - ABD-Net Attentive but Diverse Person Re-Identific.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\66YGCQS2\\Chen_ABD-Net_Attentive_but_Diverse_Person_Re-Identification_ICCV_2019_paper.html:text/html}
}

@inproceedings{he_foreground-aware_2019,
	title = {Foreground-{Aware} {Pyramid} {Reconstruction} for {Alignment}-{Free} {Occluded} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/He_Foreground-Aware_Pyramid_Reconstruction_for_Alignment-Free_Occluded_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {He, Lingxiao and Wang, Yinggang and Liu, Wu and Zhao, He and Sun, Zhenan and Feng, Jiashi},
	year = {2019},
	pages = {8450--8459},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\LZZ7TR3B\\He et al. - 2019 - Foreground-Aware Pyramid Reconstruction for Alignm.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\3SYG92MJ\\He_Foreground-Aware_Pyramid_Reconstruction_for_Alignment-Free_Occluded_Person_Re-Identification.html:text/html}
}

@inproceedings{huang_sbsgan_2019,
	title = {{SBSGAN}: {Suppression} of {Inter}-{Domain} {Background} {Shift} for {Person} {Re}-{Identification}},
	shorttitle = {{SBSGAN}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_SBSGAN_Suppression_of_Inter-Domain_Background_Shift_for_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Huang, Yan and Wu, Qiang and Xu, JingSong and Zhong, Yi},
	year = {2019},
	pages = {9527--9536},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\HR5NN8W7\\Huang et al. - 2019 - SBSGAN Suppression of Inter-Domain Background Shi.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\EISSW6GH\\Huang_SBSGAN_Suppression_of_Inter-Domain_Background_Shift_for_Person_Re-Identification_ICCV_201.html:text/html}
}

@inproceedings{chen_self-critical_2019,
	title = {Self-{Critical} {Attention} {Learning} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Self-Critical_Attention_Learning_for_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Chen, Guangyi and Lin, Chunze and Ren, Liangliang and Lu, Jiwen and Zhou, Jie},
	year = {2019},
	pages = {9637--9646},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\GRSFMS8Z\\Chen et al. - 2019 - Self-Critical Attention Learning for Person Re-Ide.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\YQRRIFZ9\\Chen_Self-Critical_Attention_Learning_for_Person_Re-Identification_ICCV_2019_paper.html:text/html}
}

@inproceedings{gu_temporal_2019,
	title = {Temporal {Knowledge} {Propagation} for {Image}-to-{Video} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Gu_Temporal_Knowledge_Propagation_for_Image-to-Video_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Gu, Xinqian and Ma, Bingpeng and Chang, Hong and Shan, Shiguang and Chen, Xilin},
	year = {2019},
	pages = {9647--9656},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\X3C7FG94\\Gu et al. - 2019 - Temporal Knowledge Propagation for Image-to-Video .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\YH6FYF79\\Gu_Temporal_Knowledge_Propagation_for_Image-to-Video_Person_Re-Identification_ICCV_2019_paper.html:text/html}
}

@inproceedings{alemu_deep_2019,
	title = {Deep {Constrained} {Dominant} {Sets} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Alemu_Deep_Constrained_Dominant_Sets_for_Person_Re-Identification_ICCV_2019_paper.html},
	urldate = {2021-04-20},
	author = {Alemu, Leulseged Tesfaye and Pelillo, Marcello and Shah, Mubarak},
	year = {2019},
	pages = {9855--9864},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\FXQPK6RF\\Alemu et al. - 2019 - Deep Constrained Dominant Sets for Person Re-Ident.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\3VJQ6VIQ\\Alemu_Deep_Constrained_Dominant_Sets_for_Person_Re-Identification_ICCV_2019_paper.html:text/html}
}

@inproceedings{sun_perceive_2019,
	title = {Perceive {Where} to {Focus}: {Learning} {Visibility}-{Aware} {Part}-{Level} {Features} for {Partial} {Person} {Re}-{Identification}},
	shorttitle = {Perceive {Where} to {Focus}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Sun_Perceive_Where_to_Focus_Learning_Visibility-Aware_Part-Level_Features_for_Partial_CVPR_2019_paper.html},
	urldate = {2021-04-20},
	author = {Sun, Yifan and Xu, Qin and Li, Yali and Zhang, Chi and Li, Yikang and Wang, Shengjin and Sun, Jian},
	year = {2019},
	pages = {393--402},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\P9IFANMJ\\Sun et al. - 2019 - Perceive Where to Focus Learning Visibility-Aware.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\XTEZCNUV\\Sun_Perceive_Where_to_Focus_Learning_Visibility-Aware_Part-Level_Features_for_Partial_CVPR_2019.html:text/html}
}

@inproceedings{zhong_invariance_2019,
	title = {Invariance {Matters}: {Exemplar} {Memory} for {Domain} {Adaptive} {Person} {Re}-{Identification}},
	shorttitle = {Invariance {Matters}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Zhong_Invariance_Matters_Exemplar_Memory_for_Domain_Adaptive_Person_Re-Identification_CVPR_2019_paper.html},
	urldate = {2021-04-20},
	author = {Zhong, Zhun and Zheng, Liang and Luo, Zhiming and Li, Shaozi and Yang, Yi},
	year = {2019},
	pages = {598--607},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\YKXJTWYL\\Zhong et al. - 2019 - Invariance Matters Exemplar Memory for Domain Ada.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\F2YQ3MEU\\Zhong_Invariance_Matters_Exemplar_Memory_for_Domain_Adaptive_Person_Re-Identification_CVPR_2019.html:text/html}
}

@inproceedings{sun_dissecting_2019,
	title = {Dissecting {Person} {Re}-{Identification} {From} the {Viewpoint} of {Viewpoint}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Sun_Dissecting_Person_Re-Identification_From_the_Viewpoint_of_Viewpoint_CVPR_2019_paper.html},
	urldate = {2021-04-20},
	author = {Sun, Xiaoxiao and Zheng, Liang},
	year = {2019},
	pages = {608--617},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\25F7N7MS\\Sun and Zheng - 2019 - Dissecting Person Re-Identification From the Viewp.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\SAEH3ZNJ\\Sun_Dissecting_Person_Re-Identification_From_the_Viewpoint_of_Viewpoint_CVPR_2019_paper.html:text/html}
}

@inproceedings{wang_learning_2019,
	title = {Learning to {Reduce} {Dual}-{Level} {Discrepancy} for {Infrared}-{Visible} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Learning_to_Reduce_Dual-Level_Discrepancy_for_Infrared-Visible_Person_Re-Identification_CVPR_2019_paper.html},
	urldate = {2021-04-20},
	author = {Wang, Zhixiang and Wang, Zheng and Zheng, Yinqiang and Chuang, Yung-Yu and Satoh, Shin'ichi},
	year = {2019},
	pages = {618--626},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\URDNK5BB\\Wang et al. - 2019 - Learning to Reduce Dual-Level Discrepancy for Infr.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\2DFBCGXI\\Wang_Learning_to_Reduce_Dual-Level_Discrepancy_for_Infrared-Visible_Person_Re-Identification_CV.html:text/html}
}

@inproceedings{zhang_densely_2019,
	title = {Densely {Semantically} {Aligned} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Densely_Semantically_Aligned_Person_Re-Identification_CVPR_2019_paper.html},
	urldate = {2021-04-20},
	author = {Zhang, Zhizheng and Lan, Cuiling and Zeng, Wenjun and Chen, Zhibo},
	year = {2019},
	pages = {667--676},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\TAJWR7KL\\Zhang et al. - 2019 - Densely Semantically Aligned Person Re-Identificat.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\E9Z8GVZP\\Zhang_Densely_Semantically_Aligned_Person_Re-Identification_CVPR_2019_paper.html:text/html}
}

@inproceedings{song_generalizable_2019,
	title = {Generalizable {Person} {Re}-{Identification} by {Domain}-{Invariant} {Mapping} {Network}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Song_Generalizable_Person_Re-Identification_by_Domain-Invariant_Mapping_Network_CVPR_2019_paper.html},
	urldate = {2021-04-20},
	author = {Song, Jifei and Yang, Yongxin and Song, Yi-Zhe and Xiang, Tao and Hospedales, Timothy M.},
	year = {2019},
	pages = {719--728},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\M533KFU9\\Song et al. - 2019 - Generalizable Person Re-Identification by Domain-I.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\XZGKI3B3\\Song_Generalizable_Person_Re-Identification_by_Domain-Invariant_Mapping_Network_CVPR_2019_paper.html:text/html}
}

@inproceedings{bai_re-ranking_2019,
	title = {Re-{Ranking} via {Metric} {Fusion} for {Object} {Retrieval} and {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Bai_Re-Ranking_via_Metric_Fusion_for_Object_Retrieval_and_Person_Re-Identification_CVPR_2019_paper.html},
	urldate = {2021-04-20},
	author = {Bai, Song and Tang, Peng and Torr, Philip H. S. and Latecki, Longin Jan},
	year = {2019},
	pages = {740--749},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\6E8364WR\\Bai et al. - 2019 - Re-Ranking via Metric Fusion for Object Retrieval .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\YILG79SU\\Bai_Re-Ranking_via_Metric_Fusion_for_Object_Retrieval_and_Person_Re-Identification_CVPR_2019_pa.html:text/html}
}

@inproceedings{meng_weakly_2019,
	title = {Weakly {Supervised} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Meng_Weakly_Supervised_Person_Re-Identification_CVPR_2019_paper.html},
	urldate = {2021-04-20},
	author = {Meng, Jingke and Wu, Sheng and Zheng, Wei-Shi},
	year = {2019},
	pages = {760--769},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\MNXN44QJ\\Meng et al. - 2019 - Weakly Supervised Person Re-Identification.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\RESKZD3W\\Meng_Weakly_Supervised_Person_Re-Identification_CVPR_2019_paper.html:text/html}
}

@inproceedings{wu_distilled_2019,
	title = {Distilled {Person} {Re}-{Identification}: {Towards} a {More} {Scalable} {System}},
	shorttitle = {Distilled {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Distilled_Person_Re-Identification_Towards_a_More_Scalable_System_CVPR_2019_paper.html},
	urldate = {2021-04-20},
	author = {Wu, Ancong and Zheng, Wei-Shi and Guo, Xiaowei and Lai, Jian-Huang},
	year = {2019},
	pages = {1187--1196},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\RRHKQJXB\\Wu et al. - 2019 - Distilled Person Re-Identification Towards a More.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\XSFH5J59\\Wu_Distilled_Person_Re-Identification_Towards_a_More_Scalable_System_CVPR_2019_paper.html:text/html}
}

@inproceedings{yang_towards_2019,
	title = {Towards {Rich} {Feature} {Discovery} {With} {Class} {Activation} {Maps} {Augmentation} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Towards_Rich_Feature_Discovery_With_Class_Activation_Maps_Augmentation_for_CVPR_2019_paper.html},
	urldate = {2021-04-20},
	author = {Yang, Wenjie and Huang, Houjing and Zhang, Zhang and Chen, Xiaotang and Huang, Kaiqi and Zhang, Shu},
	year = {2019},
	pages = {1389--1398},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\K8LKRF2Q\\Yang et al. - 2019 - Towards Rich Feature Discovery With Class Activati.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\5G63Z3MI\\Yang_Towards_Rich_Feature_Discovery_With_Class_Activation_Maps_Augmentation_for_CVPR_2019_paper.html:text/html}
}

@inproceedings{zheng_joint_2019,
	title = {Joint {Discriminative} and {Generative} {Learning} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Zheng_Joint_Discriminative_and_Generative_Learning_for_Person_Re-Identification_CVPR_2019_paper.html},
	urldate = {2021-04-20},
	author = {Zheng, Zhedong and Yang, Xiaodong and Yu, Zhiding and Zheng, Liang and Yang, Yi and Kautz, Jan},
	year = {2019},
	pages = {2138--2147},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\HEXFM6JF\\Zheng et al. - 2019 - Joint Discriminative and Generative Learning for P.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\MMDB363I\\Zheng_Joint_Discriminative_and_Generative_Learning_for_Person_Re-Identification_CVPR_2019_paper.html:text/html}
}

@inproceedings{yu_unsupervised_2019,
	title = {Unsupervised {Person} {Re}-{Identification} by {Soft} {Multilabel} {Learning}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Unsupervised_Person_Re-Identification_by_Soft_Multilabel_Learning_CVPR_2019_paper.html},
	urldate = {2021-04-20},
	author = {Yu, Hong-Xing and Zheng, Wei-Shi and Wu, Ancong and Guo, Xiaowei and Gong, Shaogang and Lai, Jian-Huang},
	year = {2019},
	pages = {2148--2157},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\TW8PA7AP\\Yu et al. - 2019 - Unsupervised Person Re-Identification by Soft Mult.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\6IR3W8K5\\Yu_Unsupervised_Person_Re-Identification_by_Soft_Multilabel_Learning_CVPR_2019_paper.html:text/html}
}

@inproceedings{yang_patch-based_2019,
	title = {Patch-{Based} {Discriminative} {Feature} {Learning} for {Unsupervised} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Patch-Based_Discriminative_Feature_Learning_for_Unsupervised_Person_Re-Identification_CVPR_2019_paper.html},
	urldate = {2021-04-20},
	author = {Yang, Qize and Yu, Hong-Xing and Wu, Ancong and Zheng, Wei-Shi},
	year = {2019},
	pages = {3633--3642},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\MBXISU6Z\\Yang et al. - 2019 - Patch-Based Discriminative Feature Learning for Un.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\P8JEJ7IL\\Yang_Patch-Based_Discriminative_Feature_Learning_for_Unsupervised_Person_Re-Identification_CVPR.html:text/html}
}

@inproceedings{hou_interaction-and-aggregation_2019,
	title = {Interaction-{And}-{Aggregation} {Network} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Hou_Interaction-And-Aggregation_Network_for_Person_Re-Identification_CVPR_2019_paper.html},
	urldate = {2021-04-20},
	author = {Hou, Ruibing and Ma, Bingpeng and Chang, Hong and Gu, Xinqian and Shan, Shiguang and Chen, Xilin},
	year = {2019},
	pages = {9317--9326},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\SYINNNJR\\Hou et al. - 2019 - Interaction-And-Aggregation Network for Person Re-.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\KVE56EDR\\Hou_Interaction-And-Aggregation_Network_for_Person_Re-Identification_CVPR_2019_paper.html:text/html}
}

@inproceedings{zhao_attribute-driven_2019,
	title = {Attribute-{Driven} {Feature} {Disentangling} and {Temporal} {Aggregation} for {Video} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Attribute-Driven_Feature_Disentangling_and_Temporal_Aggregation_for_Video_Person_Re-Identification_CVPR_2019_paper.html},
	urldate = {2021-04-20},
	author = {Zhao, Yiru and Shen, Xu and Jin, Zhongming and Lu, Hongtao and Hua, Xian-sheng},
	year = {2019},
	pages = {4913--4922},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\A7FLPQSZ\\Zhao et al. - 2019 - Attribute-Driven Feature Disentangling and Tempora.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\GYFX5GFP\\Zhao_Attribute-Driven_Feature_Disentangling_and_Temporal_Aggregation_for_Video_Person_Re-Identi.html:text/html}
}

@inproceedings{tay_aanet_2019,
	title = {{AANet}: {Attribute} {Attention} {Network} for {Person} {Re}-{Identifications}},
	shorttitle = {{AANet}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Tay_AANet_Attribute_Attention_Network_for_Person_Re-Identifications_CVPR_2019_paper.html},
	urldate = {2021-04-20},
	author = {Tay, Chiat-Pin and Roy, Sharmili and Yap, Kim-Hui},
	year = {2019},
	pages = {7134--7143},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\J3I9JAHH\\Tay et al. - 2019 - AANet Attribute Attention Network for Person Re-I.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\7HZ7T7KA\\Tay_AANet_Attribute_Attention_Network_for_Person_Re-Identifications_CVPR_2019_paper.html:text/html}
}

@inproceedings{hou_vrstc_2019,
	title = {{VRSTC}: {Occlusion}-{Free} {Video} {Person} {Re}-{Identification}},
	shorttitle = {{VRSTC}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Hou_VRSTC_Occlusion-Free_Video_Person_Re-Identification_CVPR_2019_paper.html},
	urldate = {2021-04-20},
	author = {Hou, Ruibing and Ma, Bingpeng and Chang, Hong and Gu, Xinqian and Shan, Shiguang and Chen, Xilin},
	year = {2019},
	pages = {7183--7192},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\H2A28V96\\Hou et al. - 2019 - VRSTC Occlusion-Free Video Person Re-Identificati.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\F98UPZSD\\Hou_VRSTC_Occlusion-Free_Video_Person_Re-Identification_CVPR_2019_paper.html:text/html}
}

@inproceedings{liu_adaptive_2019,
	title = {Adaptive {Transfer} {Network} for {Cross}-{Domain} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Adaptive_Transfer_Network_for_Cross-Domain_Person_Re-Identification_CVPR_2019_paper.html},
	urldate = {2021-04-20},
	author = {Liu, Jiawei and Zha, Zheng-Jun and Chen, Di and Hong, Richang and Wang, Meng},
	year = {2019},
	pages = {7202--7211},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\DJIQNDAP\\Liu et al. - 2019 - Adaptive Transfer Network for Cross-Domain Person .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\BWYS8IE5\\Liu_Adaptive_Transfer_Network_for_Cross-Domain_Person_Re-Identification_CVPR_2019_paper.html:text/html}
}

@inproceedings{zheng_pyramidal_2019,
	title = {Pyramidal {Person} {Re}-{IDentification} via {Multi}-{Loss} {Dynamic} {Training}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Zheng_Pyramidal_Person_Re-IDentification_via_Multi-Loss_Dynamic_Training_CVPR_2019_paper.html},
	urldate = {2021-04-20},
	author = {Zheng, Feng and Deng, Cheng and Sun, Xing and Jiang, Xinyang and Guo, Xiaowei and Yu, Zongqiao and Huang, Feiyue and Ji, Rongrong},
	year = {2019},
	pages = {8514--8522},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\PWFLWHU4\\Zheng et al. - 2019 - Pyramidal Person Re-IDentification via Multi-Loss .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\CYY6EUVI\\Zheng_Pyramidal_Person_Re-IDentification_via_Multi-Loss_Dynamic_Training_CVPR_2019_paper.html:text/html}
}

@inproceedings{lv_unsupervised_2018,
	title = {Unsupervised {Cross}-{Dataset} {Person} {Re}-{Identification} by {Transfer} {Learning} of {Spatial}-{Temporal} {Patterns}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Lv_Unsupervised_Cross-Dataset_Person_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Lv, Jianming and Chen, Weihang and Li, Qing and Yang, Can},
	year = {2018},
	pages = {7948--7956},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\2W8XXIIJ\\Lv et al. - 2018 - Unsupervised Cross-Dataset Person Re-Identificatio.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\3LVW9VEL\\Lv_Unsupervised_Cross-Dataset_Person_CVPR_2018_paper.html:text/html}
}

@inproceedings{wang_resource_2018,
	title = {Resource {Aware} {Person} {Re}-{Identification} {Across} {Multiple} {Resolutions}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Resource_Aware_Person_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Wang, Yan and Wang, Lequn and You, Yurong and Zou, Xu and Chen, Vincent and Li, Serena and Huang, Gao and Hariharan, Bharath and Weinberger, Kilian Q.},
	year = {2018},
	pages = {8042--8051},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\G3L9ES4S\\Wang et al. - 2018 - Resource Aware Person Re-Identification Across Mul.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\VKBUL7SM\\Wang_Resource_Aware_Person_CVPR_2018_paper.html:text/html}
}

@inproceedings{chen_group_2018,
	title = {Group {Consistent} {Similarity} {Learning} via {Deep} {CRF} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Group_Consistent_Similarity_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Chen, Dapeng and Xu, Dan and Li, Hongsheng and Sebe, Nicu and Wang, Xiaogang},
	year = {2018},
	pages = {8649--8658},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\73UYBNMK\\Chen et al. - 2018 - Group Consistent Similarity Learning via Deep CRF .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\SXXJEX3F\\Chen_Group_Consistent_Similarity_CVPR_2018_paper.html:text/html}
}

@inproceedings{liu_pose_2018,
	title = {Pose {Transferrable} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Pose_Transferrable_Person_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Liu, Jinxian and Ni, Bingbing and Yan, Yichao and Zhou, Peng and Cheng, Shuo and Hu, Jianguo},
	year = {2018},
	pages = {4099--4108},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\VH2QWSYW\\Liu et al. - 2018 - Pose Transferrable Person Re-Identification.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\3SYWLIM3\\Liu_Pose_Transferrable_Person_CVPR_2018_paper.html:text/html}
}

@inproceedings{huang_adversarially_2018,
	title = {Adversarially {Occluded} {Samples} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Adversarially_Occluded_Samples_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Huang, Houjing and Li, Dangwei and Zhang, Zhang and Chen, Xiaotang and Huang, Kaiqi},
	year = {2018},
	pages = {5098--5107},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\97ZJFV6B\\Huang et al. - 2018 - Adversarially Occluded Samples for Person Re-Ident.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\RYLKLAVH\\Huang_Adversarially_Occluded_Samples_CVPR_2018_paper.html:text/html}
}

@inproceedings{zhong_camera_2018,
	title = {Camera {Style} {Adaptation} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Zhong_Camera_Style_Adaptation_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Zhong, Zhun and Zheng, Liang and Zheng, Zhedong and Li, Shaozi and Yang, Yi},
	year = {2018},
	pages = {5157--5166},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\AG4XV598\\Zhong et al. - 2018 - Camera Style Adaptation for Person Re-Identificati.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\AA9HK2YQ\\Zhong_Camera_Style_Adaptation_CVPR_2018_paper.html:text/html}
}

@inproceedings{wu_exploit_2018,
	title = {Exploit the {Unknown} {Gradually}: {One}-{Shot} {Video}-{Based} {Person} {Re}-{Identification} by {Stepwise} {Learning}},
	shorttitle = {Exploit the {Unknown} {Gradually}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Exploit_the_Unknown_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Wu, Yu and Lin, Yutian and Dong, Xuanyi and Yan, Yan and Ouyang, Wanli and Yang, Yi},
	year = {2018},
	pages = {5177--5186},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\Z55SX8FU\\Wu et al. - 2018 - Exploit the Unknown Gradually One-Shot Video-Base.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\BMLQW8J3\\Wu_Exploit_the_Unknown_CVPR_2018_paper.html:text/html}
}

@inproceedings{si_dual_2018,
	title = {Dual {Attention} {Matching} {Network} for {Context}-{Aware} {Feature} {Sequence} {Based} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Si_Dual_Attention_Matching_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Si, Jianlou and Zhang, Honggang and Li, Chun-Guang and Kuen, Jason and Kong, Xiangfei and Kot, Alex C. and Wang, Gang},
	year = {2018},
	pages = {5363--5372},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\HXKK8CWV\\Si et al. - 2018 - Dual Attention Matching Network for Context-Aware .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\RLNCVHWC\\Si_Dual_Attention_Matching_CVPR_2018_paper.html:text/html}
}

@inproceedings{zhou_easy_2018,
	title = {Easy {Identification} {From} {Better} {Constraints}: {Multi}-{Shot} {Person} {Re}-{Identification} {From} {Reference} {Constraints}},
	shorttitle = {Easy {Identification} {From} {Better} {Constraints}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Easy_Identification_From_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Zhou, Jiahuan and Su, Bing and Wu, Ying},
	year = {2018},
	pages = {5373--5381},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\N7CHWAEM\\Zhou et al. - 2018 - Easy Identification From Better Constraints Multi.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\YB5ZXVN2\\Zhou_Easy_Identification_From_CVPR_2018_paper.html:text/html}
}

@inproceedings{tian_eliminating_2018,
	title = {Eliminating {Background}-{Bias} for {Robust} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Tian_Eliminating_Background-Bias_for_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Tian, Maoqing and Yi, Shuai and Li, Hongsheng and Li, Shihua and Zhang, Xuesen and Shi, Jianping and Yan, Junjie and Wang, Xiaogang},
	year = {2018},
	pages = {5794--5803},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\G679W3MF\\Tian et al. - 2018 - Eliminating Background-Bias for Robust Person Re-I.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\XZQAS76B\\Tian_Eliminating_Background-Bias_for_CVPR_2018_paper.html:text/html}
}

@inproceedings{shen_end--end_2018,
	title = {End-to-{End} {Deep} {Kronecker}-{Product} {Matching} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_End-to-End_Deep_Kronecker-Product_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Shen, Yantao and Xiao, Tong and Li, Hongsheng and Yi, Shuai and Wang, Xiaogang},
	year = {2018},
	pages = {6886--6895},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\7WVM3J4Y\\Shen et al. - 2018 - End-to-End Deep Kronecker-Product Matching for Per.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\3FVEEHTJ\\Shen_End-to-End_Deep_Kronecker-Product_CVPR_2018_paper.html:text/html}
}

@inproceedings{roy_exploiting_2018,
	title = {Exploiting {Transitivity} for {Learning} {Person} {Re}-{Identification} {Models} on a {Budget}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Roy_Exploiting_Transitivity_for_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Roy, Sourya and Paul, Sujoy and Young, Neal E. and Roy-Chowdhury, Amit K.},
	year = {2018},
	pages = {7064--7072},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\9SVYAI5P\\Roy et al. - 2018 - Exploiting Transitivity for Learning Person Re-Ide.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\QMBKULSF\\Roy_Exploiting_Transitivity_for_CVPR_2018_paper.html:text/html}
}

@inproceedings{he_deep_2018,
	title = {Deep {Spatial} {Feature} {Reconstruction} for {Partial} {Person} {Re}-{Identification}: {Alignment}-{Free} {Approach}},
	shorttitle = {Deep {Spatial} {Feature} {Reconstruction} for {Partial} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/He_Deep_Spatial_Feature_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {He, Lingxiao and Liang, Jian and Li, Haiqing and Sun, Zhenan},
	year = {2018},
	pages = {7073--7082},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\L6UGIE4Q\\He et al. - 2018 - Deep Spatial Feature Reconstruction for Partial Pe.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\8QW85THG\\He_Deep_Spatial_Feature_CVPR_2018_paper.html:text/html}
}

@inproceedings{wei_person_2018,
	title = {Person {Transfer} {GAN} to {Bridge} {Domain} {Gap} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Wei_Person_Transfer_GAN_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Wei, Longhui and Zhang, Shiliang and Gao, Wen and Tian, Qi},
	year = {2018},
	pages = {79--88},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\TICVH72W\\Wei et al. - 2018 - Person Transfer GAN to Bridge Domain Gap for Perso.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\V9MPWTET\\Wei_Person_Transfer_GAN_CVPR_2018_paper.html:text/html}
}

@inproceedings{li_diversity_2018,
	title = {Diversity {Regularized} {Spatiotemporal} {Attention} for {Video}-{Based} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Diversity_Regularized_Spatiotemporal_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Li, Shuang and Bak, Slawomir and Carr, Peter and Wang, Xiaogang},
	year = {2018},
	pages = {369--378},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\J8SQPB7C\\Li et al. - 2018 - Diversity Regularized Spatiotemporal Attention for.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\N2QN59MJ\\Li_Diversity_Regularized_Spatiotemporal_CVPR_2018_paper.html:text/html}
}

@inproceedings{sarfraz_pose-sensitive_2018,
	title = {A {Pose}-{Sensitive} {Embedding} for {Person} {Re}-{Identification} {With} {Expanded} {Cross} {Neighborhood} {Re}-{Ranking}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Sarfraz_A_Pose-Sensitive_Embedding_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Sarfraz, M. Saquib and Schumann, Arne and Eberle, Andreas and Stiefelhagen, Rainer},
	year = {2018},
	pages = {420--429},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\8TQSE2SH\\Sarfraz et al. - 2018 - A Pose-Sensitive Embedding for Person Re-Identific.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\H94MLMNV\\Sarfraz_A_Pose-Sensitive_Embedding_CVPR_2018_paper.html:text/html}
}

@inproceedings{deng_image-image_2018,
	title = {Image-{Image} {Domain} {Adaptation} {With} {Preserved} {Self}-{Similarity} and {Domain}-{Dissimilarity} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Deng_Image-Image_Domain_Adaptation_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Deng, Weijian and Zheng, Liang and Ye, Qixiang and Kang, Guoliang and Yang, Yi and Jiao, Jianbin},
	year = {2018},
	pages = {994--1003},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\CJIGM48N\\Deng et al. - 2018 - Image-Image Domain Adaptation With Preserved Self-.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\IUA7XJQZ\\Deng_Image-Image_Domain_Adaptation_CVPR_2018_paper.html:text/html}
}

@inproceedings{kalayeh_human_2018,
	title = {Human {Semantic} {Parsing} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Kalayeh_Human_Semantic_Parsing_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Kalayeh, Mahdi M. and Basaran, Emrah and Gökmen, Muhittin and Kamasak, Mustafa E. and Shah, Mubarak},
	year = {2018},
	pages = {1062--1071},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\HFH9H8AD\\Kalayeh et al. - 2018 - Human Semantic Parsing for Person Re-Identificatio.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\KRRJPYZ2\\Kalayeh_Human_Semantic_Parsing_CVPR_2018_paper.html:text/html}
}

@inproceedings{chen_video_2018,
	title = {Video {Person} {Re}-{Identification} {With} {Competitive} {Snippet}-{Similarity} {Aggregation} and {Co}-{Attentive} {Snippet} {Embedding}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Video_Person_Re-Identification_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Chen, Dapeng and Li, Hongsheng and Xiao, Tong and Yi, Shuai and Wang, Xiaogang},
	year = {2018},
	pages = {1169--1178},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\PJEUP9YD\\Chen et al. - 2018 - Video Person Re-Identification With Competitive Sn.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\ZPVKAUT3\\Chen_Video_Person_Re-Identification_CVPR_2018_paper.html:text/html}
}

@inproceedings{song_mask-guided_2018,
	title = {Mask-{Guided} {Contrastive} {Attention} {Model} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Song_Mask-Guided_Contrastive_Attention_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Song, Chunfeng and Huang, Yan and Ouyang, Wanli and Wang, Liang},
	year = {2018},
	pages = {1179--1188},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\LI54VCDU\\Song et al. - 2018 - Mask-Guided Contrastive Attention Model for Person.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\2RMRGMJV\\Song_Mask-Guided_Contrastive_Attention_CVPR_2018_paper.html:text/html}
}

@inproceedings{wang_person_2018,
	title = {Person {Re}-{Identification} {With} {Cascaded} {Pairwise} {Convolutions}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Person_Re-Identification_With_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Wang, Yicheng and Chen, Zhenzhong and Wu, Feng and Wang, Gang},
	year = {2018},
	pages = {1470--1478},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\WXIQ6TPH\\Wang et al. - 2018 - Person Re-Identification With Cascaded Pairwise Co.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\J37H5KVM\\Wang_Person_Re-Identification_With_CVPR_2018_paper.html:text/html}
}

@inproceedings{chang_multi-level_2018,
	title = {Multi-{Level} {Factorisation} {Net} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Chang_Multi-Level_Factorisation_Net_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Chang, Xiaobin and Hospedales, Timothy M. and Xiang, Tao},
	year = {2018},
	pages = {2109--2118},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\LUFYIMMT\\Chang et al. - 2018 - Multi-Level Factorisation Net for Person Re-Identi.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\P297YIPH\\Chang_Multi-Level_Factorisation_Net_CVPR_2018_paper.html:text/html}
}

@inproceedings{xu_attention-aware_2018,
	title = {Attention-{Aware} {Compositional} {Network} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Attention-Aware_Compositional_Network_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Xu, Jing and Zhao, Rui and Zhu, Feng and Wang, Huaming and Ouyang, Wanli},
	year = {2018},
	pages = {2119--2128},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\2D5KLYN4\\Xu et al. - 2018 - Attention-Aware Compositional Network for Person R.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\6M4PU3FF\\Xu_Attention-Aware_Compositional_Network_CVPR_2018_paper.html:text/html}
}

@inproceedings{huang_unifying_2018,
	title = {Unifying {Identification} and {Context} {Learning} for {Person} {Recognition}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Unifying_Identification_and_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Huang, Qingqiu and Xiong, Yu and Lin, Dahua},
	year = {2018},
	pages = {2217--2225},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\ULTMJR57\\Huang et al. - 2018 - Unifying Identification and Context Learning for P.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\8HARJMIS\\Huang_Unifying_Identification_and_CVPR_2018_paper.html:text/html}
}

@inproceedings{shen_deep_2018,
	title = {Deep {Group}-{Shuffling} {Random} {Walk} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Deep_Group-Shuffling_Random_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Shen, Yantao and Li, Hongsheng and Xiao, Tong and Yi, Shuai and Chen, Dapeng and Wang, Xiaogang},
	year = {2018},
	pages = {2265--2274},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\FD2PTN5J\\Shen et al. - 2018 - Deep Group-Shuffling Random Walk for Person Re-Ide.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\RMV77FR9\\Shen_Deep_Group-Shuffling_Random_CVPR_2018_paper.html:text/html}
}

@inproceedings{wang_transferable_2018,
	title = {Transferable {Joint} {Attribute}-{Identity} {Deep} {Learning} for {Unsupervised} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Transferable_Joint_Attribute-Identity_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Wang, Jingya and Zhu, Xiatian and Gong, Shaogang and Li, Wei},
	year = {2018},
	pages = {2275--2284},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\WTI2DWRB\\Wang et al. - 2018 - Transferable Joint Attribute-Identity Deep Learnin.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\GWYIVBBR\\Wang_Transferable_Joint_Attribute-Identity_CVPR_2018_paper.html:text/html}
}

@inproceedings{guo_efficient_2018,
	title = {Efficient and {Deep} {Person} {Re}-{Identification} {Using} {Multi}-{Level} {Similarity}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Guo_Efficient_and_Deep_CVPR_2018_paper.html},
	urldate = {2021-04-20},
	author = {Guo, Yiluan and Cheung, Ngai-Man},
	year = {2018},
	pages = {2335--2344},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\XLVTFG79\\Guo and Cheung - 2018 - Efficient and Deep Person Re-Identification Using .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\K2LSKZEM\\Guo_Efficient_and_Deep_CVPR_2018_paper.html:text/html}
}

@article{karanam_systematic_2019,
	title = {A {Systematic} {Evaluation} and {Benchmark} for {Person} {Re}-{Identification}: {Features}, {Metrics}, and {Datasets}},
	volume = {41},
	issn = {1939-3539},
	shorttitle = {A {Systematic} {Evaluation} and {Benchmark} for {Person} {Re}-{Identification}},
	doi = {10.1109/TPAMI.2018.2807450},
	abstract = {Person re-identification (re-id) is a critical problem in video analytics applications such as security and surveillance. The public release of several datasets and code for vision algorithms has facilitated rapid progress in this area over the last few years. However, directly comparing re-id algorithms reported in the literature has become difficult since a wide variety of features, experimental protocols, and evaluation metrics are employed. In order to address this need, we present an extensive review and performance evaluation of single- and multi-shot re-id algorithms. The experimental protocol incorporates the most recent advances in both feature extraction and metric learning. To ensure a fair comparison, all of the approaches were implemented using a unified code library that includes 11 feature extraction algorithms and 22 metric learning and ranking techniques. All approaches were evaluated using a new large-scale dataset that closely mimics a real-world problem setting, in addition to 16 other publicly available datasets: VIPeR, GRID, CAVIAR, DukeMTMC4ReID, 3DPeS, PRID, V47, WARD, SAIVT-SoftBio, CUHK01, CHUK02, CUHK03, RAiD, iLIDSVID, HDA+, and Market1501. The evaluation codebase and results will be made publicly available for community use.},
	number = {3},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {karanam, S. and Gou, M. and Wu, Z. and Rates-Borras, A. and Camps, O. and Radke, R. J.},
	month = mar,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Person re-identification, Cameras, Feature extraction, Benchmark testing, Measurement, Probes, benchmark, camera network, Histograms, Image color analysis, video analytics},
	pages = {523--536},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\R5K4RFSA\\8294254.html:text/html;Submitted Version:C\:\\Users\\lxt76\\Zotero\\storage\\6ETP3T9R\\karanam et al. - 2019 - A Systematic Evaluation and Benchmark for Person R.pdf:application/pdf}
}

@article{wang_intelligent_2013,
	series = {Extracting {Semantics} from {Multi}-{Spectrum} {Video}},
	title = {Intelligent multi-camera video surveillance: {A} review},
	volume = {34},
	issn = {0167-8655},
	shorttitle = {Intelligent multi-camera video surveillance},
	url = {https://www.sciencedirect.com/science/article/pii/S016786551200219X},
	doi = {10.1016/j.patrec.2012.07.005},
	abstract = {Intelligent multi-camera video surveillance is a multidisciplinary field related to computer vision, pattern recognition, signal processing, communication, embedded computing and image sensors. This paper reviews the recent development of relevant technologies from the perspectives of computer vision and pattern recognition. The covered topics include multi-camera calibration, computing the topology of camera networks, multi-camera tracking, object re-identification, multi-camera activity analysis and cooperative video surveillance both with active and static cameras. Detailed descriptions of their technical challenges and comparison of different solutions are provided. It emphasizes the connection and integration of different modules in various environments and application scenarios. According to the most recent works, some problems can be jointly solved in order to improve the efficiency and accuracy. With the fast development of surveillance systems, the scales and complexities of camera networks are increasing and the monitored environments are becoming more and more complicated and crowded. This paper discusses how to face these emerging challenges.},
	language = {en},
	number = {1},
	urldate = {2021-04-21},
	journal = {Pattern Recognition Letters},
	author = {Wang, Xiaogang},
	month = jan,
	year = {2013},
	keywords = {Multi-camera activity analysis, Multi-camera calibration, Multi-camera tracking, Multi-camera video surveillance, Object re-identification, Topology of camera networks},
	pages = {3--19},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\QXL352ZP\\Wang - 2013 - Intelligent multi-camera video surveillance A rev.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\VPNVI2JQ\\S016786551200219X.html:text/html}
}

@inproceedings{gheissari_person_2006,
	title = {Person {Reidentification} {Using} {Spatiotemporal} {Appearance}},
	volume = {2},
	doi = {10.1109/CVPR.2006.223},
	abstract = {In many surveillance applications it is desirable to determine if a given individual has been previously observed over a network of cameras. This is the person reidentification problem. This paper focuses on reidentification algorithms that use the overall appearance of an individual as opposed to passive biometrics such as face and gait. Person reidentification approaches have two aspects: (i) establish correspondence between parts, and (ii) generate signatures that are invariant to variations in illumination, pose, and the dynamic appearance of clothing. A novel spatiotemporal segmentation algorithm is employed to generate salient edgels that are robust to changes in appearance of clothing. The invariant signatures are generated by combining normalized color and salient edgel histograms. Two approaches are proposed to generate correspondences: (i) a model based approach that fits an articulated model to each individual to establish a correspondence map, and (ii) an interest point operator approach that nominates a large number of potential correspondences which are evaluated using a region growing scheme. Finally, the approaches are evaluated on a 44 person database across 3 disparate views.},
	booktitle = {2006 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'06)},
	author = {Gheissari, N. and Sebastian, T.B. and Hartley, R.},
	month = jun,
	year = {2006},
	note = {ISSN: 1063-6919},
	keywords = {Cameras, Robustness, Histograms, Biometrics, Brightness, Clothing, Face detection, Lighting, Spatiotemporal phenomena, Surveillance},
	pages = {1528--1535},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\427LKTPH\\1640938.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\CESTUU8S\\Gheissari et al. - 2006 - Person Reidentification Using Spatiotemporal Appea.pdf:application/pdf}
}

@inproceedings{zheng_scalable_2015,
	title = {Scalable {Person} {Re}-{Identification}: {A} {Benchmark}},
	shorttitle = {Scalable {Person} {Re}-{Identification}},
	url = {https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.html},
	urldate = {2021-04-22},
	author = {Zheng, Liang and Shen, Liyue and Tian, Lu and Wang, Shengjin and Wang, Jingdong and Tian, Qi},
	year = {2015},
	pages = {1116--1124},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\DUQNGYSX\\Zheng et al. - 2015 - Scalable Person Re-Identification A Benchmark.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\3TGAVSKP\\Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.html:text/html}
}

@inproceedings{zhao_unsupervised_2013,
	title = {Unsupervised {Salience} {Learning} for {Person} {Re}-identification},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2013/html/Zhao_Unsupervised_Salience_Learning_2013_CVPR_paper.html},
	urldate = {2021-04-22},
	author = {Zhao, Rui and Ouyang, Wanli and Wang, Xiaogang},
	year = {2013},
	pages = {3586--3593},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\P9E9GTBI\\Zhao et al. - 2013 - Unsupervised Salience Learning for Person Re-ident.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\54PXHXI8\\Zhao_Unsupervised_Salience_Learning_2013_CVPR_paper.html:text/html}
}

@inproceedings{zheng_person_2011,
	title = {Person re-identification by probabilistic relative distance comparison},
	doi = {10.1109/CVPR.2011.5995598},
	abstract = {Matching people across non-overlapping camera views, known as person re-identification, is challenging due to the lack of spatial and temporal constraints and large visual appearance changes caused by variations in view angle, lighting, background clutter and occlusion. To address these challenges, most previous approaches aim to extract visual features that are both distinctive and stable under appearance changes. However, most visual features and their combinations under realistic conditions are neither stable nor distinctive thus should not be used indiscriminately. In this paper, we propose to formulate person re-identification as a distance learning problem, which aims to learn the optimal distance that can maximises matching accuracy regardless the choice of representation. To that end, we introduce a novel Probabilistic Relative Distance Comparison (PRDC) model, which differs from most existing distance learning methods in that, rather than minimising intra-class variation whilst maximising intra-class variation, it aims to maximise the probability of a pair of true match having a smaller distance than that of a wrong match pair. This makes our model more tolerant to appearance changes and less susceptible to model over-fitting. Extensive experiments are carried out to demonstrate that 1) by formulating the person re-identification problem as a distance learning problem, notable improvement on matching accuracy can be obtained against conventional person re-identification techniques, which is particularly significant when the training sample size is small; and 2) our PRDC outperforms not only existing distance learning methods but also alternative learning methods based on boosting and learning to rank.},
	booktitle = {{CVPR} 2011},
	author = {Zheng, Wei-Shi and Gong, Shaogang and Xiang, Tao},
	month = jun,
	year = {2011},
	note = {ISSN: 1063-6919},
	keywords = {Cameras, Feature extraction, Training, Computer aided instruction, Learning systems, Optimization, Probabilistic logic},
	pages = {649--656},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\M8PQS4UE\\5995598.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\QA7KDSB4\\Zheng et al. - 2011 - Person re-identification by probabilistic relative.pdf:application/pdf}
}

@inproceedings{li_harmonious_2018,
	title = {Harmonious {Attention} {Network} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Harmonious_Attention_Network_CVPR_2018_paper.html},
	urldate = {2021-04-22},
	author = {Li, Wei and Zhu, Xiatian and Gong, Shaogang},
	year = {2018},
	keywords = {Attention deep learning models},
	pages = {2285--2294},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\L26E2LTV\\Li et al. - 2018 - Harmonious Attention Network for Person Re-Identif.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\2DYEBWAA\\Li_Harmonious_Attention_Network_CVPR_2018_paper.html:text/html}
}

@inproceedings{farenzena_person_2010,
	title = {Person re-identification by symmetry-driven accumulation of local features},
	doi = {10.1109/CVPR.2010.5539926},
	abstract = {In this paper, we present an appearance-based method for person re-identification. It consists in the extraction of features that model three complementary aspects of the human appearance: the overall chromatic content, the spatial arrangement of colors into stable regions, and the presence of recurrent local motifs with high entropy. All this information is derived from different body parts, and weighted opportunely by exploiting symmetry and asymmetry perceptual principles. In this way, robustness against very low resolution, occlusions and pose, viewpoint and illumination changes is achieved. The approach applies to situations where the number of candidates varies continuously, considering single images or bunch of frames for each individual. It has been tested on several public benchmark datasets (ViPER, iLIDS, ETHZ), gaining new state-of-the-art performances.},
	booktitle = {2010 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Farenzena, M. and Bazzani, L. and Perina, A. and Murino, V. and Cristani, M.},
	month = jun,
	year = {2010},
	note = {ISSN: 1063-6919},
	keywords = {Feature extraction, Benchmark testing, Robustness, Lighting, Biological system modeling, Data mining, Entropy, Humans, Performance evaluation, Spatial resolution},
	pages = {2360--2367},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\CARDRSHV\\5539926.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\S2U3R5J2\\Farenzena et al. - 2010 - Person re-identification by symmetry-driven accumu.pdf:application/pdf}
}

@inproceedings{yi_deep_2014,
	title = {Deep {Metric} {Learning} for {Person} {Re}-identification},
	doi = {10.1109/ICPR.2014.16},
	abstract = {Various hand-crafted features and metric learning methods prevail in the field of person re-identification. Compared to these methods, this paper proposes a more general way that can learn a similarity metric from image pixels directly. By using a "siamese" deep neural network, the proposed method can jointly learn the color feature, texture feature and metric in a unified framework. The network has a symmetry structure with two sub-networks which are connected by a cosine layer. Each sub network includes two convolutional layers and a full connected layer. To deal with the big variations of person images, binomial deviance is used to evaluate the cost between similarities and labels, which is proved to be robust to outliers. Experiments on VIPeR illustrate the superior performance of our method and a cross database experiment also shows its good generalization.},
	booktitle = {2014 22nd {International} {Conference} on {Pattern} {Recognition}},
	author = {Yi, Dong and Lei, Zhen and Liao, Shengcai and Li, Stan Z.},
	month = aug,
	year = {2014},
	note = {ISSN: 1051-4651},
	keywords = {Cameras, Training, Measurement, Image color analysis, Databases, Neural networks, Testing},
	pages = {34--39},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\GNL7VATZ\\6976727.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\XSVZ9QIY\\Yi et al. - 2014 - Deep Metric Learning for Person Re-identification.pdf:application/pdf}
}

@inproceedings{zhao_person_2013,
	title = {Person {Re}-identification by {Salience} {Matching}},
	url = {https://openaccess.thecvf.com/content_iccv_2013/html/Zhao_Person_Re-identification_by_2013_ICCV_paper.html},
	urldate = {2021-04-22},
	author = {Zhao, Rui and Ouyang, Wanli and Wang, Xiaogang},
	year = {2013},
	pages = {2528--2535},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\T7HWBJ97\\Zhao et al. - 2013 - Person Re-identification by Salience Matching.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\R2ZHXB4T\\Zhao_Person_Re-identification_by_2013_ICCV_paper.html:text/html}
}

@inproceedings{layne_person_2012,
	address = {Surrey},
	title = {Person {Re}-identification by {Attributes}},
	isbn = {978-1-901725-46-9},
	url = {http://www.bmva.org/bmvc/2012/BMVC/paper024/index.html},
	doi = {10.5244/C.26.24},
	abstract = {Visually identifying a target individual reliably in a crowded environment observed by a distributed camera network is critical to a variety of tasks in managing business information, border control, and crime prevention. Automatic re-identiﬁcation of a human candidate from public space CCTV video is challenging due to spatiotemporal visual feature variations and strong visual similarity between different people, compounded by low-resolution and poor quality video data. In this work, we propose a novel method for re-identiﬁcation that learns a selection and weighting of mid-level semantic attributes to describe people. Speciﬁcally, the model learns an attribute-centric, parts-based feature representation. This differs from and complements existing low-level features for re-identiﬁcation that rely purely on bottom-up statistics for feature selection, which are limited in discriminating and identifying reliably visual appearances of target people appearing in different camera views under certain degrees of occlusion due to crowdedness. Our experiments demonstrate the effectiveness of our approach compared to existing feature representations when applied to benchmarking datasets.},
	language = {en},
	urldate = {2021-04-22},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2012},
	publisher = {British Machine Vision Association},
	author = {Layne, Ryan and Hospedales, Tim and Gong, Shaogang},
	year = {2012},
	pages = {24.1--24.11},
	file = {Layne et al. - 2012 - Person Re-identification by Attributes.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\W5MWSFGR\\Layne et al. - 2012 - Person Re-identification by Attributes.pdf:application/pdf}
}

@article{zhou_learning_2021,
	title = {Learning {Generalisable} {Omni}-{Scale} {Representations} for {Person} {Re}-{Identification}},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2021.3069237},
	abstract = {An effective person re-identification (re-ID) model should learn feature representations that are both discriminative, for distinguishing similar-looking people, and generalisable, for deployment across datasets without any adaptation. In this paper, we develop novel CNN architectures to address both challenges. First, we present a re-ID CNN termed omni-scale network (OSNet) to learn features that not only capture different spatial scales but also encapsulate a synergistic combination of multiple scales, namely omni-scale features. The basic building block consists of multiple convolutional streams, each detecting features at a certain scale. For omni-scale feature learning, a unified aggregation gate is introduced to dynamically fuse multi-scale features with channel-wise weights. OSNet is lightweight as its building blocks comprise factorised convolutions. Second, to improve generalisable feature learning, we introduce instance normalisation (IN) layers into OSNet to cope with cross-dataset discrepancies. Further, to determine the optimal placements of these IN layers in the architecture, we formulate an efficient differentiable architecture search algorithm. Extensive experiments show that, in the conventional same-dataset setting, OSNet achieves state-of-the-art performance, despite being much smaller than existing re-ID models. In the more challenging yet practical cross-dataset setting, OSNet beats most recent unsupervised domain adaptation methods without using any target data.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhou, Kaiyang and Yang, Yongxin and Cavallaro, Andrea and Xiang, Tao},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Cameras, Data models, Feature extraction, Person Re-Identification, Adaptation models, Convolutional codes, Cross-Domain Re-ID, Lightweight Network, Neural Architecture Search, Omni-Scale Learning},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\IU8XGBNB\\9387612.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\D6KJWDZ5\\Zhou et al. - 2021 - Learning Generalisable Omni-Scale Representations .pdf:application/pdf}
}

@article{liu_watching_2021,
	title = {Watching {You}: {Global}-guided {Reciprocal} {Learning} for {Video}-based {Person} {Re}-identification},
	shorttitle = {Watching {You}},
	url = {http://arxiv.org/abs/2103.04337},
	abstract = {Video-based person re-identification (Re-ID) aims to automatically retrieve video sequences of the same person under non-overlapping cameras. To achieve this goal, it is the key to fully utilize abundant spatial and temporal cues in videos. Existing methods usually focus on the most conspicuous image regions, thus they may easily miss out fine-grained clues due to the person varieties in image sequences. To address above issues, in this paper, we propose a novel Global-guided Reciprocal Learning (GRL) framework for video-based person Re-ID. Specifically, we first propose a Global-guided Correlation Estimation (GCE) to generate feature correlation maps of local features and global features, which help to localize the high- and low-correlation regions for identifying the same person. After that, the discriminative features are disentangled into high-correlation features and low-correlation features under the guidance of the global representations. Moreover, a novel Temporal Reciprocal Learning (TRL) mechanism is designed to sequentially enhance the high-correlation semantic information and accumulate the low-correlation sub-critical clues. Extensive experiments are conducted on three public benchmarks. The experimental results indicate that our approach can achieve better performance than other state-of-the-art approaches. The code is released at https://github.com/flysnowtiger/GRL.},
	urldate = {2021-04-22},
	journal = {arXiv:2103.04337 [cs]},
	author = {Liu, Xuehu and Zhang, Pingping and Yu, Chenyang and Lu, Huchuan and Yang, Xiaoyun},
	month = apr,
	year = {2021},
	note = {arXiv: 2103.04337},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\U94L2XQT\\Liu et al. - 2021 - Watching You Global-guided Reciprocal Learning fo.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\52YRUJAD\\2103.html:text/html}
}

@article{bedagkar-gala_survey_2014,
	title = {A survey of approaches and trends in person re-identification},
	volume = {32},
	issn = {0262-8856},
	url = {https://www.sciencedirect.com/science/article/pii/S0262885614000262},
	doi = {10.1016/j.imavis.2014.02.001},
	abstract = {Person re-identification is a fundamental task in automated video surveillance and has been an area of intense research in the past few years. Given an image/video of a person taken from one camera, re-identification is the process of identifying the person from images/videos taken from a different camera. Re-identification is indispensable in establishing consistent labeling across multiple cameras or even within the same camera to re-establish disconnected or lost tracks. Apart from surveillance it has applications in robotics, multimedia and forensics. Person re-identification is a difficult problem because of the visual ambiguity and spatiotemporal uncertainty in a person's appearance across different cameras. These difficulties are often compounded by low resolution images or poor quality video feeds with large amounts of unrelated information in them that does not aid re-identification. The spatial or temporal conditions to constrain the problem are hard to capture. However, the problem has received significant attention from the computer vision research community due to its wide applicability and utility. In this paper, we explore the problem of person re-identification and discuss the current solutions. Open issues and challenges of the problem are highlighted with a discussion on potential directions for further research.},
	language = {en},
	number = {4},
	urldate = {2021-04-22},
	journal = {Image and Vision Computing},
	author = {Bedagkar-Gala, Apurva and Shah, Shishir K.},
	month = apr,
	year = {2014},
	keywords = {Person re-identification, Multi-camera tracking, Closed set Re-ID, Open set Re-ID, Short and long period Re-ID, Video surveillance},
	pages = {270--286},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\Y3W92ITE\\Bedagkar-Gala and Shah - 2014 - A survey of approaches and trends in person re-ide.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\6954M38G\\S0262885614000262.html:text/html}
}

@article{wu_deep_2021,
	title = {Deep features for person re-identification on metric learning},
	volume = {110},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320302272},
	doi = {10.1016/j.patcog.2020.107424},
	abstract = {Person re-identification, a branch of image retrieval, is an increasingly important public safety application. When monitoring larger areas, it is crucial to correctly match the same person in different camera views. With the emergence of deep learning and large-scale data, metric learning has significantly improved person re-identification performance, but the extent to which deep features affect metric learning performance is unknown. However, given the large number of approaches, datasets, evaluation indices, and experimental environments, comparing metric learning methods directly is difficult. To obtain a more comprehensive empirical evaluation of the person re-identification, here we summarize the different types of features and metric learning approaches from a label attributes perspective. Then, by combining advanced approaches to data enhancement and feature extraction, we conduct comprehensive experiments on metric learning methods with two datasets. For fairness, all methods use a unified code library that includes two data enhancement schemes, eight feature extraction algorithms, and eight metric learning methods. Our results show that, the relations of loss function with deep feature space and metric learning.},
	language = {en},
	urldate = {2021-04-22},
	journal = {Pattern Recognition},
	author = {Wu, Wanyin and Tao, Dapeng and Li, Hao and Yang, Zhao and Cheng, Jun},
	month = feb,
	year = {2021},
	keywords = {Deep features, Person re-identification, Empirical comparison, Metric learning},
	pages = {107424},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\QQ4IA2RY\\Wu et al. - 2021 - Deep features for person re-identification on metr.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\UG4SXKBJ\\S0031320320302272.html:text/html}
}

@article{liu_prgcn_2021,
	title = {{PrGCN}: {Probability} prediction with graph convolutional network for person re-identification},
	volume = {423},
	issn = {0925-2312},
	shorttitle = {{PrGCN}},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231220315150},
	doi = {10.1016/j.neucom.2020.10.019},
	abstract = {Robust similarity measurement is an important issue for person re-identification (ReID). Most existing ReID models estimate the similarity between query and gallery images by computing their Euclidean distances while ignoring the rich context information contained in the image space. In this paper, we propose a graph convolutional network (GCN) based method to improve the similarity measurement in ReID, which regards the ReID task as a prediction problem of the link probability between node pairs. Our method is named as PrGCN (Probability GCN), in which each person is regarded as an instance node. Firstly, an Instance Centered Sub-graphs (ICS) is constructed for each instance node to depict its rich local context information. Secondly, the constructed ICS is input to a GCN to infer and predict the link probability of node pairs, followed by a similarity ranking between the query and gallery images according to the predicted probabilities. Extensive experiments show that the proposed method improves the mAP and Top-1 accuracy of ReID significantly, yielding better or comparable results to the state-of-the-art methods on various benchmarks (Market1501, DukeMTMC-ReID and CUHK03). In addition, we validate that the proposed PrGCN can be easily embedded into other deep learning architectures to replace Euclidean distance metric and achieve significant performance improvements.},
	language = {en},
	urldate = {2021-04-22},
	journal = {Neurocomputing},
	author = {Liu, Hongmin and Xiao, Zhenzhen and Fan, Bin and Zeng, Hui and Zhang, Yifan and Jiang, Guoquan},
	month = jan,
	year = {2021},
	keywords = {Person re-identification, Graph convolutional network, Link probability prediction, Similarity measurement},
	pages = {57--70},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\D2IURSP9\\Liu et al. - 2021 - PrGCN Probability prediction with graph convoluti.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\P7SS8VTJ\\S0925231220315150.html:text/html}
}

@article{lowe_distinctive_2004,
	title = {Distinctive {Image} {Features} from {Scale}-{Invariant} {Keypoints}},
	volume = {60},
	issn = {1573-1405},
	url = {https://doi.org/10.1023/B:VISI.0000029664.99615.94},
	doi = {10.1023/B:VISI.0000029664.99615.94},
	abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
	language = {en},
	number = {2},
	urldate = {2021-04-22},
	journal = {International Journal of Computer Vision},
	author = {Lowe, David G.},
	month = nov,
	year = {2004},
	pages = {91--110},
	file = {Springer Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\SZDP2G98\\Lowe - 2004 - Distinctive Image Features from Scale-Invariant Ke.pdf:application/pdf}
}

@inproceedings{ma_local_2012,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Local {Descriptors} {Encoded} by {Fisher} {Vectors} for {Person} {Re}-identification},
	isbn = {978-3-642-33863-2},
	doi = {10.1007/978-3-642-33863-2_41},
	abstract = {This paper proposes a new descriptor for person re-identification building on the recent advances of Fisher Vectors. Specifically, a simple vector of attributes consisting in the pixel coordinates, its intensity as well as the first and second-order derivatives is computed for each pixel of the image. These local descriptors are turned into Fisher Vectors before being pooled to produce a global representation of the image. The so-obtained Local Descriptors encoded by Fisher Vector (LDFV) have been validated through experiments on two person re-identification benchmarks (VIPeR and ETHZ), achieving state-of-the-art performance on both datasets.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2012. {Workshops} and {Demonstrations}},
	publisher = {Springer},
	author = {Ma, Bingpeng and Su, Yu and Jurie, Frédéric},
	editor = {Fusiello, Andrea and Murino, Vittorio and Cucchiara, Rita},
	year = {2012},
	keywords = {Pairwise Constraint, Gaussian Mixture Model, IEEE Conf, Local Binary Pattern, Local Descriptor},
	pages = {413--422},
	file = {Springer Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\5ZT96IZT\\Ma et al. - 2012 - Local Descriptors Encoded by Fisher Vectors for Pe.pdf:application/pdf}
}



@inproceedings{zhao_learning_2014,
	title = {Learning {Mid}-level {Filters} for {Person} {Re}-identification},
	url = {https://openaccess.thecvf.com/content_cvpr_2014/html/Zhao_Learning_Mid-level_Filters_2014_CVPR_paper.html},
	urldate = {2021-04-22},
	author = {Zhao, Rui and Ouyang, Wanli and Wang, Xiaogang},
	year = {2014},
	pages = {144--151},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\47D34PI2\\Zhao et al. - 2014 - Learning Mid-level Filters for Person Re-identific.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\6A75AQHG\\Zhao_Learning_Mid-level_Filters_2014_CVPR_paper.html:text/html}
}

@article{wang_survey_2018,
	title = {Survey on person re-identification based on deep learning},
	doi = {10.1049/TRIT.2018.1001},
	abstract = {Person re-identification (Re-ID) is a fundamental subject in the field of the computer vision technologies. The traditional methods of person Re-ID have difficulty in solving the problems of person illumination, occlusion and attitude change under complex background. Meanwhile, the introduction of deep learning opens a new way of person Re-ID research and becomes a hot spot in this field. This study reviews the traditional methods of person Re-ID, then the authors focus on the related papers about different person Re-ID frameworks on the basis of deep learning, and discusses their advantages and disadvantages. Finally, they propose the direction of further research, especially the prospect of person Re-ID methods based on deep learning.},
	journal = {CAAI Trans. Intell. Technol.},
	author = {Wang, K. and Wang, H. and Liu, Meichen and Xing, Xianglei and Han, Tian},
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\N9N3GEPQ\\Wang et al. - 2018 - Survey on person re-identification based on deep l.pdf:application/pdf}
}

@article{wang_comprehensive_2020,
	title = {A {Comprehensive} {Overview} of {Person} {Re}-{Identification} {Approaches}},
	doi = {10.1109/ACCESS.2020.2978344},
	abstract = {Person re-identification, identifying and tracking pedestrians in cross-domain monitoring systems, is an important technology in the computer vision field and of real significance for the construction of smart cities. With the development of deep learning techniques, especially convolutional neural networks, this technology has received more extensive attention and improvement in recent years and a large number of noteworthy achievements have emerged. This paper provides a comprehensive overview of person re-identification approaches to assist researchers in quickly understand this field with preference as well as to provide a more structured framework. By reviewing more than 300 re-identification related papers, the focus of these studies is summarized as information extraction, metric learning, post-processing, efficiency improvement, labeling cost reduction, and data type expansion. This classification is then organized based on different technologies, and on this basis, the pros and cons of each technology are analyzed. Moreover, this overview summarizes the difficulties and challenges of re-identification and discusses the possible research directions for reference.},
	journal = {IEEE Access},
	author = {Wang, H. and Du, Haomin and Zhao, Y. and Yan, Jiming},
	year = {2020},
	file = {Full Text:C\:\\Users\\lxt76\\Zotero\\storage\\BWAYDFBB\\Wang et al. - 2020 - A Comprehensive Overview of Person Re-Identificati.pdf:application/pdf}
}

@article{he_person_2021,
	title = {Person {Re}-{Identification} by {Effective} {Features} and {Self}-{Optimized} {Pseudo}-{Label}},
	doi = {10.1109/ACCESS.2021.3062281},
	abstract = {With the development of deep learning, person re-identification (ReID) has been widely concerned and studied. At present, in practical application, there are three main problems in person ReID: first, it is difficult to locate the target person because the person is frequently partially occluded in crowed scenes; second, it is difficult to match the target person due to the similarity of the target person and other pedestrian features; third, the problem of model performance degradation caused by the large style discrepancies across domain/datasets. These three problems greatly limit the application of person ReID in real scenes. To solve these problems, we proposed a person ReID method based on effective features and self-optimized pseudo-label. Firstly, we designed a feature aggregation module which combines mask channel and pose channel to accurately extract the global saliency features, so as to solve the occlusion problem; secondly, we designed a head-shoulder feature auxiliary module to enhance the feature representation of the head-shoulder, so as to solve the problem of similarity between the target person and other pedestrian features; finally, we designed a self-optimized pseudo-label training module to improves the generalization ability of the model, so as to solve the problem of different styles in the cross-domain environment. Extensive contrast experiments with the state-of-the-art methods on multiple person re-ID datasets show that our method leads to significant improvement, which prove the effectiveness of our method.},
	journal = {IEEE Access},
	author = {He, Ming-Xiang and Gao, Jin-Fang and Li, Guan and Xin, You-Zhi},
	year = {2021},
	file = {Full Text:C\:\\Users\\lxt76\\Zotero\\storage\\5F25NT99\\He et al. - 2021 - Person Re-Identification by Effective Features and.pdf:application/pdf}
}

@article{bai_object_2020,
	title = {Object {Detection} {Recognition} and {Robot} {Grasping} {Based} on {Machine} {Learning}: {A} {Survey}},
	shorttitle = {Object {Detection} {Recognition} and {Robot} {Grasping} {Based} on {Machine} {Learning}},
	doi = {10.1109/ACCESS.2020.3028740},
	abstract = {With the rapid development of machine learning, its powerful function in the machine vision field is increasingly reflected. The combination of machine vision and robotics to achieve the same precise and fast grasping as that of humans requires high-precision target detection and recognition, location and reasonable grasp strategy generation, which is the ultimate goal of global researchers and one of the prerequisites for the large-scale application of robots. Traditional machine learning has a long history and good achievements in the field of image processing and robot control. The CNN (convolutional neural network) algorithm realizes training of large-scale image datasets, solves the disadvantages of traditional machine learning in large datasets, and greatly improves accuracy, thereby positioning CNNs as a global research hotspot. However, the increasing difficulty of labeled data acquisition limits their development. Therefore, unsupervised learning, self-supervised learning and reinforcement learning, which are less dependent on labeled data, have also undergone rapid development and achieved good performance in the fields of image processing and robot capture. According to the inherent defects of vision, this paper summarizes the research achievements of tactile feedback in the fields of target recognition and robot grasping and finds that the combination of vision and tactile feedback can improve the success rate and robustness of robot grasping. This paper provides a systematic summary and analysis of the research status of machine vision and tactile feedback in the field of robot grasping and establishes a reasonable reference for future research.},
	journal = {IEEE Access},
	author = {Bai, Qiang and Li, Shaobo and Yang, Jing and Song, Qisong and Li, Zhiang and Zhang, Xingxing},
	year = {2020},
	file = {Full Text:C\:\\Users\\lxt76\\Zotero\\storage\\MTE7HQWV\\Bai et al. - 2020 - Object Detection Recognition and Robot Grasping Ba.pdf:application/pdf}
}

@article{fisher_use_1936,
	title = {The {Use} of {Multiple} {Measurements} in {Taxonomic} {Problems}},
	volume = {7},
	copyright = {1936 Blackwell Publishing Ltd/University College London},
	issn = {2050-1439},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x},
	doi = {https://doi.org/10.1111/j.1469-1809.1936.tb02137.x},
	abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
	language = {en},
	number = {2},
	urldate = {2021-04-25},
	journal = {Annals of Eugenics},
	author = {Fisher, R. A.},
	year = {1936},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x},
	pages = {179--188},
	file = {extractexcitation.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\BPT2Z2PU\\extractexcitation.pdf:application/pdf;Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\MN9S5V5M\\Fisher - 1936 - The Use of Multiple Measurements in Taxonomic Prob.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\4EHIPF23\\j.1469-1809.1936.tb02137.html:text/html}
}

@inproceedings{ahmed_improved_2015,
	title = {An improved deep learning architecture for person re-identification},
	doi = {10.1109/CVPR.2015.7299016},
	abstract = {In this work, we propose a method for simultaneously learning features and a corresponding similarity metric for person re-identification. We present a deep convolutional architecture with layers specially designed to address the problem of re-identification. Given a pair of images as input, our network outputs a similarity value indicating whether the two input images depict the same person. Novel elements of our architecture include a layer that computes cross-input neighborhood differences, which capture local relationships between the two input images based on mid-level features from each input image. A high-level summary of the outputs of this layer is computed by a layer of patch summary features, which are then spatially integrated in subsequent layers. Our method significantly outperforms the state of the art on both a large data set (CUHK03) and a medium-sized data set (CUHK01), and is resistant to over-fitting. We also demonstrate that by initially training on an unrelated large data set before fine-tuning on a small target data set, our network can achieve results comparable to the state of the art even on a small data set (VIPeR).},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Ahmed, Ejaz and Jones, Michael and Marks, Tim K.},
	month = jun,
	year = {2015},
	note = {ISSN: 1063-6919},
	keywords = {Feature extraction, Training, Measurement, Machine learning, Image color analysis, Convolution, Computer architecture},
	pages = {3908--3916},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\6794HEZZ\\7299016.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\I5QIP3HJ\\Ahmed et al. - 2015 - An improved deep learning architecture for person .pdf:application/pdf}
}

@article{wu_personnet_2016,
	title = {{PersonNet}: {Person} {Re}-identification with {Deep} {Convolutional} {Neural} {Networks}},
	shorttitle = {{PersonNet}},
	abstract = {In this paper, we propose a deep end-to-end neu- ral network to simultaneously learn high-level features and a corresponding similarity metric for person re-identification. The network takes a pair of raw RGB images as input, and outputs a similarity value indicating whether the two input images depict the same person. A layer of computing neighborhood range differences across two input images is employed to capture local relationship between patches. This operation is to seek a robust feature from input images. By increasing the depth to 10 weight layers and using very small (3\${\textbackslash}times\$3) convolution filters, our architecture achieves a remarkable improvement on the prior-art configurations. Meanwhile, an adaptive Root- Mean-Square (RMSProp) gradient decent algorithm is integrated into our architecture, which is beneficial to deep nets. Our method consistently outperforms state-of-the-art on two large datasets (CUHK03 and Market-1501), and a medium-sized data set (CUHK01).},
	journal = {ArXiv},
	author = {Wu, L. and Shen, Chunhua and Hengel, A. V.},
	year = {2016},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\5MMSKFF9\\Wu et al. - 2016 - PersonNet Person Re-identification with Deep Conv.pdf:application/pdf}
}

@inproceedings{wang_joint_2016,
	title = {Joint {Learning} of {Single}-{Image} and {Cross}-{Image} {Representations} for {Person} {Re}-identification},
	doi = {10.1109/CVPR.2016.144},
	abstract = {Person re-identification has been usually solved as either the matching of single-image representation (SIR) or the classification of cross-image representation (CIR). In this work, we exploit the connection between these two categories of methods, and propose a joint learning frame-work to unify SIR and CIR using convolutional neural network (CNN). Specifically, our deep architecture contains one shared sub-network together with two sub-networks that extract the SIRs of given images and the CIRs of given image pairs, respectively. The SIR sub-network is required to be computed once for each image (in both the probe and gallery sets), and the depth of the CIR sub-network is required to be minimal to reduce computational burden. Therefore, the two types of representation can be jointly optimized for pursuing better matching accuracy with moderate computational cost. Furthermore, the representations learned with pairwise comparison and triplet comparison objectives can be combined to improve matching performance. Experiments on the CUHK03, CUHK01 and VIPeR datasets show that the proposed method can achieve favorable accuracy while compared with state-of-the-arts.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Wang, Faqiang and Zuo, Wangmeng and Lin, Liang and Zhang, David and Zhang, Lei},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {Feature extraction, Probes, Computer architecture, Bayes methods, Computational efficiency, Euclidean distance},
	pages = {1288--1296},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\ALGZ6IJ9\\7780513.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\6HUB6TGT\\Wang et al. - 2016 - Joint Learning of Single-Image and Cross-Image Rep.pdf:application/pdf}
}

@article{subramaniam_deep_2016,
	title = {Deep {Neural} {Networks} with {Inexact} {Matching} for {Person} {Re}-{Identification}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/e56b06c51e1049195d7b26d043c478a0-Abstract.html},
	language = {en},
	urldate = {2021-04-25},
	journal = {Advances in Neural Information Processing Systems},
	author = {Subramaniam, Arulkumar and Chatterjee, Moitreya and Mittal, Anurag},
	year = {2016},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\8F6HXBG2\\Subramaniam et al. - 2016 - Deep Neural Networks with Inexact Matching for Per.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\3MPNLWU9\\e56b06c51e1049195d7b26d043c478a0-Abstract.html:text/html}
}

@article{eom_learning_2019,
	title = {Learning {Disentangled} {Representation} for {Robust} {Person} {Re}-identification},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/d3aeec875c479e55d1cdeea161842ec6-Abstract.html},
	language = {en},
	urldate = {2021-04-25},
	journal = {Advances in Neural Information Processing Systems},
	author = {Eom, Chanho and Ham, Bumsub},
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\86NX83YF\\Eom and Ham - 2019 - Learning Disentangled Representation for Robust Pe.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\C2Z3JKJX\\d3aeec875c479e55d1cdeea161842ec6-Abstract.html:text/html}
}

@article{ge_fd-gan_2018,
	title = {{FD}-{GAN}: {Pose}-guided {Feature} {Distilling} {GAN} for {Robust} {Person} {Re}-identification},
	volume = {31},
	shorttitle = {{FD}-{GAN}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/c5ab0bc60ac7929182aadd08703f1ec6-Abstract.html},
	language = {en},
	urldate = {2021-04-25},
	journal = {Advances in Neural Information Processing Systems},
	author = {Ge, Yixiao and Li, Zhuowan and Zhao, Haiyu and Yin, Guojun and Yi, Shuai and Wang, Xiaogang and Li, Hongsheng},
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\VRDRJE72\\Ge et al. - 2018 - FD-GAN Pose-guided Feature Distilling GAN for Rob.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\QD75HWFF\\c5ab0bc60ac7929182aadd08703f1ec6-Abstract.html:text/html}
}

@inproceedings{li_deepreid_2014,
	title = {{DeepReID}: {Deep} {Filter} {Pairing} {Neural} {Network} for {Person} {Re}-{Identification}},
	shorttitle = {{DeepReID}},
	url = {https://openaccess.thecvf.com/content_cvpr_2014/html/Li_DeepReID_Deep_Filter_2014_CVPR_paper.html},
	urldate = {2021-04-25},
	author = {Li, Wei and Zhao, Rui and Xiao, Tong and Wang, Xiaogang},
	year = {2014},
	pages = {152--159},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\CLJLBNUK\\Li et al. - 2014 - DeepReID Deep Filter Pairing Neural Network for P.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\3VKVK5ZP\\Li_DeepReID_Deep_Filter_2014_CVPR_paper.html:text/html}
}

@article{yi_deep_2014-1,
	title = {Deep {Metric} {Learning} for {Practical} {Person} {Re}-{Identification}},
	doi = {10.1109/ICPR.2014.16},
	abstract = {Various hand-crafted features and metric learning methods prevail in the field of person re-identification. Compared to these methods, this paper proposes a more general way that can learn a similarity metric from image pixels directly. By using a "siamese" deep neural network, the proposed method can jointly learn the color feature, texture feature and metric in a unified framework. The network has a symmetry structure with two sub-networks which are connected by Cosine function. To deal with the big variations of person images, binomial deviance is used to evaluate the cost between similarities and labels, which is proved to be robust to outliers.
Compared to existing researches, a more practical setting is studied in the experiments that is training and test on different datasets (cross dataset person re-identification). Both in "intra dataset" and "cross dataset" settings, the superiorities of the proposed method are illustrated on VIPeR and PRID.},
	journal = {Proceedings - International Conference on Pattern Recognition},
	author = {Yi, Dong and Lei, Zhen and Li, Stan},
	month = jul,
	year = {2014},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\DU8YWZH6\\Yi et al. - 2014 - Deep Metric Learning for Practical Person Re-Ident.pdf:application/pdf}
}

@inproceedings{li_locally_2013,
	title = {Locally {Aligned} {Feature} {Transforms} across {Views}},
	url = {https://openaccess.thecvf.com/content_cvpr_2013/html/Li_Locally_Aligned_Feature_2013_CVPR_paper.html},
	urldate = {2021-04-25},
	author = {Li, Wei and Wang, Xiaogang},
	year = {2013},
	pages = {3594--3601},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\8YSCQ2AF\\Li and Wang - 2013 - Locally Aligned Feature Transforms across Views.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\D2BJEPTR\\Li_Locally_Aligned_Feature_2013_CVPR_paper.html:text/html}
}

@article{zheng_reidentification_2013,
	title = {Reidentification by {Relative} {Distance} {Comparison}},
	volume = {35},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2012.138},
	abstract = {Matching people across nonoverlapping camera views at different locations and different times, known as person reidentification, is both a hard and important problem for associating behavior of people observed in a large distributed space over a prolonged period of time. Person reidentification is fundamentally challenging because of the large visual appearance changes caused by variations in view angle, lighting, background clutter, and occlusion. To address these challenges, most previous approaches aim to model and extract distinctive and reliable visual features. However, seeking an optimal and robust similarity measure that quantifies a wide range of features against realistic viewing conditions from a distance is still an open and unsolved problem for person reidentification. In this paper, we formulate person reidentification as a relative distance comparison (RDC) learning problem in order to learn the optimal similarity measure between a pair of person images. This approach avoids treating all features indiscriminately and does not assume the existence of some universally distinctive and reliable features. To that end, a novel relative distance comparison model is introduced. The model is formulated to maximize the likelihood of a pair of true matches having a relatively smaller distance than that of a wrong match pair in a soft discriminant manner. Moreover, in order to maintain the tractability of the model in large scale learning, we further develop an ensemble RDC model. Extensive experiments on three publicly available benchmarking datasets are carried out to demonstrate the clear superiority of the proposed RDC models over related popular person reidentification techniques. The results also show that the new RDC models are more robust against visual appearance changes and less susceptible to model overfitting compared to other related existing models.},
	number = {3},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zheng, Wei-Shi and Gong, Shaogang and Xiang, Tao},
	month = mar,
	year = {2013},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Cameras, Data models, Feature extraction, Image color analysis, Computer aided instruction, Optimization, feature quantification, feature selection, Person reidentification, relative distance comparison, Vectors},
	pages = {653--668},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\P6FZ9B8Y\\6226421.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\JSZ9BT9D\\Zheng et al. - 2013 - Reidentification by Relative Distance Comparison.pdf:application/pdf}
}

@inproceedings{li_human_2013,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Human {Reidentification} with {Transferred} {Metric} {Learning}},
	isbn = {978-3-642-37331-2},
	doi = {10.1007/978-3-642-37331-2_3},
	abstract = {Human reidentification is to match persons observed in non-overlapping camera views with visual features for inter-camera tracking. The ambiguity increases with the number of candidates to be distinguished. Simple temporal reasoning can simplify the problem by pruning the candidate set to be matched. Existing approaches adopt a fixed metric for matching all the subjects. Our approach is motivated by the insight that different visual metrics should be optimally learned for different candidate sets. We tackle this problem under a transfer learning framework. Given a large training set, the training samples are selected and reweighted according to their visual similarities with the query sample and its candidate set. A weighted maximum margin metric is online learned and transferred from a generic metric to a candidate-set-specific metric. The whole online reweighting and learning process takes less than two seconds per candidate set. Experiments on the VIPeR dataset and our dataset show that the proposed transferred metric learning significantly outperforms directly matching visual features or using a single generic metric learned from the whole training set.},
	language = {en},
	booktitle = {Computer {Vision} – {ACCV} 2012},
	publisher = {Springer},
	author = {Li, Wei and Zhao, Rui and Wang, Xiaogang},
	editor = {Lee, Kyoung Mu and Matsushita, Yasuyuki and Rehg, James M. and Hu, Zhanyi},
	year = {2013},
	keywords = {Camera View, Temporal Reasoning, Training Sample, Transfer Learning, Visual Feature},
	pages = {31--44},
	file = {Springer Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\UIAQ9I79\\Li et al. - 2013 - Human Reidentification with Transferred Metric Lea.pdf:application/pdf}
}



@inproceedings{dalal_histograms_2005,
	title = {Histograms of oriented gradients for human detection},
	volume = {1},
	doi = {10.1109/CVPR.2005.177},
	abstract = {We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	author = {Dalal, N. and Triggs, B.},
	month = jun,
	year = {2005},
	note = {ISSN: 1063-6919},
	keywords = {Robustness, Histograms, Humans, Testing, High performance computing, Image databases, Image edge detection, Object detection, Object recognition, Support vector machines},
	pages = {886--893 vol. 1},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\RVUHAT9Z\\1467360.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\6ZTXE2SM\\Dalal and Triggs - 2005 - Histograms of oriented gradients for human detecti.pdf:application/pdf}
}

@article{ojala_multiresolution_2002,
	title = {Multiresolution gray-scale and rotation invariant texture classification with local binary patterns},
	volume = {24},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2002.1017623},
	abstract = {Presents a theoretically very simple, yet efficient, multiresolution approach to gray-scale and rotation invariant texture classification based on local binary patterns and nonparametric discrimination of sample and prototype distributions. The method is based on recognizing that certain local binary patterns, termed "uniform," are fundamental properties of local image texture and their occurrence histogram is proven to be a very powerful texture feature. We derive a generalized gray-scale and rotation invariant operator presentation that allows for detecting the "uniform" patterns for any quantization of the angular space and for any spatial resolution and presents a method for combining multiple operators for multiresolution analysis. The proposed approach is very robust in terms of gray-scale variations since the operator is, by definition, invariant against any monotonic transformation of the gray scale. Another advantage is computational simplicity as the operator can be realized with a few operations in a small neighborhood and a lookup table. Experimental results demonstrate that good discrimination can be achieved with the occurrence statistics of simple rotation invariant local binary patterns.},
	number = {7},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ojala, T. and Pietikainen, M. and Maenpaa, T.},
	month = jul,
	year = {2002},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Robustness, Histograms, Spatial resolution, Gray-scale, Image recognition, Image texture, Multiresolution analysis, Pattern recognition, Prototypes, Quantization},
	pages = {971--987},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\JCG5BWCA\\1017623.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\3I6A2VKM\\Ojala et al. - 2002 - Multiresolution gray-scale and rotation invariant .pdf:application/pdf}
}

@inproceedings{jiang_cross-domain_2008,
	title = {Cross-domain learning methods for high-level visual concept classification},
	doi = {10.1109/ICIP.2008.4711716},
	abstract = {Exploding amounts of multimedia data increasingly require automatic indexing and classification, e.g. training classifiers to produce high-level features, or semantic concepts, chosen to represent image content, like car, person, etc. When changing the applied domain (i.e. from news domain to consumer home videos), the classifiers trained in one domain often perform poorly in the other domain due to changes in feature distributions. Additionally, classifiers trained on the new domain alone may suffer from too few positive training samples. Appropriately adapting data/models from an old domain to help classify data in a new domain is an important issue. In this work, we develop a new cross-domain SVM (CDSVM) algorithm for adapting previously learned support vectors from one domain to help classification in another domain. Better precision is obtained with almost no additional computational cost. Also, we give a comprehensive summary and comparative study of the state- of-the-art SVM-based cross-domain learning methods. Evaluation over the latest large-scale TRECVID benchmark data set shows that our CDSVM method can improve mean average precision over 36 concepts by 7.5\%. For further performance gain, we also propose an intuitive selection criterion to determine which cross-domain learning method to use for each concept.},
	booktitle = {2008 15th {IEEE} {International} {Conference} on {Image} {Processing}},
	author = {Jiang, Wei and Zavesky, Eric and Chang, Shih-Fu and Loui, Alex},
	month = oct,
	year = {2008},
	note = {ISSN: 2381-8549},
	keywords = {Machine learning, Learning systems, Computational efficiency, Support vector machines, adaptive systems, feature extraction, image processing, Large-scale systems, learning systems, Machine assisted indexing, Multimedia systems, Performance gain, Support vector machine classification, Videos},
	pages = {161--164},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\7FHC8GVW\\4711716.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\BMQAKBES\\Jiang et al. - 2008 - Cross-domain learning methods for high-level visua.pdf:application/pdf}
}

@inproceedings{gray_viewpoint_2008,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Viewpoint {Invariant} {Pedestrian} {Recognition} with an {Ensemble} of {Localized} {Features}},
	isbn = {978-3-540-88682-2},
	doi = {10.1007/978-3-540-88682-2_21},
	abstract = {Viewpoint invariant pedestrian recognition is an important yet under-addressed problem in computer vision. This is likely due to the difficulty in matching two objects with unknown viewpoint and pose. This paper presents a method of performing viewpoint invariant pedestrian recognition using an efficiently and intelligently designed object representation, the ensemble of localized features (ELF). Instead of designing a specific feature by hand to solve the problem, we define a feature space using our intuition about the problem and let a machine learning algorithm find the best representation. We show how both an object class specific representation and a discriminative recognition model can be learned using the AdaBoost algorithm. This approach allows many different kinds of simple features to be combined into a single similarity function. The method is evaluated using a viewpoint invariant pedestrian recognition dataset and the results are shown to be superior to all previous benchmarks for both recognition and reacquisition of pedestrians.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2008},
	publisher = {Springer},
	author = {Gray, Douglas and Tao, Hai},
	editor = {Forsyth, David and Torr, Philip and Zisserman, Andrew},
	year = {2008},
	keywords = {Appearance Model, Computer Vision, Correct Match, Feature Channel, IEEE Computer Society},
	pages = {262--275},
	file = {Springer Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\FCWGXHE2\\Gray and Tao - 2008 - Viewpoint Invariant Pedestrian Recognition with an.pdf:application/pdf}
}

@inproceedings{cheng_custom_2011,
	address = {Dundee},
	title = {Custom {Pictorial} {Structures} for {Re}-identification},
	isbn = {978-1-901725-43-8},
	url = {http://www.bmva.org/bmvc/2011/proceedings/paper68/index.html},
	doi = {10.5244/C.25.68},
	abstract = {We propose a novel methodology for re-identiﬁcation, based on Pictorial Structures (PS). Whenever face or other biometric information is missing, humans recognize an individual by selectively focusing on the body parts, looking for part-to-part correspondences. We want to take inspiration from this strategy in a re-identiﬁcation context, using PS to achieve this objective. For single image re-identiﬁcation, we adopt PS to localize the parts, extract and match their descriptors. When multiple images of a single individual are available, we propose a new algorithm to customize the ﬁt of PS on that speciﬁc person, leading to what we call a Custom Pictorial Structure (CPS). CPS learns the appearance of an individual, improving the localization of its parts, thus obtaining more reliable visual characteristics for re-identiﬁcation. It is based on the statistical learning of pixel attributes collected through spatio-temporal reasoning. The use of PS and CPS leads to state-of-the-art results on all the available public benchmarks, and opens a fresh new direction for research on re-identiﬁcation.},
	language = {en},
	urldate = {2021-04-25},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2011},
	publisher = {British Machine Vision Association},
	author = {Cheng, Dong Seon and Cristani, Marco and Stoppa, Michele and Bazzani, Loris and Murino, Vittorio},
	year = {2011},
	pages = {68.1--68.11},
	file = {Cheng et al. - 2011 - Custom Pictorial Structures for Re-identification.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\ACPHQHZU\\Cheng et al. - 2011 - Custom Pictorial Structures for Re-identification.pdf:application/pdf}
}

@inproceedings{schmid_constructing_2001,
	title = {Constructing models for content-based image retrieval},
	volume = {2},
	doi = {10.1109/CVPR.2001.990922},
	abstract = {This paper presents a new method for constructing models from a set of positive and negative sample images; the method requires no manual extraction of significant objects or features. Our model representation is based on two layers. The first one consists of "generic" descriptors which represent sets of similar rotational invariant feature vectors. Rotation invariance allows to group similar, but rotated patterns and makes the method robust to model deformations. The second layer is the joint probability on the frequencies of the "generic" descriptors over neighborhoods. This probability is multi-modal and is represented by a set of "spatial-frequency" clusters. It adds a statistical spatial constraint which is rotationally invariant. Our two-layer representation is novel; it allows to efficiently capture "texture-like" visual structure. The selection of distinctive structure determines characteristic model features (common to the positive and rare in the negative examples) and increases the performance of the model. Models are retrieved and localized using a probabilistic score. Experimental results for "textured" animals and faces show a very good performance for retrieval as well as localization.},
	booktitle = {Proceedings of the 2001 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}. {CVPR} 2001},
	author = {Schmid, C.},
	month = dec,
	year = {2001},
	note = {ISSN: 1063-6919},
	keywords = {Image retrieval, Robustness, Solid modeling, Animal structures, Content based retrieval, Deformable models, Frequency, Probability},
	pages = {II--II},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\B4MZ86AE\\990922.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\EDKNLN8R\\Schmid - 2001 - Constructing models for content-based image retrie.pdf:application/pdf}
}

@article{fogel_gabor_1989,
	title = {Gabor filters as texture discriminator},
	volume = {61},
	issn = {1432-0770},
	url = {https://doi.org/10.1007/BF00204594},
	doi = {10.1007/BF00204594},
	abstract = {The present paper presents a model for texture discrimination based on Gabor functions. In this model the Gabor power spectrum of the micropatterns corresponding to different textures is calculated. A function that measures the difference between the spectrum of two micropatterns is introduced and its values are correlated with human performance in preattentive detection tasks. In addition, a two stage algorithm for texture segregation is presented. In the first stage the input image is transformed via Gabor filters into a representation image that allows discrimination between features by means of intensity differences. In the second stage the borders between areas of different textures are found using a Laplacian of Gaussian operator. This algorithm is sensitive to energy differences, rotation and spatial frequency and is insensitive to local translation. The model was tested by means of several simulations and was found to be in good correlation with known psychophysical characteristics as texton based texture segregation and micropattern density sensitivity. However, this simple model fails to predict human performance in discrimination tasks based on differences in the density of “terminators”. In this case human performance is better than expected.},
	language = {en},
	number = {2},
	urldate = {2021-04-25},
	journal = {Biological Cybernetics},
	author = {Fogel, I. and Sagi, D.},
	month = jun,
	year = {1989},
	pages = {103--113},
	file = {Springer Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\64SE7FBN\\Fogel and Sagi - 1989 - Gabor filters as texture discriminator.pdf:application/pdf}
}

@inproceedings{viola_robust_2001,
	title = {Robust {Real}-time {Object} {Detection}},
	abstract = {This paper describes a visual object detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the “Integral Image ” which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features and yields extremely efficient classifiers [6]. The third contribution is a method for combining classifiers in a “cascade ” which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. A set of experiments in the domain of face detection are presented. The system yields face detection performace comparable to the best previous systems [18, 13, 16, 12, 1]. Implemented on a conventional desktop, face detection proceeds at 15 frames per second. 1.},
	booktitle = {International {Journal} of {Computer} {Vision}},
	author = {Viola, Paul and Jones, Michael},
	year = {2001},
	file = {Citeseer - Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\NXJISLLP\\Viola and Jones - 2001 - Robust Real-time Object Detection.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\7U3GMVUI\\summary.html:text/html}
}

@article{freund_decision-theoretic_1997,
	title = {A {Decision}-{Theoretic} {Generalization} of {On}-{Line} {Learning} and an {Application} to {Boosting}},
	volume = {55},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S002200009791504X},
	doi = {10.1006/jcss.1997.1504},
	abstract = {In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone–Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.},
	language = {en},
	number = {1},
	urldate = {2021-04-26},
	journal = {Journal of Computer and System Sciences},
	author = {Freund, Yoav and Schapire, Robert E},
	month = aug,
	year = {1997},
	pages = {119--139},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\TFKNF3VB\\Freund and Schapire - 1997 - A Decision-Theoretic Generalization of On-Line Lea.pdf:application/pdf}
}

@inproceedings{li_learning_2017,
	title = {Learning {Deep} {Context}-{Aware} {Features} {Over} {Body} and {Latent} {Parts} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Learning_Deep_Context-Aware_CVPR_2017_paper.html},
	urldate = {2021-04-26},
	author = {Li, Dangwei and Chen, Xiaotang and Zhang, Zhang and Huang, Kaiqi},
	year = {2017},
	keywords = {Attention deep learning models},
	pages = {384--393},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\L92CCZZ7\\Li et al. - 2017 - Learning Deep Context-Aware Features Over Body and.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\H4U8J83C\\Li_Learning_Deep_Context-Aware_CVPR_2017_paper.html:text/html}
}

@inproceedings{chen_beyond_2017,
	title = {Beyond {Triplet} {Loss}: {A} {Deep} {Quadruplet} {Network} for {Person} {Re}-{Identification}},
	shorttitle = {Beyond {Triplet} {Loss}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Chen_Beyond_Triplet_Loss_CVPR_2017_paper.html},
	urldate = {2021-04-26},
	author = {Chen, Weihua and Chen, Xiaotang and Zhang, Jianguo and Huang, Kaiqi},
	year = {2017},
	pages = {403--412},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\ATX5QYLA\\Chen et al. - 2017 - Beyond Triplet Loss A Deep Quadruplet Network for.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\EJUFDM2J\\Chen_Beyond_Triplet_Loss_CVPR_2017_paper.html:text/html}
}

@inproceedings{zhao_spindle_2017,
	title = {Spindle {Net}: {Person} {Re}-{Identification} {With} {Human} {Body} {Region} {Guided} {Feature} {Decomposition} and {Fusion}},
	shorttitle = {Spindle {Net}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Zhao_Spindle_Net_Person_CVPR_2017_paper.html},
	urldate = {2021-04-26},
	author = {Zhao, Haiyu and Tian, Maoqing and Sun, Shuyang and Shao, Jing and Yan, Junjie and Yi, Shuai and Wang, Xiaogang and Tang, Xiaoou},
	year = {2017},
	pages = {1077--1085},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\9BQSCFRE\\Zhao et al. - 2017 - Spindle Net Person Re-Identification With Human B.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\DUVBTHN7\\Zhao_Spindle_Net_Person_CVPR_2017_paper.html:text/html}
}

@inproceedings{zhong_re-ranking_2017,
	title = {Re-{Ranking} {Person} {Re}-{Identification} {With} k-{Reciprocal} {Encoding}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Zhong_Re-Ranking_Person_Re-Identification_CVPR_2017_paper.html},
	urldate = {2021-04-26},
	author = {Zhong, Zhun and Zheng, Liang and Cao, Donglin and Li, Shaozi},
	year = {2017},
	pages = {1318--1327},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\7IIRNYD4\\Zhong et al. - 2017 - Re-Ranking Person Re-Identification With k-Recipro.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\76KIKYDH\\Zhong_Re-Ranking_Person_Re-Identification_CVPR_2017_paper.html:text/html}
}

@inproceedings{li_sequential_2017,
	title = {Sequential {Person} {Recognition} in {Photo} {Albums} {With} a {Recurrent} {Network}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Sequential_Person_Recognition_CVPR_2017_paper.html},
	urldate = {2021-04-26},
	author = {Li, Yao and Lin, Guosheng and Zhuang, Bohan and Liu, Lingqiao and Shen, Chunhua and van den Hengel, Anton},
	year = {2017},
	pages = {1338--1346},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\FIHT44ZB\\Li et al. - 2017 - Sequential Person Recognition in Photo Albums With.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\6UKTE8CM\\Li_Sequential_Person_Recognition_CVPR_2017_paper.html:text/html}
}

@inproceedings{bai_scalable_2017,
	title = {Scalable {Person} {Re}-{Identification} on {Supervised} {Smoothed} {Manifold}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Bai_Scalable_Person_Re-Identification_CVPR_2017_paper.html},
	urldate = {2021-04-26},
	author = {Bai, Song and Bai, Xiang and Tian, Qi},
	year = {2017},
	pages = {2530--2539},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\V7FM33ML\\Bai et al. - 2017 - Scalable Person Re-Identification on Supervised Sm.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\FTHM9RB8\\Bai_Scalable_Person_Re-Identification_CVPR_2017_paper.html:text/html}
}

@inproceedings{bak_one-shot_2017,
	title = {One-{Shot} {Metric} {Learning} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Bak_One-Shot_Metric_Learning_CVPR_2017_paper.html},
	urldate = {2021-04-26},
	author = {Bak, Slawomir and Carr, Peter},
	year = {2017},
	pages = {2990--2999},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\ZZXVHJGT\\Bak and Carr - 2017 - One-Shot Metric Learning for Person Re-Identificat.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\2D3T2372\\Bak_One-Shot_Metric_Learning_CVPR_2017_paper.html:text/html}
}

@inproceedings{tang_multiple_2017,
	title = {Multiple {People} {Tracking} by {Lifted} {Multicut} and {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Tang_Multiple_People_Tracking_CVPR_2017_paper.html},
	urldate = {2021-04-26},
	author = {Tang, Siyu and Andriluka, Mykhaylo and Andres, Bjoern and Schiele, Bernt},
	year = {2017},
	pages = {3539--3548},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\PDN5AJEB\\Tang et al. - 2017 - Multiple People Tracking by Lifted Multicut and Pe.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\YCGYXD4N\\Tang_Multiple_People_Tracking_CVPR_2017_paper.html:text/html}
}

@inproceedings{zhou_point_2017,
	title = {Point to {Set} {Similarity} {Based} {Deep} {Feature} {Learning} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_Point_to_Set_CVPR_2017_paper.html},
	urldate = {2021-04-26},
	author = {Zhou, Sanping and Wang, Jinjun and Wang, Jiayun and Gong, Yihong and Zheng, Nanning},
	year = {2017},
	pages = {3741--3750},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\WPFVYPUV\\Zhou et al. - 2017 - Point to Set Similarity Based Deep Feature Learnin.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\Z7BMI839\\Zhou_Point_to_Set_CVPR_2017_paper.html:text/html}
}

@inproceedings{chen_fast_2017,
	title = {Fast {Person} {Re}-{Identification} via {Cross}-{Camera} {Semantic} {Binary} {Transformation}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Chen_Fast_Person_Re-Identification_CVPR_2017_paper.html},
	urldate = {2021-04-26},
	author = {Chen, Jiaxin and Wang, Yunhong and Qin, Jie and Liu, Li and Shao, Ling},
	year = {2017},
	pages = {3873--3882},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\W7XB5Q2Z\\Chen et al. - 2017 - Fast Person Re-Identification via Cross-Camera Sem.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\RFNNG596\\Chen_Fast_Person_Re-Identification_CVPR_2017_paper.html:text/html}
}

@inproceedings{zhou_see_2017,
	title = {See the {Forest} for the {Trees}: {Joint} {Spatial} and {Temporal} {Recurrent} {Neural} {Networks} for {Video}-{Based} {Person} {Re}-{Identification}},
	shorttitle = {See the {Forest} for the {Trees}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_See_the_Forest_CVPR_2017_paper.html},
	urldate = {2021-04-26},
	author = {Zhou, Zhen and Huang, Yan and Wang, Wei and Wang, Liang and Tan, Tieniu},
	year = {2017},
	pages = {4747--4756},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\SSBLXPHT\\Zhou et al. - 2017 - See the Forest for the Trees Joint Spatial and Te.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\MAAQD3SH\\Zhou_See_the_Forest_CVPR_2017_paper.html:text/html}
}

@inproceedings{lin_consistent-aware_2017,
	title = {Consistent-{Aware} {Deep} {Learning} for {Person} {Re}-{Identification} in a {Camera} {Network}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Lin_Consistent-Aware_Deep_Learning_CVPR_2017_paper.html},
	urldate = {2021-04-26},
	author = {Lin, Ji and Ren, Liangliang and Lu, Jiwen and Feng, Jianjiang and Zhou, Jie},
	year = {2017},
	pages = {5771--5780},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\IVNRJXMU\\Lin et al. - 2017 - Consistent-Aware Deep Learning for Person Re-Ident.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\IRAKP9E2\\Lin_Consistent-Aware_Deep_Learning_CVPR_2017_paper.html:text/html}
}

@inproceedings{kumar_pose-aware_2017,
	title = {Pose-{Aware} {Person} {Recognition}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Kumar_Pose-Aware_Person_Recognition_CVPR_2017_paper.html},
	urldate = {2021-04-26},
	author = {Kumar, Vijay and Namboodiri, Anoop and Paluri, Manohar and Jawahar, C. V.},
	year = {2017},
	pages = {6223--6232},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\9TT3PWQM\\Kumar et al. - 2017 - Pose-Aware Person Recognition.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\VYJSRI9C\\Kumar_Pose-Aware_Person_Recognition_CVPR_2017_paper.html:text/html}
}

@inproceedings{zhang_learning_2016,
	title = {Learning a {Discriminative} {Null} {Space} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Learning_a_Discriminative_CVPR_2016_paper.html},
	urldate = {2021-04-26},
	author = {Zhang, Li and Xiang, Tao and Gong, Shaogang},
	year = {2016},
	pages = {1239--1248},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\HWRRAZID\\Zhang et al. - 2016 - Learning a Discriminative Null Space for Person Re.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\5T6JGXYG\\Zhang_Learning_a_Discriminative_CVPR_2016_paper.html:text/html}
}

@inproceedings{xiao_learning_2016,
	title = {Learning {Deep} {Feature} {Representations} {With} {Domain} {Guided} {Dropout} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Xiao_Learning_Deep_Feature_CVPR_2016_paper.html},
	urldate = {2021-04-26},
	author = {Xiao, Tong and Li, Hongsheng and Ouyang, Wanli and Wang, Xiaogang},
	year = {2016},
	pages = {1249--1258},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\2YNBY9VC\\Xiao et al. - 2016 - Learning Deep Feature Representations With Domain .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\I5SUDDZ2\\Xiao_Learning_Deep_Feature_CVPR_2016_paper.html:text/html}
}

@inproceedings{chen_similarity_2016,
	title = {Similarity {Learning} {With} {Spatial} {Constraints} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Chen_Similarity_Learning_With_CVPR_2016_paper.html},
	urldate = {2021-04-26},
	author = {Chen, Dapeng and Yuan, Zejian and Chen, Badong and Zheng, Nanning},
	year = {2016},
	pages = {1268--1277},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\8HLFUKR9\\Chen et al. - 2016 - Similarity Learning With Spatial Constraints for P.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\2YAYR4RR\\Chen_Similarity_Learning_With_CVPR_2016_paper.html:text/html}
}

@inproceedings{zhang_sample-specific_2016,
	title = {Sample-{Specific} {SVM} {Learning} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Sample-Specific_SVM_Learning_CVPR_2016_paper.html},
	urldate = {2021-04-26},
	author = {Zhang, Ying and Li, Baohua and Lu, Huchuan and Irie, Atshushi and Ruan, Xiang},
	year = {2016},
	pages = {1278--1287},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\7ML7G326\\Zhang et al. - 2016 - Sample-Specific SVM Learning for Person Re-Identif.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\KGJFBEKD\\Zhang_Sample-Specific_SVM_Learning_CVPR_2016_paper.html:text/html}
}

@inproceedings{li_multi-level_2016,
	title = {A {Multi}-{Level} {Contextual} {Model} {For} {Person} {Recognition} in {Photo} {Albums}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Li_A_Multi-Level_Contextual_CVPR_2016_paper.html},
	urldate = {2021-04-26},
	author = {Li, Haoxiang and Brandt, Jonathan and Lin, Zhe and Shen, Xiaohui and Hua, Gang},
	year = {2016},
	pages = {1297--1305},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\4BWCBADM\\Li et al. - 2016 - A Multi-Level Contextual Model For Person Recognit.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\P35X8B3K\\Li_A_Multi-Level_Contextual_CVPR_2016_paper.html:text/html}
}

@inproceedings{peng_unsupervised_2016,
	title = {Unsupervised {Cross}-{Dataset} {Transfer} {Learning} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Peng_Unsupervised_Cross-Dataset_Transfer_CVPR_2016_paper.html},
	urldate = {2021-04-26},
	author = {Peng, Peixi and Xiang, Tao and Wang, Yaowei and Pontil, Massimiliano and Gong, Shaogang and Huang, Tiejun and Tian, Yonghong},
	year = {2016},
	pages = {1306--1315},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\44TM43FY\\Peng et al. - 2016 - Unsupervised Cross-Dataset Transfer Learning for P.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\EXL5JWGG\\Peng_Unsupervised_Cross-Dataset_Transfer_CVPR_2016_paper.html:text/html}
}

@inproceedings{mclaughlin_recurrent_2016,
	title = {Recurrent {Convolutional} {Network} for {Video}-{Based} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/McLaughlin_Recurrent_Convolutional_Network_CVPR_2016_paper.html},
	urldate = {2021-04-26},
	author = {McLaughlin, Niall and del Rincon, Jesus Martinez and Miller, Paul},
	year = {2016},
	pages = {1325--1334},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\QD98G6BB\\McLaughlin et al. - 2016 - Recurrent Convolutional Network for Video-Based Pe.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\P3JC5ZDF\\McLaughlin_Recurrent_Convolutional_Network_CVPR_2016_paper.html:text/html}
}

@inproceedings{cheng_person_2016,
	title = {Person {Re}-{Identification} by {Multi}-{Channel} {Parts}-{Based} {CNN} {With} {Improved} {Triplet} {Loss} {Function}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Cheng_Person_Re-Identification_by_CVPR_2016_paper.html},
	urldate = {2021-04-26},
	author = {Cheng, De and Gong, Yihong and Zhou, Sanping and Wang, Jinjun and Zheng, Nanning},
	year = {2016},
	pages = {1335--1344},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\6LQ4Y99R\\Cheng et al. - 2016 - Person Re-Identification by Multi-Channel Parts-Ba.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\4AF89AI5\\Cheng_Person_Re-Identification_by_CVPR_2016_paper.html:text/html}
}

@inproceedings{you_top-push_2016,
	title = {Top-{Push} {Video}-{Based} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/You_Top-Push_Video-Based_Person_CVPR_2016_paper.html},
	urldate = {2021-04-26},
	author = {You, Jinjie and Wu, Ancong and Li, Xiang and Zheng, Wei-Shi},
	year = {2016},
	pages = {1345--1353},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\6WDQ366V\\You et al. - 2016 - Top-Push Video-Based Person Re-Identification.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\EC9XXTP4\\You_Top-Push_Video-Based_Person_CVPR_2016_paper.html:text/html}
}

@inproceedings{cho_improving_2016,
	title = {Improving {Person} {Re}-{Identification} via {Pose}-{Aware} {Multi}-{Shot} {Matching}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Cho_Improving_Person_Re-Identification_CVPR_2016_paper.html},
	urldate = {2021-04-26},
	author = {Cho, Yeong-Jun and Yoon, Kuk-Jin},
	year = {2016},
	pages = {1354--1362},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\DYFYWWMD\\Cho and Yoon - 2016 - Improving Person Re-Identification via Pose-Aware .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\7482PMRN\\Cho_Improving_Person_Re-Identification_CVPR_2016_paper.html:text/html}
}

@inproceedings{matsukawa_hierarchical_2016,
	title = {Hierarchical {Gaussian} {Descriptor} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Matsukawa_Hierarchical_Gaussian_Descriptor_CVPR_2016_paper.html},
	urldate = {2021-04-26},
	author = {Matsukawa, Tetsu and Okabe, Takahiro and Suzuki, Einoshin and Sato, Yoichi},
	year = {2016},
	pages = {1363--1372},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\3B7TJSN4\\Matsukawa et al. - 2016 - Hierarchical Gaussian Descriptor for Person Re-Ide.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\S9RJQUPH\\Matsukawa_Hierarchical_Gaussian_Descriptor_CVPR_2016_paper.html:text/html}
}

@inproceedings{jing_super-resolution_2015,
	title = {Super-{Resolution} {Person} {Re}-{Identification} {With} {Semi}-{Coupled} {Low}-{Rank} {Discriminant} {Dictionary} {Learning}},
	url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Jing_Super-Resolution_Person_Re-Identification_2015_CVPR_paper.html},
	urldate = {2021-04-26},
	author = {Jing, Xiao-Yuan and Zhu, Xiaoke and Wu, Fei and You, Xinge and Liu, Qinglong and Yue, Dong and Hu, Ruimin and Xu, Baowen},
	year = {2015},
	pages = {695--704},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\JQ69MBDJ\\Jing et al. - 2015 - Super-Resolution Person Re-Identification With Sem.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\3U3DUYS9\\Jing_Super-Resolution_Person_Re-Identification_2015_CVPR_paper.html:text/html}
}

@inproceedings{chen_similarity_2015,
	title = {Similarity {Learning} on an {Explicit} {Polynomial} {Kernel} {Feature} {Map} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Chen_Similarity_Learning_on_2015_CVPR_paper.html},
	urldate = {2021-04-26},
	author = {Chen, Dapeng and Yuan, Zejian and Hua, Gang and Zheng, Nanning and Wang, Jingdong},
	year = {2015},
	pages = {1565--1573},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\FCLAVW4E\\Chen et al. - 2015 - Similarity Learning on an Explicit Polynomial Kern.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\5XPC5B69\\Chen_Similarity_Learning_on_2015_CVPR_paper.html:text/html}
}

@inproceedings{zheng_query-adaptive_2015,
	title = {Query-{Adaptive} {Late} {Fusion} for {Image} {Search} and {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Zheng_Query-Adaptive_Late_Fusion_2015_CVPR_paper.html},
	urldate = {2021-04-26},
	author = {Zheng, Liang and Wang, Shengjin and Tian, Lu and He, Fei and Liu, Ziqiong and Tian, Qi},
	year = {2015},
	pages = {1741--1750},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\YI9MKY7C\\Zheng et al. - 2015 - Query-Adaptive Late Fusion for Image Search and Pe.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\VLEU76F3\\Zheng_Query-Adaptive_Late_Fusion_2015_CVPR_paper.html:text/html}
}

@inproceedings{paisitkriangkrai_learning_2015,
	title = {Learning to {Rank} in {Person} {Re}-{Identification} {With} {Metric} {Ensembles}},
	url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Paisitkriangkrai_Learning_to_Rank_2015_CVPR_paper.html},
	urldate = {2021-04-26},
	author = {Paisitkriangkrai, Sakrapee and Shen, Chunhua and van den Hengel, Anton},
	year = {2015},
	pages = {1846--1855},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\2Q7ESV9A\\Paisitkriangkrai et al. - 2015 - Learning to Rank in Person Re-Identification With .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\7HP8PX8K\\Paisitkriangkrai_Learning_to_Rank_2015_CVPR_paper.html:text/html}
}

@inproceedings{liao_person_2015,
	title = {Person {Re}-{Identification} by {Local} {Maximal} {Occurrence} {Representation} and {Metric} {Learning}},
	url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Liao_Person_Re-Identification_by_2015_CVPR_paper.html},
	urldate = {2021-04-26},
	author = {Liao, Shengcai and Hu, Yang and Zhu, Xiangyu and Li, Stan Z.},
	year = {2015},
	pages = {2197--2206},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\52R4389K\\Liao et al. - 2015 - Person Re-Identification by Local Maximal Occurren.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\AXI6EQKM\\Liao_Person_Re-Identification_by_2015_CVPR_paper.html:text/html}
}

@inproceedings{shi_transferring_2015,
	title = {Transferring a {Semantic} {Representation} for {Person} {Re}-{Identification} and {Search}},
	url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Shi_Transferring_a_Semantic_2015_CVPR_paper.html},
	urldate = {2021-04-26},
	author = {Shi, Zhiyuan and Hospedales, Timothy M. and Xiang, Tao},
	year = {2015},
	pages = {4184--4193},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\75MBALR7\\Shi et al. - 2015 - Transferring a Semantic Representation for Person .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\MNTWHVDR\\Shi_Transferring_a_Semantic_2015_CVPR_paper.html:text/html}
}

@inproceedings{zhang_beyond_2015,
	title = {Beyond {Frontal} {Faces}: {Improving} {Person} {Recognition} {Using} {Multiple} {Cues}},
	shorttitle = {Beyond {Frontal} {Faces}},
	url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Zhang_Beyond_Frontal_Faces_2015_CVPR_paper.html},
	urldate = {2021-04-26},
	author = {Zhang, Ning and Paluri, Manohar and Taigman, Yaniv and Fergus, Rob and Bourdev, Lubomir},
	year = {2015},
	pages = {4804--4813},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\96VV6DPU\\Zhang et al. - 2015 - Beyond Frontal Faces Improving Person Recognition.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\LPEHVLH6\\Zhang_Beyond_Frontal_Faces_2015_CVPR_paper.html:text/html}
}

@inproceedings{yu_cross-view_2017,
	title = {Cross-{View} {Asymmetric} {Metric} {Learning} for {Unsupervised} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Yu_Cross-View_Asymmetric_Metric_ICCV_2017_paper.html},
	urldate = {2021-04-26},
	author = {Yu, Hong-Xing and Wu, Ancong and Zheng, Wei-Shi},
	year = {2017},
	pages = {994--1002},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\7RH72MAT\\Yu et al. - 2017 - Cross-View Asymmetric Metric Learning for Unsuperv.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\F8XBJ6E7\\Yu_Cross-View_Asymmetric_Metric_ICCV_2017_paper.html:text/html}
}

@inproceedings{barman_shape_2017,
	title = {{SHaPE}: {A} {Novel} {Graph} {Theoretic} {Algorithm} for {Making} {Consensus}-{Based} {Decisions} in {Person} {Re}-{Identification} {Systems}},
	shorttitle = {{SHaPE}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Barman_SHaPE_A_Novel_ICCV_2017_paper.html},
	urldate = {2021-04-26},
	author = {Barman, Arko and Shah, Shishir K.},
	year = {2017},
	pages = {1115--1124},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\A3J4MTF8\\Barman and Shah - 2017 - SHaPE A Novel Graph Theoretic Algorithm for Makin.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\VECZ2KIG\\Barman_SHaPE_A_Novel_ICCV_2017_paper.html:text/html}
}

@inproceedings{yamaguchi_spatio-temporal_2017,
	title = {Spatio-{Temporal} {Person} {Retrieval} via {Natural} {Language} {Queries}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Yamaguchi_Spatio-Temporal_Person_Retrieval_ICCV_2017_paper.html},
	urldate = {2021-04-26},
	author = {Yamaguchi, Masataka and Saito, Kuniaki and Ushiku, Yoshitaka and Harada, Tatsuya},
	year = {2017},
	pages = {1453--1462},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\IFTD9XJZ\\Yamaguchi et al. - 2017 - Spatio-Temporal Person Retrieval via Natural Langu.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\HQQXVMV2\\Yamaguchi_Spatio-Temporal_Person_Retrieval_ICCV_2017_paper.html:text/html}
}

@inproceedings{chung_two_2017,
	title = {A {Two} {Stream} {Siamese} {Convolutional} {Neural} {Network} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Chung_A_Two_Stream_ICCV_2017_paper.html},
	urldate = {2021-04-26},
	author = {Chung, Dahjung and Tahboub, Khalid and Delp, Edward J.},
	year = {2017},
	pages = {1983--1991},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\YCC9WDBR\\Chung et al. - 2017 - A Two Stream Siamese Convolutional Neural Network .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\477JHAGS\\Chung_A_Two_Stream_ICCV_2017_paper.html:text/html}
}

@inproceedings{zhou_efficient_2017,
	title = {Efficient {Online} {Local} {Metric} {Adaptation} via {Negative} {Samples} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Zhou_Efficient_Online_Local_ICCV_2017_paper.html},
	urldate = {2021-04-26},
	author = {Zhou, Jiahuan and Yu, Pei and Tang, Wei and Wu, Ying},
	year = {2017},
	pages = {2420--2428},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\5H7K3PGW\\Zhou et al. - 2017 - Efficient Online Local Metric Adaptation via Negat.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\6IRY5DUM\\Zhou_Efficient_Online_Local_ICCV_2017_paper.html:text/html}
}

@inproceedings{liu_stepwise_2017,
	title = {Stepwise {Metric} {Promotion} for {Unsupervised} {Video} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Liu_Stepwise_Metric_Promotion_ICCV_2017_paper.html},
	urldate = {2021-04-26},
	author = {Liu, Zimo and Wang, Dong and Lu, Huchuan},
	year = {2017},
	pages = {2429--2438},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\WC89DHUF\\Liu et al. - 2017 - Stepwise Metric Promotion for Unsupervised Video P.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\5XNYC9RE\\Liu_Stepwise_Metric_Promotion_ICCV_2017_paper.html:text/html}
}

@inproceedings{zhao_deeply-learned_2017,
	title = {Deeply-{Learned} {Part}-{Aligned} {Representations} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Zhao_Deeply-Learned_Part-Aligned_Representations_ICCV_2017_paper.html},
	urldate = {2021-04-26},
	author = {Zhao, Liming and Li, Xi and Zhuang, Yueting and Wang, Jingdong},
	year = {2017},
	pages = {3219--3228},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\JR7UQN22\\Zhao et al. - 2017 - Deeply-Learned Part-Aligned Representations for Pe.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\6N2UN84H\\Zhao_Deeply-Learned_Part-Aligned_Representations_ICCV_2017_paper.html:text/html}
}

@inproceedings{zheng_unlabeled_2017,
	title = {Unlabeled {Samples} {Generated} by {GAN} {Improve} the {Person} {Re}-{Identification} {Baseline} in {Vitro}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper.html},
	urldate = {2021-04-26},
	author = {Zheng, Zhedong and Zheng, Liang and Yang, Yi},
	year = {2017},
	pages = {3754--3762},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\PNMJBQLN\\Zheng et al. - 2017 - Unlabeled Samples Generated by GAN Improve the Per.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\VI7YJN33\\Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper.html:text/html}
}

@inproceedings{su_pose-driven_2017,
	title = {Pose-{Driven} {Deep} {Convolutional} {Model} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Su_Pose-Driven_Deep_Convolutional_ICCV_2017_paper.html},
	urldate = {2021-04-26},
	author = {Su, Chi and Li, Jianing and Zhang, Shiliang and Xing, Junliang and Gao, Wen and Tian, Qi},
	year = {2017},
	pages = {3960--3969},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\TSGZL24G\\Su et al. - 2017 - Pose-Driven Deep Convolutional Model for Person Re.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\2Y2I6N75\\Su_Pose-Driven_Deep_Convolutional_ICCV_2017_paper.html:text/html}
}

@inproceedings{xu_jointly_2017,
	title = {Jointly {Attentive} {Spatial}-{Temporal} {Pooling} {Networks} for {Video}-{Based} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Xu_Jointly_Attentive_Spatial-Temporal_ICCV_2017_paper.html},
	urldate = {2021-04-26},
	author = {Xu, Shuangjie and Cheng, Yu and Gu, Kang and Yang, Yang and Chang, Shiyu and Zhou, Pan},
	year = {2017},
	pages = {4733--4742},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\E7J8WVDR\\Xu et al. - 2017 - Jointly Attentive Spatial-Temporal Pooling Network.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\D33YHDAL\\Xu_Jointly_Attentive_Spatial-Temporal_ICCV_2017_paper.html:text/html}
}

@inproceedings{wu_rgb-infrared_2017,
	title = {{RGB}-{Infrared} {Cross}-{Modality} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Wu_RGB-Infrared_Cross-Modality_Person_ICCV_2017_paper.html},
	urldate = {2021-04-26},
	author = {Wu, Ancong and Zheng, Wei-Shi and Yu, Hong-Xing and Gong, Shaogang and Lai, Jianhuang},
	year = {2017},
	pages = {5380--5389},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\32SF49EA\\Wu et al. - 2017 - RGB-Infrared Cross-Modality Person Re-Identificati.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\65ET6S9B\\Wu_RGB-Infrared_Cross-Modality_Person_ICCV_2017_paper.html:text/html}
}

@inproceedings{qian_multi-scale_2017,
	title = {Multi-{Scale} {Deep} {Learning} {Architectures} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Qian_Multi-Scale_Deep_Learning_ICCV_2017_paper.html},
	urldate = {2021-04-26},
	author = {Qian, Xuelin and Fu, Yanwei and Jiang, Yu-Gang and Xiang, Tao and Xue, Xiangyang},
	year = {2017},
	pages = {5399--5408},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\DERZHT9H\\Qian et al. - 2017 - Multi-Scale Deep Learning Architectures for Person.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\DHJX9FWF\\Qian_Multi-Scale_Deep_Learning_ICCV_2017_paper.html:text/html}
}

@inproceedings{garcia_person_2015,
	title = {Person {Re}-{Identification} {Ranking} {Optimisation} by {Discriminant} {Context} {Information} {Analysis}},
	url = {https://openaccess.thecvf.com/content_iccv_2015/html/Garcia_Person_Re-Identification_Ranking_ICCV_2015_paper.html},
	urldate = {2021-04-26},
	author = {Garcia, Jorge and Martinel, Niki and Micheloni, Christian and Gardel, Alfredo},
	year = {2015},
	pages = {1305--1313},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\J7J3APGM\\Garcia et al. - 2015 - Person Re-Identification Ranking Optimisation by D.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\WFM74YL4\\Garcia_Person_Re-Identification_Ranking_ICCV_2015_paper.html:text/html}
}

@inproceedings{shen_person_2015,
	title = {Person {Re}-{Identification} {With} {Correspondence} {Structure} {Learning}},
	url = {https://openaccess.thecvf.com/content_iccv_2015/html/Shen_Person_Re-Identification_With_ICCV_2015_paper.html},
	urldate = {2021-04-26},
	author = {Shen, Yang and Lin, Weiyao and Yan, Junchi and Xu, Mingliang and Wu, Jianxin and Wang, Jingdong},
	year = {2015},
	pages = {3200--3208},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\QJVP8RJ7\\Shen et al. - 2015 - Person Re-Identification With Correspondence Struc.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\SSQSCYY9\\Shen_Person_Re-Identification_With_ICCV_2015_paper.html:text/html}
}

@inproceedings{liao_efficient_2015,
	title = {Efficient {PSD} {Constrained} {Asymmetric} {Metric} {Learning} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_iccv_2015/html/Liao_Efficient_PSD_Constrained_ICCV_2015_paper.html},
	urldate = {2021-04-26},
	author = {Liao, Shengcai and Li, Stan Z.},
	year = {2015},
	pages = {3685--3693},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\9E622V32\\Liao and Li - 2015 - Efficient PSD Constrained Asymmetric Metric Learni.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\YJAL9DBD\\Liao_Efficient_PSD_Constrained_ICCV_2015_paper.html:text/html}
}

@inproceedings{su_multi-task_2015,
	title = {Multi-{Task} {Learning} {With} {Low} {Rank} {Attribute} {Embedding} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_iccv_2015/html/Su_Multi-Task_Learning_With_ICCV_2015_paper.html},
	urldate = {2021-04-26},
	author = {Su, Chi and Yang, Fan and Zhang, Shiliang and Tian, Qi and Davis, Larry S. and Gao, Wen},
	year = {2015},
	pages = {3739--3747},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\VVL6VBC5\\Su et al. - 2015 - Multi-Task Learning With Low Rank Attribute Embedd.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\QMCUURKV\\Su_Multi-Task_Learning_With_ICCV_2015_paper.html:text/html}
}

@inproceedings{li_multi-scale_2015,
	title = {Multi-{Scale} {Learning} for {Low}-{Resolution} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_iccv_2015/html/Li_Multi-Scale_Learning_for_ICCV_2015_paper.html},
	urldate = {2021-04-26},
	author = {Li, Xiang and Zheng, Wei-Shi and Wang, Xiaojuan and Xiang, Tao and Gong, Shaogang},
	year = {2015},
	pages = {3765--3773},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\WGQYLDHN\\Li et al. - 2015 - Multi-Scale Learning for Low-Resolution Person Re-.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\ZRVH9Z7A\\Li_Multi-Scale_Learning_for_ICCV_2015_paper.html:text/html}
}

@inproceedings{oh_person_2015,
	title = {Person {Recognition} in {Personal} {Photo} {Collections}},
	url = {https://openaccess.thecvf.com/content_iccv_2015/html/Oh_Person_Recognition_in_ICCV_2015_paper.html},
	urldate = {2021-04-26},
	author = {Oh, Seong Joon and Benenson, Rodrigo and Fritz, Mario and Schiele, Bernt},
	year = {2015},
	pages = {3862--3870},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\UDPC5GRM\\Oh et al. - 2015 - Person Recognition in Personal Photo Collections.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\EUW5IS6W\\Oh_Person_Recognition_in_ICCV_2015_paper.html:text/html}
}

@inproceedings{karanam_person_2015,
	title = {Person {Re}-{Identification} {With} {Discriminatively} {Trained} {Viewpoint} {Invariant} {Dictionaries}},
	url = {https://openaccess.thecvf.com/content_iccv_2015/html/Karanam_Person_Re-Identification_With_ICCV_2015_paper.html},
	urldate = {2021-04-26},
	author = {Karanam, Srikrishna and Li, Yang and Radke, Richard J.},
	year = {2015},
	pages = {4516--4524},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\D4NZ3DJ5\\Karanam et al. - 2015 - Person Re-Identification With Discriminatively Tra.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\PE2CN87F\\Karanam_Person_Re-Identification_With_ICCV_2015_paper.html:text/html}
}

@inproceedings{zheng_partial_2015,
	title = {Partial {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_iccv_2015/html/Zheng_Partial_Person_Re-Identification_ICCV_2015_paper.html},
	urldate = {2021-04-26},
	author = {Zheng, Wei-Shi and Li, Xiang and Xiang, Tao and Liao, Shengcai and Lai, Jianhuang and Gong, Shaogang},
	year = {2015},
	pages = {4678--4686},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\8SU4I4V7\\Zheng et al. - 2015 - Partial Person Re-Identification.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\57V93C7F\\Zheng_Partial_Person_Re-Identification_ICCV_2015_paper.html:text/html}
}

@inproceedings{ma_domain_2013,
	title = {Domain {Transfer} {Support} {Vector} {Ranking} for {Person} {Re}-identification without {Target} {Camera} {Label} {Information}},
	url = {https://openaccess.thecvf.com/content_iccv_2013/html/Ma_Domain_Transfer_Support_2013_ICCV_paper.html},
	urldate = {2021-04-26},
	author = {Ma, Andy J. and Yuen, Pong C. and Li, Jiawei},
	year = {2013},
	pages = {3567--3574},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\EMB7M2YV\\Ma et al. - 2013 - Domain Transfer Support Vector Ranking for Person .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\XNCN69IW\\Ma_Domain_Transfer_Support_2013_ICCV_paper.html:text/html}
}

@inproceedings{liu_pop_2013,
	title = {{POP}: {Person} {Re}-identification {Post}-rank {Optimisation}},
	shorttitle = {{POP}},
	url = {https://openaccess.thecvf.com/content_iccv_2013/html/Liu_POP_Person_Re-identification_2013_ICCV_paper.html},
	urldate = {2021-04-26},
	author = {Liu, Chunxiao and Loy, Chen Change and Gong, Shaogang and Wang, Guijin},
	year = {2013},
	pages = {441--448},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\R8V9WJQH\\Liu et al. - 2013 - POP Person Re-identification Post-rank Optimisati.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\TJ3P2DJ9\\Liu_POP_Person_Re-identification_2013_ICCV_paper.html:text/html}
}

@inproceedings{liu_semi-supervised_2014,
	title = {Semi-{Supervised} {Coupled} {Dictionary} {Learning} for {Person} {Re}-identification},
	url = {https://openaccess.thecvf.com/content_cvpr_2014/html/Liu_Semi-Supervised_Coupled_Dictionary_2014_CVPR_paper.html},
	urldate = {2021-04-26},
	author = {Liu, Xiao and Song, Mingli and Tao, Dacheng and Zhou, Xingchen and Chen, Chun and Bu, Jiajun},
	year = {2014},
	pages = {3550--3557},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\8A9TDRNF\\Liu et al. - 2014 - Semi-Supervised Coupled Dictionary Learning for Pe.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\PTSNQYY3\\Liu_Semi-Supervised_Coupled_Dictionary_2014_CVPR_paper.html:text/html}
}

@inproceedings{qian_long-term_2020,
	title = {Long-{Term} {Cloth}-{Changing} {Person} {Re}-identification},
	url = {https://openaccess.thecvf.com/content/ACCV2020/html/Qian_Long-Term_Cloth-Changing_Person_Re-identification_ACCV_2020_paper.html},
	language = {en},
	urldate = {2021-04-26},
	author = {Qian, Xuelin and Wang, Wenxuan and Zhang, Li and Zhu, Fangrui and Fu, Yanwei and Xiang, Tao and Jiang, Yu-Gang and Xue, Xiangyang},
	year = {2020},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\AFMCQ97S\\Qian et al. - 2020 - Long-Term Cloth-Changing Person Re-identification.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\QGW64HBZ\\Qian_Long-Term_Cloth-Changing_Person_Re-identification_ACCV_2020_paper.html:text/html}
}

@inproceedings{hao_horizontal_2020,
	title = {Horizontal {Flipping} {Assisted} {Disentangled} {Feature} {Learning} for {Semi}-{Supervised} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content/ACCV2020/html/Hao_Horizontal_Flipping_Assisted_Disentangled_Feature_Learning_for_Semi-Supervised_Person_Re-Identification_ACCV_2020_paper.html},
	language = {en},
	urldate = {2021-04-26},
	author = {Hao, Gehan and Yang, Yang and Zhou, Xue and Wang, Guanan and Lei, Zhen},
	year = {2020},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\9I8ZMLIY\\Hao et al. - 2020 - Horizontal Flipping Assisted Disentangled Feature .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\JM769DUV\\Hao_Horizontal_Flipping_Assisted_Disentangled_Feature_Learning_for_Semi-Supervised_Person_Re-Id.html:text/html}
}

@inproceedings{xiang_second-order_2020,
	title = {Second-order {Camera}-aware {Color} {Transformation} for {Cross}-domain {Person} {Re}-identification},
	url = {https://openaccess.thecvf.com/content/ACCV2020/html/Xiang_Second-order_Camera-aware_Color_Transformation_for_Cross-domain_Person_Re-identification_ACCV_2020_paper.html},
	language = {en},
	urldate = {2021-04-26},
	author = {Xiang, Wangmeng and Yong, Hongwei and Huang, Jianqiang and Hua, Xian-Sheng and Zhang, Lei},
	year = {2020},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\EZCANHJT\\Xiang et al. - 2020 - Second-order Camera-aware Color Transformation for.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\YVN3YTAA\\Xiang_Second-order_Camera-aware_Color_Transformation_for_Cross-domain_Person_Re-identification_.html:text/html}
}

@inproceedings{tang_branch_2020,
	title = {Branch {Interaction} {Network} for {Person} {Re}-identification},
	url = {https://openaccess.thecvf.com/content/ACCV2020/html/Tang_Branch_Interaction_Network_for_Person_Re-identification_ACCV_2020_paper.html},
	language = {en},
	urldate = {2021-04-26},
	author = {Tang, Zengming and Huang, Jun},
	year = {2020},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\6SWUUWEC\\Tang and Huang - 2020 - Branch Interaction Network for Person Re-identific.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\G92MEK9E\\Tang_Branch_Interaction_Network_for_Person_Re-identification_ACCV_2020_paper.html:text/html}
}

@inproceedings{wang_dense-scale_2020,
	title = {Dense-{Scale} {Feature} {Learning} in {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content/ACCV2020/html/Wang_Dense-Scale_Feature_Learning_in_Person_Re-Identification_ACCV_2020_paper.html},
	language = {en},
	urldate = {2021-04-26},
	author = {Wang, Li and Fan, Baoyu and Guo, Zhenhua and Zhao, Yaqian and Zhang, Runze and Li, Rengang and Gong, Weifeng},
	year = {2020},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\65BL4DW4\\Wang et al. - 2020 - Dense-Scale Feature Learning in Person Re-Identifi.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\WTUELNTQ\\Wang_Dense-Scale_Feature_Learning_in_Person_Re-Identification_ACCV_2020_paper.html:text/html}
}

@inproceedings{xiang_part-aware_2020,
	title = {Part-aware {Attention} {Network} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content/ACCV2020/html/Xiang_Part-aware_Attention_Network_for_Person_Re-Identification_ACCV_2020_paper.html},
	language = {en},
	urldate = {2021-04-26},
	author = {Xiang, Wangmeng and Huang, Jianqiang and Hua, Xian-Sheng and Zhang, Lei},
	year = {2020},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\JCD49X5T\\Xiang et al. - 2020 - Part-aware Attention Network for Person Re-Identif.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\IPK9UPXH\\Xiang_Part-aware_Attention_Network_for_Person_Re-Identification_ACCV_2020_paper.html:text/html}
}

@inproceedings{zhong_generalizing_2018,
	title = {Generalizing {A} {Person} {Retrieval} {Model} {Hetero}- and {Homogeneously}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper.html},
	urldate = {2021-04-26},
	author = {Zhong, Zhun and Zheng, Liang and Li, Shaozi and Yang, Yi},
	year = {2018},
	pages = {172--188},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\5ZZUN9KC\\Zhong et al. - 2018 - Generalizing A Person Retrieval Model Hetero- and .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\CXS6TA3X\\Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper.html:text/html}
}

@inproceedings{bak_domain_2018,
	title = {Domain {Adaptation} through {Synthesis} for {Unsupervised} {Person} {Re}-identification},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Slawomir_Bak_Domain_Adaptation_through_ECCV_2018_paper.html},
	urldate = {2021-04-26},
	author = {Bak, Slawomir and Carr, Peter and Lalonde, Jean-Francois},
	year = {2018},
	pages = {189--205},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\T49S8NNY\\Bak et al. - 2018 - Domain Adaptation through Synthesis for Unsupervis.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\4TY2DJRX\\Slawomir_Bak_Domain_Adaptation_through_ECCV_2018_paper.html:text/html}
}

@inproceedings{ye_robust_2018,
	title = {Robust {Anchor} {Embedding} for {Unsupervised} {Video} {Person} {Re}-{Identification} in the {Wild}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Mang_YE_Robust_Anchor_Embedding_ECCV_2018_paper.html},
	urldate = {2021-04-26},
	author = {Ye, Mang and Lan, Xiangyuan and Yuen, Pong C.},
	year = {2018},
	pages = {170--186},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\K4RH54MJ\\Ye et al. - 2018 - Robust Anchor Embedding for Unsupervised Video Per.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\ZU8DTB35\\Mang_YE_Robust_Anchor_Embedding_ECCV_2018_paper.html:text/html}
}

@inproceedings{shen_person_2018,
	title = {Person {Re}-identification with {Deep} {Similarity}-{Guided} {Graph} {Neural} {Network}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Yantao_Shen_Person_Re-identification_with_ECCV_2018_paper.html},
	urldate = {2021-04-26},
	author = {Shen, Yantao and Li, Hongsheng and Yi, Shuai and Chen, Dapeng and Wang, Xiaogang},
	year = {2018},
	pages = {486--504},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\EHBE3JV9\\Shen et al. - 2018 - Person Re-identification with Deep Similarity-Guid.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\BMIIMXIH\\Yantao_Shen_Person_Re-identification_with_ECCV_2018_paper.html:text/html}
}

@inproceedings{yu_hard-aware_2018,
	title = {Hard-{Aware} {Point}-to-{Set} {Deep} {Metric} for {Person} {Re}-identification},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Rui_Yu_Hard-Aware_Point-to-Set_Deep_ECCV_2018_paper.html},
	urldate = {2021-04-26},
	author = {Yu, Rui and Dou, Zhiyong and Bai, Song and Zhang, Zhaoxiang and Xu, Yongchao and Bai, Xiang},
	year = {2018},
	pages = {188--204},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\FVA2F5F2\\Yu et al. - 2018 - Hard-Aware Point-to-Set Deep Metric for Person Re-.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\6A2YK7KN\\Rui_Yu_Hard-Aware_Point-to-Set_Deep_ECCV_2018_paper.html:text/html}
}

@inproceedings{li_adversarial_2018,
	title = {Adversarial {Open}-{World} {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Xiang_Li_Adversarial_Open-World_Person_ECCV_2018_paper.html},
	urldate = {2021-04-26},
	author = {Li, Xiang and Wu, Ancong and Zheng, Wei-Shi},
	year = {2018},
	pages = {280--296},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\JI6R5HRR\\Li et al. - 2018 - Adversarial Open-World Person Re-Identification.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\AK4IVE8L\\Xiang_Li_Adversarial_Open-World_Person_ECCV_2018_paper.html:text/html}
}

@inproceedings{suh_part-aligned_2018,
	title = {Part-{Aligned} {Bilinear} {Representations} for {Person} {Re}-{Identification}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Yumin_Suh_Part-Aligned_Bilinear_Representations_ECCV_2018_paper.html},
	urldate = {2021-04-26},
	author = {Suh, Yumin and Wang, Jingdong and Tang, Siyu and Mei, Tao and Lee, Kyoung Mu},
	year = {2018},
	pages = {402--419},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\LR7UYHC3\\Suh et al. - 2018 - Part-Aligned Bilinear Representations for Person R.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\GGLWEQQT\\Yumin_Suh_Part-Aligned_Bilinear_Representations_ECCV_2018_paper.html:text/html}
}

@inproceedings{wang_mancs_2018,
	title = {Mancs: {A} {Multi}-task {Attentional} {Network} with {Curriculum} {Sampling} for {Person} {Re}-identification},
	shorttitle = {Mancs},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Cheng_Wang_Mancs_A_Multi-task_ECCV_2018_paper.html},
	urldate = {2021-04-26},
	author = {Wang, Cheng and Zhang, Qian and Huang, Chang and Liu, Wenyu and Wang, Xinggang},
	year = {2018},
	pages = {365--381},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\WD3AEADQ\\Wang et al. - 2018 - Mancs A Multi-task Attentional Network with Curri.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\QLUSM3Q6\\Cheng_Wang_Mancs_A_Multi-task_ECCV_2018_paper.html:text/html}
}

@inproceedings{sun_beyond_2018,
	title = {Beyond {Part} {Models}: {Person} {Retrieval} with {Refined} {Part} {Pooling} (and {A} {Strong} {Convolutional} {Baseline})},
	shorttitle = {Beyond {Part} {Models}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Yifan_Sun_Beyond_Part_Models_ECCV_2018_paper.html},
	urldate = {2021-04-26},
	author = {Sun, Yifan and Zheng, Liang and Yang, Yi and Tian, Qi and Wang, Shengjin},
	year = {2018},
	pages = {480--496},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\ZJWFQTYN\\Sun et al. - 2018 - Beyond Part Models Person Retrieval with Refined .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\LJW642EI\\Yifan_Sun_Beyond_Part_Models_ECCV_2018_paper.html:text/html}
}

@incollection{vedaldi_joint_2020,
	address = {Cham},
	title = {Joint {Disentangling} and {Adaptation} for {Cross}-{Domain} {Person} {Re}-{Identification}},
	volume = {12347},
	isbn = {978-3-030-58535-8 978-3-030-58536-5},
	url = {https://link.springer.com/10.1007/978-3-030-58536-5_6},
	abstract = {Although a signiﬁcant progress has been witnessed in supervised person re-identiﬁcation (re-id), it remains challenging to generalize re-id models to new domains due to the huge domain gaps. Recently, there has been a growing interest in using unsupervised domain adaptation to address this scalability issue. Existing methods typically conduct adaptation on the representation space that contains both id-related and id-unrelated factors, thus inevitably undermining the adaptation eﬃcacy of id-related features. In this paper, we seek to improve adaptation by purifying the representation space to be adapted. To this end, we propose a joint learning framework that disentangles id-related/unrelated features and enforces adaptation to work on the id-related feature space exclusively. Our model involves a disentangling module that encodes cross-domain images into a shared appearance space and two separate structure spaces, and an adaptation module that performs adversarial alignment and self-training on the shared appearance space. The two modules are co-designed to be mutually beneﬁcial. Extensive experiments demonstrate that the proposed joint learning framework outperforms the state-of-the-art methods by clear margins.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Zou, Yang and Yang, Xiaodong and Yu, Zhiding and Kumar, B. V. K. Vijaya and Kautz, Jan},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58536-5_6},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {87--104},
	file = {Zou et al. - 2020 - Joint Disentangling and Adaptation for Cross-Domai.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\H3PIRZ5X\\Zou et al. - 2020 - Joint Disentangling and Adaptation for Cross-Domai.pdf:application/pdf}
}

@incollection{vedaldi_appearance-preserving_2020,
	address = {Cham},
	title = {Appearance-{Preserving} {3D} {Convolution} for {Video}-{Based} {Person} {Re}-identification},
	volume = {12347},
	isbn = {978-3-030-58535-8 978-3-030-58536-5},
	url = {https://link.springer.com/10.1007/978-3-030-58536-5_14},
	abstract = {Due to the imperfect person detection results and posture changes, temporal appearance misalignment is unavoidable in videobased person re-identiﬁcation (ReID). In this case, 3D convolution may destroy the appearance representation of person video clips, thus it is harmful to ReID. To address this problem, we propose AppearancePreserving 3D Convolution (AP3D), which is composed of two components: an Appearance-Preserving Module (APM) and a 3D convolution kernel. With APM aligning the adjacent feature maps in pixel level, the following 3D convolution can model temporal information on the premise of maintaining the appearance representation quality. It is easy to combine AP3D with existing 3D ConvNets by simply replacing the original 3D convolution kernels with AP3Ds. Extensive experiments demonstrate the eﬀectiveness of AP3D for video-based ReID and the results on three widely used datasets surpass the state-of-the-arts. Code is available at: https://github.com/guxinqian/AP3D.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Gu, Xinqian and Chang, Hong and Ma, Bingpeng and Zhang, Hongkai and Chen, Xilin},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58536-5_14},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {228--243},
	file = {Gu et al. - 2020 - Appearance-Preserving 3D Convolution for Video-Bas.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\9KKRHITC\\Gu et al. - 2020 - Appearance-Preserving 3D Convolution for Video-Bas.pdf:application/pdf}
}

@incollection{vedaldi_identity-guided_2020,
	address = {Cham},
	title = {Identity-{Guided} {Human} {Semantic} {Parsing} for {Person} {Re}-identification},
	volume = {12348},
	isbn = {978-3-030-58579-2 978-3-030-58580-8},
	url = {https://link.springer.com/10.1007/978-3-030-58580-8_21},
	abstract = {Existing alignment-based methods have to employ the pretrained human parsing models to achieve the pixel-level alignment, and cannot identify the personal belongings (e.g., backpacks and reticule) which are crucial to person re-ID. In this paper, we propose the identityguided human semantic parsing approach (ISP) to locate both the human body parts and personal belongings at pixel-level for aligned person reID only with person identity labels. We design the cascaded clustering on feature maps to generate the pseudo-labels of human parts. Speciﬁcally, for the pixels of all images of a person, we ﬁrst group them to foreground or background and then group the foreground pixels to human parts. The cluster assignments are subsequently used as pseudo-labels of human parts to supervise the part estimation and ISP iteratively learns the feature maps and groups them. Finally, local features of both human body parts and personal belongings are obtained according to the selflearned part estimation, and only features of visible parts are utilized for the retrieval. Extensive experiments on three widely used datasets validate the superiority of ISP over lots of state-of-the-art methods. Our code is available at https://github.com/CASIA-IVA-Lab/ISP-reID.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Zhu, Kuan and Guo, Haiyun and Liu, Zhiwei and Tang, Ming and Wang, Jinqiao},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58580-8_21},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {346--363},
	file = {Zhu et al. - 2020 - Identity-Guided Human Semantic Parsing for Person .pdf:C\:\\Users\\lxt76\\Zotero\\storage\\LWQGKXF4\\Zhu et al. - 2020 - Identity-Guided Human Semantic Parsing for Person .pdf:application/pdf}
}

@incollection{vedaldi_not_2020,
	address = {Cham},
	title = {Do {Not} {Disturb} {Me}: {Person} {Re}-identification {Under} the {Interference} of {Other} {Pedestrians}},
	volume = {12351},
	isbn = {978-3-030-58538-9 978-3-030-58539-6},
	shorttitle = {Do {Not} {Disturb} {Me}},
	url = {https://link.springer.com/10.1007/978-3-030-58539-6_39},
	abstract = {In the conventional person Re-ID setting, it is assumed that cropped images are the person images within the bounding box for each individual. However, in a crowded scene, oﬀ-shelf-detectors may generate bounding boxes involving multiple people, where the large proportion of background pedestrians or human occlusion exists. The representation extracted from such cropped images, which contain both the target and the interference pedestrians, might include distractive information. This will lead to wrong retrieval results. To address this problem, this paper presents a novel deep network termed Pedestrian-Interference Suppression Network (PISNet). PISNet leverages a Query-Guided Attention Block (QGAB) to enhance the feature of the target in the gallery, under the guidance of the query. Furthermore, the involving Guidance Reversed Attention Module and the Multi-Person Separation Loss promote QGAB to suppress the interference of other pedestrians. Our method is evaluated on two new pedestrian-interference datasets and the results show that the proposed method performs favorably against existing Re-ID methods.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Zhao, Shizhen and Gao, Changxin and Zhang, Jun and Cheng, Hao and Han, Chuchu and Jiang, Xinyang and Guo, Xiaowei and Zheng, Wei-Shi and Sang, Nong and Sun, Xing},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58539-6_39},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {647--663},
	file = {Zhao et al. - 2020 - Do Not Disturb Me Person Re-identification Under .pdf:C\:\\Users\\lxt76\\Zotero\\storage\\AV4WUDQM\\Zhao et al. - 2020 - Do Not Disturb Me Person Re-identification Under .pdf:application/pdf}
}

@incollection{vedaldi_multiple_2020,
	address = {Cham},
	title = {Multiple {Expert} {Brainstorming} for {Domain} {Adaptive} {Person} {Re}-{Identification}},
	volume = {12352},
	isbn = {978-3-030-58570-9 978-3-030-58571-6},
	url = {https://link.springer.com/10.1007/978-3-030-58571-6_35},
	abstract = {Often the best performing deep neural models are ensembles of multiple base-level networks, nevertheless, ensemble learning with respect to domain adaptive person re-ID remains unexplored. In this paper, we propose a multiple expert brainstorming network (MEB-Net) for domain adaptive person re-ID, opening up a promising direction about model ensemble problem under unsupervised conditions. MEBNet adopts a mutual learning strategy, where multiple networks with diﬀerent architectures are pre-trained within a source domain as expert models equipped with speciﬁc features and knowledge, while the adaptation is then accomplished through brainstorming (mutual learning) among expert models. MEB-Net accommodates the heterogeneity of experts learned with diﬀerent architectures and enhances discrimination capability of the adapted re-ID model, by introducing a regularization scheme about authority of experts. Extensive experiments on large-scale datasets (Market-1501 and DukeMTMC-reID) demonstrate the superior performance of MEB-Net over the state-of-the-arts. Code is available at https://github.com/YunpengZhai/MEB-Net.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Zhai, Yunpeng and Ye, Qixiang and Lu, Shijian and Jia, Mengxi and Ji, Rongrong and Tian, Yonghong},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58571-6_35},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {594--611},
	file = {Zhai et al. - 2020 - Multiple Expert Brainstorming for Domain Adaptive .pdf:C\:\\Users\\lxt76\\Zotero\\storage\\BFPYJXLV\\Zhai et al. - 2020 - Multiple Expert Brainstorming for Domain Adaptive .pdf:application/pdf}
}

@incollection{vedaldi_global_2020,
	address = {Cham},
	title = {Global {Distance}-{Distributions} {Separation} for {Unsupervised} {Person} {Re}-identification},
	volume = {12352},
	isbn = {978-3-030-58570-9 978-3-030-58571-6},
	url = {https://link.springer.com/10.1007/978-3-030-58571-6_43},
	abstract = {Supervised person re-identiﬁcation (ReID) often has poor scalability and usability in real-world deployments due to domain gaps and the lack of annotations for the target domain data. Unsupervised person ReID through domain adaptation is attractive yet challenging. Existing unsupervised ReID approaches often fail in correctly identifying the positive samples and negative samples through the distance-based matching/ranking. The two distributions of distances for positive sample pairs (Pos-distr) and negative sample pairs (Neg-distr) are often not well separated, having large overlap. To address this problem, we introduce a global distance-distributions separation (GDS) constraint over the two distributions to encourage the clear separation of positive and negative samples from a global view. We model the two global distance distributions as Gaussian distributions and push apart the two distributions while encouraging their sharpness in the unsupervised training process. Particularly, to model the distributions from a global view and facilitate the timely updating of the distributions and the GDS related losses, we leverage a momentum update mechanism for building and maintaining the distribution parameters (mean and variance) and calculate the loss on the ﬂy during the training. Distribution-based hard mining is proposed to further promote the separation of the two distributions. We validate the eﬀectiveness of the GDS constraint in unsupervised ReID networks. Extensive experiments on multiple ReID benchmark datasets show our method leads to signiﬁcant improvement over the baselines and achieves the state-of-the-art performance.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Jin, Xin and Lan, Cuiling and Zeng, Wenjun and Chen, Zhibo},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58571-6_43},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {735--751},
	file = {Jin et al. - 2020 - Global Distance-Distributions Separation for Unsup.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\YA8J5LKZ\\Jin et al. - 2020 - Global Distance-Distributions Separation for Unsup.pdf:application/pdf}
}

@incollection{vedaldi_temporal_2020,
	address = {Cham},
	title = {Temporal {Coherence} or {Temporal} {Motion}: {Which} {Is} {More} {Critical} for {Video}-{Based} {Person} {Re}-identification?},
	volume = {12353},
	isbn = {978-3-030-58597-6 978-3-030-58598-3},
	shorttitle = {Temporal {Coherence} or {Temporal} {Motion}},
	url = {https://link.springer.com/10.1007/978-3-030-58598-3_39},
	abstract = {Video-based person re-identiﬁcation aims to match pedestrians with the consecutive video sequences. While a rich line of work focuses solely on extracting the motion features from pedestrian videos, we show in this paper that the temporal coherence plays a more critical role. To distill the temporal coherence part of video representation from frame representations, we propose a simple yet eﬀective Adversarial Feature Augmentation (AFA) method, which highlights the temporal coherence features by introducing adversarial augmented temporal motion noise. Speciﬁcally, we disentangle the video representation into the temporal coherence and motion parts and randomly change the scale of the temporal motion features as the adversarial noise. The proposed AFA method is a general lightweight component that can be readily incorporated into various methods with negligible cost. We conduct extensive experiments on three challenging datasets including MARS, iLIDS-VID, and DukeMTMC-VideoReID, and the experimental results verify our argument and demonstrate the eﬀectiveness of the proposed method.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Chen, Guangyi and Rao, Yongming and Lu, Jiwen and Zhou, Jie},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58598-3_39},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {660--676},
	file = {Chen et al. - 2020 - Temporal Coherence or Temporal Motion Which Is Mo.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\7ZXY2ZAN\\Chen et al. - 2020 - Temporal Coherence or Temporal Motion Which Is Mo.pdf:application/pdf}
}

@incollection{vedaldi_faster_2020,
	address = {Cham},
	title = {Faster {Person} {Re}-identification},
	volume = {12353},
	isbn = {978-3-030-58597-6 978-3-030-58598-3},
	url = {https://link.springer.com/10.1007/978-3-030-58598-3_17},
	abstract = {Fast person re-identiﬁcation (ReID) aims to search person images quickly and accurately. The main idea of recent fast ReID methods is the hashing algorithm, which learns compact binary codes and performs fast Hamming distance and counting sort. However, a very long code is needed for high accuracy (e.g. 2048), which compromises search speed. In this work, we introduce a new solution for fast ReID by formulating a novel Coarse-to-Fine (CtF) hashing code search strategy, which complementarily uses short and long codes, achieving both faster speed and better accuracy. It uses shorter codes to coarsely rank broad matching similarities and longer codes to reﬁne only a few top candidates for more accurate instance ReID. Speciﬁcally, we design an All-in-One (AiO) framework together with a Distance Threshold Optimization (DTO) algorithm. In AiO, we simultaneously learn and enhance multiple codes of diﬀerent lengths in a single model. It learns multiple codes in a pyramid structure, and encourage shorter codes to mimic longer codes by selfdistillation. DTO solves a complex threshold search problem by a simple optimization process, and the balance between accuracy and speed is easily controlled by a single parameter. It formulates the optimization target as a Fβ score that can be optimised by Gaussian cumulative distribution functions. Experimental results on 2 datasets show that our proposed method (CtF) is not only 8\% more accurate but also 5× faster than contemporary hashing ReID methods. Compared with non-hashing ReID methods, CtF is 50× faster with comparable accuracy. Code is available at https://github.com/wangguanan/light-reid.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Wang, Guan’an and Gong, Shaogang and Cheng, Jian and Hou, Zengguang},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58598-3_17},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {275--292},
	file = {Wang et al. - 2020 - Faster Person Re-identification.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\8Y2FHM98\\Wang et al. - 2020 - Faster Person Re-identification.pdf:application/pdf}
}

@incollection{vedaldi_interpretable_2020,
	address = {Cham},
	title = {Interpretable and {Generalizable} {Person} {Re}-identification with {Query}-{Adaptive} {Convolution} and {Temporal} {Lifting}},
	volume = {12356},
	isbn = {978-3-030-58620-1 978-3-030-58621-8},
	url = {https://link.springer.com/10.1007/978-3-030-58621-8_27},
	abstract = {For person re-identiﬁcation, existing deep networks often focus on representation learning. However, without transfer learning, the learned model is ﬁxed as is, which is not adaptable for handling various unseen scenarios. In this paper, beyond representation learning, we consider how to formulate person image matching directly in deep feature maps. We treat image matching as ﬁnding local correspondences in feature maps, and construct query-adaptive convolution kernels on the ﬂy to achieve local matching. In this way, the matching process and results are interpretable, and this explicit matching is more generalizable than representation features to unseen scenarios, such as unknown misalignments, pose or viewpoint changes. To facilitate end-to-end training of this architecture, we further build a class memory module to cache feature maps of the most recent samples of each class, so as to compute image matching losses for metric learning. Through direct cross-dataset evaluation, the proposed Query-Adaptive Convolution (QAConv) method gains large improvements over popular learning methods (about 10\%+ mAP), and achieves comparable results to many transfer learning methods. Besides, a model-free temporal cooccurrence based score weighting method called TLift is proposed, which improves the performance to a further extent, achieving state-of-the-art results in cross-dataset person re-identiﬁcation. Code is available at https://github.com/ShengcaiLiao/QAConv.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Liao, Shengcai and Shao, Ling},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58621-8_27},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {456--474},
	file = {Liao and Shao - 2020 - Interpretable and Generalizable Person Re-identifi.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\AILYRERR\\Liao and Shao - 2020 - Interpretable and Generalizable Person Re-identifi.pdf:application/pdf}
}

@incollection{vedaldi_deep_2020,
	address = {Cham},
	title = {Deep {Credible} {Metric} {Learning} for {Unsupervised} {Domain} {Adaptation} {Person} {Re}-identification},
	volume = {12353},
	isbn = {978-3-030-58597-6 978-3-030-58598-3},
	url = {https://link.springer.com/10.1007/978-3-030-58598-3_38},
	abstract = {The trained person re-identiﬁcation systems fundamentally need to be deployed on diﬀerent target environments. Learning the crossdomain model has great potential for the scalability of real-world applications. In this paper, we propose a deep credible metric learning (DCML) method for unsupervised domain adaptation person re-identiﬁcation. Unlike existing methods that directly ﬁnetune the model in the target domain with pseudo labels generated by the source pre-trained model, our DCML method adaptively mines credible samples for training to avoid the misleading from noise labels. Speciﬁcally, we design two credibility metrics for sample mining including the k-Nearest Neighbor similarity for density evaluation and the prototype similarity for centrality evaluation. As the increasing of the pseudo label credibility, we progressively adjust the sampling strategy in the training process. In addition, we propose an instance margin spreading loss to further increase instancewise discrimination. Experimental results demonstrate that our DCML method explores credible and valuable training data and improves the performance of unsupervised domain adaptation.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Chen, Guangyi and Lu, Yuhao and Lu, Jiwen and Zhou, Jie},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58598-3_38},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {643--659},
	file = {Chen et al. - 2020 - Deep Credible Metric Learning for Unsupervised Dom.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\WRSRMDWL\\Chen et al. - 2020 - Deep Credible Metric Learning for Unsupervised Dom.pdf:application/pdf}
}

@incollection{vedaldi_rethinking_2020,
	address = {Cham},
	title = {Rethinking the {Distribution} {Gap} of {Person} {Re}-identification with {Camera}-{Based} {Batch} {Normalization}},
	volume = {12357},
	isbn = {978-3-030-58609-6 978-3-030-58610-2},
	url = {https://link.springer.com/10.1007/978-3-030-58610-2_9},
	abstract = {The fundamental difficulty in person re-identification (ReID) lies in learning the correspondence among individual cameras. It strongly demands costly inter-camera annotations, yet the trained models are not guaranteed to transfer well to previously unseen cameras. These problems significantly limit the application of ReID. This paper rethinks the working mechanism of conventional ReID approaches and puts forward a new solution. With an effective operator named Camera-based Batch Normalization (CBN), we force the image data of all cameras to fall onto the same subspace, so that the distribution gap between any camera pair is largely shrunk. This alignment brings two benefits. First, the trained model enjoys better abilities to generalize across scenarios with unseen cameras as well as transfer across multiple training sets. Second, we can rely on intra-camera annotations, which have been undervalued before due to the lack of cross-camera information, to achieve competitive ReID performance. Experiments on a wide range of ReID tasks demonstrate the effectiveness of our approach. The code is available at https://github.com/automan000/Camera-based-Person-ReID.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Zhuang, Zijie and Wei, Longhui and Xie, Lingxi and Zhang, Tianyu and Zhang, Hengheng and Wu, Haozhe and Ai, Haizhou and Tian, Qi},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58610-2_9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {140--157},
	file = {Zhuang et al. - 2020 - Rethinking the Distribution Gap of Person Re-ident.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\XREYEAXU\\Zhuang et al. - 2020 - Rethinking the Distribution Gap of Person Re-ident.pdf:application/pdf}
}

@incollection{vedaldi_generalizing_2020,
	address = {Cham},
	title = {Generalizing {Person} {Re}-{Identification} by {Camera}-{Aware} {Invariance} {Learning} and {Cross}-{Domain} {Mixup}},
	volume = {12360},
	isbn = {978-3-030-58554-9 978-3-030-58555-6},
	url = {https://link.springer.com/10.1007/978-3-030-58555-6_14},
	abstract = {Despite the impressive performance under the single-domain setup, current fully-supervised models for person re-identiﬁcation (re-ID) degrade signiﬁcantly when deployed to an unseen domain. According to the characteristics of cross-domain re-ID, such degradation is mainly attributed to the dramatic variation within the target domain and the severe shift between the source and target domain. To achieve a model that generalizes well to the target domain, it is desirable to take both issues into account. In terms of the former issue, one of the most successful solutions is to enforce consistency between nearest-neighbors in the embedding space. However, we ﬁnd that the search of neighbors is highly biased due to the discrepancy across cameras. To this end, we improve the vanilla neighborhood invariance approach by imposing the constraint in a camera-aware manner. As for the latter issue, we propose a novel cross-domain mixup scheme. It alleviates the abrupt transfer by introducing the interpolation between the two domains as a transition state. Extensive experiments on three public benchmarks demonstrate the superiority of our method. Without any auxiliary data or models, it outperforms existing state-of-the-arts by a large margin. The code is available at https://github.com/LuckyDC/generalizing-reid.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Luo, Chuanchen and Song, Chunfeng and Zhang, Zhaoxiang},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58555-6_14},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {224--241},
	file = {Luo et al. - 2020 - Generalizing Person Re-Identification by Camera-Aw.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\HBIPMFFK\\Luo et al. - 2020 - Generalizing Person Re-Identification by Camera-Aw.pdf:application/pdf}
}

@incollection{vedaldi_unsupervised_2020,
	address = {Cham},
	title = {Unsupervised {Domain} {Adaptation} with {Noise} {Resistible} {Mutual}-{Training} for {Person} {Re}-identification},
	volume = {12356},
	isbn = {978-3-030-58620-1 978-3-030-58621-8},
	url = {https://link.springer.com/10.1007/978-3-030-58621-8_31},
	abstract = {Unsupervised domain adaptation (UDA) in the task of person re-identiﬁcation (re-ID) is highly challenging due to large domain divergence and no class overlap between domains. Pseudo-label based selftraining is one of the representative techniques to address UDA. However, label noise caused by unsupervised clustering is always a trouble to selftraining methods. To depress noises in pseudo-labels, this paper proposes a Noise Resistible Mutual-Training (NRMT) method, which maintains two networks during training to perform collaborative clustering and mutual instance selection. On one hand, collaborative clustering eases the ﬁtting to noisy instances by allowing the two networks to use pseudolabels provided by each other as an additional supervision. On the other hand, mutual instance selection further selects reliable and informative instances for training according to the peer-conﬁdence and relationship disagreement of the networks. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art UDA methods for person re-ID.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Zhao, Fang and Liao, Shengcai and Xie, Guo-Sen and Zhao, Jian and Zhang, Kaihao and Shao, Ling},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58621-8_31},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {526--544},
	file = {Zhao et al. - 2020 - Unsupervised Domain Adaptation with Noise Resistib.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\64FDWSCG\\Zhao et al. - 2020 - Unsupervised Domain Adaptation with Noise Resistib.pdf:application/pdf}
}

@incollection{vedaldi_dynamic_2020,
	address = {Cham},
	title = {Dynamic {Dual}-{Attentive} {Aggregation} {Learning} for {Visible}-{Infrared} {Person} {Re}-identification},
	volume = {12362},
	isbn = {978-3-030-58519-8 978-3-030-58520-4},
	url = {https://link.springer.com/10.1007/978-3-030-58520-4_14},
	abstract = {Visible-infrared person re-identiﬁcation (VI-ReID) is a challenging cross-modality pedestrian retrieval problem. Due to the large intra-class variations and cross-modality discrepancy with large amount of sample noise, it is diﬃcult to learn discriminative part features. Existing VI-ReID methods instead tend to learn global representations, which have limited discriminability and weak robustness to noisy images. In this paper, we propose a novel dynamic dual-attentive aggregation (DDAG) learning method by mining both intra-modality part-level and cross-modality graph-level contextual cues for VI-ReID. We propose an intra-modality weighted-part attention module to extract discriminative part-aggregated features, by imposing the domain knowledge on the part relationship mining. To enhance robustness against noisy samples, we introduce cross-modality graph structured attention to reinforce the representation with the contextual relations across the two modalities. We also develop a parameter-free dynamic dual aggregation learning strategy to adaptively integrate the two components in a progressive joint training manner. Extensive experiments demonstrate that DDAG outperforms the state-of-the-art methods under various settings.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Ye, Mang and Shen, Jianbing and J. Crandall, David and Shao, Ling and Luo, Jiebo},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58520-4_14},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {229--247},
	file = {Ye et al. - 2020 - Dynamic Dual-Attentive Aggregation Learning for Vi.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\BMTULVHT\\Ye et al. - 2020 - Dynamic Dual-Attentive Aggregation Learning for Vi.pdf:application/pdf}
}

@incollection{vedaldi_joint_2020-1,
	address = {Cham},
	title = {Joint {Visual} and {Temporal} {Consistency} for {Unsupervised} {Domain} {Adaptive} {Person} {Re}-identification},
	volume = {12369},
	isbn = {978-3-030-58585-3 978-3-030-58586-0},
	url = {https://link.springer.com/10.1007/978-3-030-58586-0_29},
	abstract = {Unsupervised domain adaptive person Re-IDentiﬁcation (ReID) is challenging because of the large domain gap between source and target domains, as well as the lackage of labeled data on the target domain. This paper tackles this challenge through jointly enforcing visual and temporal consistency in the combination of a local one-hot classiﬁcation and a global multi-class classiﬁcation. The local one-hot classiﬁcation assigns images in a training batch with diﬀerent person IDs, then adopts a Self-Adaptive Classiﬁcation (SAC) model to classify them. The global multi-class classiﬁcation is achieved by predicting labels on the entire unlabeled training set with the Memory-based Temporal-guided Cluster (MTC). MTC predicts multi-class labels by considering both visual similarity and temporal consistency to ensure the quality of label prediction. The two classiﬁcation models are combined in a uniﬁed framework, which eﬀectively leverages the unlabeled data for discriminative feature learning. Experimental results on three large-scale ReID datasets demonstrate the superiority of proposed method in both unsupervised and unsupervised domain adaptive ReID tasks. For example, under unsupervised setting, our method outperforms recent unsupervised domain adaptive methods, which leverage more labels for training.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Li, Jianing and Zhang, Shiliang},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58586-0_29},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {483--499},
	file = {Li and Zhang - 2020 - Joint Visual and Temporal Consistency for Unsuperv.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\LLYXYETZ\\Li and Zhang - 2020 - Joint Visual and Temporal Consistency for Unsuperv.pdf:application/pdf}
}

@incollection{vedaldi_temporal_2020-1,
	address = {Cham},
	title = {Temporal {Complementary} {Learning} for {Video} {Person} {Re}-identification},
	volume = {12370},
	isbn = {978-3-030-58594-5 978-3-030-58595-2},
	url = {https://link.springer.com/10.1007/978-3-030-58595-2_24},
	abstract = {This paper proposes a Temporal Complementary Learning Network that extracts complementary features of consecutive video frames for video person re-identiﬁcation. Firstly, we introduce a Temporal Saliency Erasing (TSE) module including a saliency erasing operation and a series of ordered learners. Speciﬁcally, for a speciﬁc frame of a video, the saliency erasing operation drives the speciﬁc learner to mine new and complementary parts by erasing the parts activated by previous frames. Such that the diverse visual features can be discovered for consecutive frames and ﬁnally form an integral characteristic of the target identity. Furthermore, a Temporal Saliency Boosting (TSB) module is designed to propagate the salient information among video frames to enhance the salient feature. It is complementary to TSE by eﬀectively alleviating the information loss caused by the erasing operation of TSE. Extensive experiments show our method performs favorably against stateof-the-arts. The source code is available at https://github.com/blueblue272/VideoReID-TCLNet.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Hou, Ruibing and Chang, Hong and Ma, Bingpeng and Shan, Shiguang and Chen, Xilin},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58595-2_24},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {388--405},
	file = {Hou et al. - 2020 - Temporal Complementary Learning for Video Person R.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\FSNM7S5Y\\Hou et al. - 2020 - Temporal Complementary Learning for Video Person R.pdf:application/pdf}
}



@incollection{vedaldi_unsupervised_2020-1,
	address = {Cham},
	title = {Unsupervised {Domain} {Adaptation} in the {Dissimilarity} {Space} for {Person} {Re}-identification},
	volume = {12372},
	isbn = {978-3-030-58582-2 978-3-030-58583-9},
	url = {https://link.springer.com/10.1007/978-3-030-58583-9_10},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Mekhazni, Djebril and Bhuiyan, Amran and Ekladious, George and Granger, Eric},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58583-9_10},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {159--174},
	file = {Mekhazni et al. - 2020 - Unsupervised Domain Adaptation in the Dissimilarit.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\XR2LRZDL\\Mekhazni et al. - 2020 - Unsupervised Domain Adaptation in the Dissimilarit.pdf:application/pdf}
}

@incollection{vedaldi_guided_2020,
	address = {Cham},
	title = {Guided {Saliency} {Feature} {Learning} for {Person} {Re}-identification in {Crowded} {Scenes}},
	volume = {12373},
	isbn = {978-3-030-58603-4 978-3-030-58604-1},
	url = {https://link.springer.com/10.1007/978-3-030-58604-1_22},
	abstract = {Person Re-identiﬁcation (Re-ID) in crowed scenes is a challenging problem, where people are frequently partially occluded by objects and other people. However, few studies have provided ﬂexible solutions to re-identifying people in an image containing a partial occlusion body part. In this paper, we propose a simple occlusion-aware approach to address the problem. The proposed method ﬁrst leverages a fully convolutional network to generate spatial features. And then we design a combination of a pose-guided and mask-guided layer to generate saliency heatmap to further guide discriminative feature learning. More importantly, we propose a new matching approach, called Guided Adaptive Spatial Matching (GASM), which expects that each spatial feature in the query can ﬁnd the most similar spatial features of a person in a gallery to match. Especially, We use the saliency heatmap to guide the adaptive spatial matching by assigning the foreground human parts with larger weights adaptively. The eﬀectiveness of the proposed GASM is demonstrated on two occluded person datasets: Crowd REID (51.52\%) and Occluded REID (80.25\%) and three benchmark person datasets: Market1501 (95.31\%), DukeMTMC-reID (88.12\%) and MSMT17 (79.52\%). Additionally, GASM achieves good performance on cross-domain person Re-ID. The code and models are available at: https://github.com/ JDAI-CV/fast-reid/blob/master/projects/CrowdReID.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {He, Lingxiao and Liu, Wu},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58604-1_22},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {357--373},
	file = {He and Liu - 2020 - Guided Saliency Feature Learning for Person Re-ide.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\83KRWDA3\\He and Liu - 2020 - Guided Saliency Feature Learning for Person Re-ide.pdf:application/pdf}
}

@incollection{vedaldi_attention-driven_2020,
	address = {Cham},
	title = {An {Attention}-{Driven} {Two}-{Stage} {Clustering} {Method} for {Unsupervised} {Person} {Re}-identification},
	volume = {12373},
	isbn = {978-3-030-58603-4 978-3-030-58604-1},
	url = {https://link.springer.com/10.1007/978-3-030-58604-1_2},
	abstract = {The progressive clustering method and its variants, which iteratively generate pseudo labels for unlabeled data and per form feature learning, have shown great process in unsupervised person re-identiﬁcation (re-id). However, they have an intrinsic problem of modeling the incamera variability of images successfully, that is, pedestrian features extracted from the same camera tend to be clustered into the same class. This often results in a non-convergent model in the real world application of clustering based re-id models, leading to degenerated performance. In the present study, we propose an attention-driven two-stage clustering (ADTC) method to solve this problem. Speciﬁcally, our method consists of two strategies. Firstly, we use an unsupervised attention kernel to shift the learned features from the image background to the pedestrian foreground, which results in more informative clusters. Secondly, to aid the learning of the attention driven clustering model, we separate the clustering process into two stages. We ﬁrst use kmeans to generate the centroids of clusters (stage 1) and then apply the k-reciprocal Jaccard distance (KRJD) metric to re-assign data points to each cluster (stage 2). By iteratively learning with the two strategies, the attentive regions are gradually shifted from the background to the foreground and the features become more discriminative. Using two benchmark datasets Market1501 and DukeMTMC, we demonstrate that our model outperforms other state-of-the-art unsupervised approaches for person re-id.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Ji, Zilong and Zou, Xiaolong and Lin, Xiaohan and Liu, Xiao and Huang, Tiejun and Wu, Si},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58604-1_2},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {20--36},
	file = {Ji et al. - 2020 - An Attention-Driven Two-Stage Clustering Method fo.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\7SMTXA73\\Ji et al. - 2020 - An Attention-Driven Two-Stage Clustering Method fo.pdf:application/pdf}
}

@incollection{ferrari_unsupervised_2018,
	address = {Cham},
	title = {Unsupervised {Person} {Re}-identification by {Deep} {Learning} {Tracklet} {Association}},
	volume = {11208},
	isbn = {978-3-030-01224-3 978-3-030-01225-0},
	url = {http://link.springer.com/10.1007/978-3-030-01225-0_45},
	abstract = {Most existing person re-identiﬁcation (re-id) methods rely on supervised model learning on per-camera-pair manually labelled pairwise training data. This leads to poor scalability in practical re-id deployment due to the lack of exhaustive identity labelling of image positive and negative pairs for every camera pair. In this work, we address this problem by proposing an unsupervised re-id deep learning approach capable of incrementally discovering and exploiting the underlying re-id discriminative information from automatically generated person tracklet data from videos in an end-to-end model optimisation. We formulate a Tracklet Association Unsupervised Deep Learning (TAUDL) framework characterised by jointly learning per-camera (within-camera) tracklet association (labelling) and cross-camera tracklet correlation by maximising the discovery of most likely tracklet relationships across camera views. Extensive experiments demonstrate the superiority of the proposed TAUDL model over the state-of-the-art unsupervised and domain adaptation reid methods using six person re-id benchmarking datasets.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Li, Minxian and Zhu, Xiatian and Gong, Shaogang},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01225-0_45},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {772--788},
	file = {Li et al. - 2018 - Unsupervised Person Re-identification by Deep Lear.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\L3BV3IFU\\Li et al. - 2018 - Unsupervised Person Re-identification by Deep Lear.pdf:application/pdf}
}

@incollection{ferrari_improving_2018,
	address = {Cham},
	title = {Improving {Deep} {Visual} {Representation} for {Person} {Re}-identification by {Global} and {Local} {Image}-language {Association}},
	volume = {11220},
	isbn = {978-3-030-01269-4 978-3-030-01270-0},
	url = {http://link.springer.com/10.1007/978-3-030-01270-0_4},
	abstract = {Person re-identiﬁcation is an important task that requires learning discriminative visual features for distinguishing diﬀerent person identities. Diverse auxiliary information has been utilized to improve the visual feature learning. In this paper, we propose to exploit natural language description as additional training supervisions for eﬀective visual features. Compared with other auxiliary information, language can describe a speciﬁc person from more compact and semantic visual aspects, thus is complementary to the pixel-level image data. Our method not only learns better global visual feature with the supervision of the overall description but also enforces semantic consistencies between local visual and linguistic features, which is achieved by building global and local image-language associations. The global image-language association is established according to the identity labels, while the local association is based upon the implicit correspondences between image regions and noun phrases. Extensive experiments demonstrate the eﬀectiveness of employing language as training supervisions with the two association schemes. Our method achieves state-of-the-art performance without utilizing any auxiliary information during testing and shows better performance than other joint embedding methods for the image-language association.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Chen, Dapeng and Li, Hongsheng and Liu, Xihui and Shen, Yantao and Shao, Jing and Yuan, Zejian and Wang, Xiaogang},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01270-0_4},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {56--73},
	file = {Chen et al. - 2018 - Improving Deep Visual Representation for Person Re.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\JC7YA5R4\\Chen et al. - 2018 - Improving Deep Visual Representation for Person Re.pdf:application/pdf}
}

@incollection{ferrari_reinforced_2018,
	address = {Cham},
	title = {Reinforced {Temporal} {Attention} and {Split}-{Rate} {Transfer} for {Depth}-{Based} {Person} {Re}-identification},
	volume = {11209},
	isbn = {978-3-030-01227-4 978-3-030-01228-1},
	url = {http://link.springer.com/10.1007/978-3-030-01228-1_44},
	abstract = {We address the problem of person re-identiﬁcation from commodity depth sensors. One challenge for depth-based recognition is data scarcity. Our ﬁrst contribution addresses this problem by introducing split-rate RGB-to-Depth transfer, which leverages large RGB datasets more eﬀectively than popular ﬁne-tuning approaches. Our transfer scheme is based on the observation that the model parameters at the bottom layers of a deep convolutional neural network can be directly shared between RGB and depth data while the remaining layers need to be ﬁne-tuned rapidly. Our second contribution enhances re-identiﬁcation for video by implementing temporal attention as a Bernoulli-Sigmoid unit acting upon frame-level features. Since this unit is stochastic, the temporal attention parameters are trained using reinforcement learning. Extensive experiments validate the accuracy of our method in person re-identiﬁcation from depth sequences. Finally, in a scenario where subjects wear unseen clothes, we show large performance gains compared to a state-of-the-art model which relies on RGB data.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Karianakis, Nikolaos and Liu, Zicheng and Chen, Yinpeng and Soatto, Stefano},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01228-1_44},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {737--756},
	file = {Karianakis et al. - 2018 - Reinforced Temporal Attention and Split-Rate Trans.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\L2FKDXR8\\Karianakis et al. - 2018 - Reinforced Temporal Attention and Split-Rate Trans.pdf:application/pdf}
}

@incollection{ferrari_maximum_2018,
	address = {Cham},
	title = {Maximum {Margin} {Metric} {Learning} over {Discriminative} {Nullspace} for {Person} {Re}-identification},
	volume = {11217},
	isbn = {978-3-030-01260-1 978-3-030-01261-8},
	url = {http://link.springer.com/10.1007/978-3-030-01261-8_8},
	abstract = {In this paper we propose a novel metric learning framework called Nullspace Kernel Maximum Margin Metric Learning (NK3ML) which efﬁciently addresses the small sample size (SSS) problem inherent in person re-identiﬁcation and offers a signiﬁcant performance gain over existing state-of-the-art methods. Taking advantage of the very high dimensionality of the feature space, the metric is learned using a maximum margin criterion (MMC) over a discriminative nullspace where all training sample points of a given class map onto a single point, minimizing the within class scatter. A kernel version of MMC is used to obtain a better between class separation. Extensive experiments on four challenging benchmark datasets for person re-identiﬁcation demonstrate that the proposed algorithm outperforms all existing methods. We obtain 99.8\% rank-1 accuracy on the most widely accepted and challenging dataset VIPeR, compared to the previous state of the art being only 63.92\%.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Ali, T. M. Feroz and Chaudhuri, Subhasis},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01261-8_8},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {123--141},
	file = {Ali and Chaudhuri - 2018 - Maximum Margin Metric Learning over Discriminative.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\IYIT9AE9\\Ali and Chaudhuri - 2018 - Maximum Margin Metric Learning over Discriminative.pdf:application/pdf}
}

@incollection{ferrari_pose-normalized_2018,
	address = {Cham},
	title = {Pose-{Normalized} {Image} {Generation} for {Person} {Re}-identification},
	volume = {11213},
	isbn = {978-3-030-01239-7 978-3-030-01240-3},
	url = {http://link.springer.com/10.1007/978-3-030-01240-3_40},
	abstract = {Person Re-identiﬁcation (re-id) faces two major challenges: the lack of cross-view paired training data and learning discriminative identity-sensitive and view-invariant features in the presence of large pose variations. In this work, we address both problems by proposing a novel deep person image generation model for synthesizing realistic person images conditional on the pose. The model is based on a generative adversarial network (GAN) designed speciﬁcally for pose normalization in re-id, thus termed pose-normalization GAN (PN-GAN). With the synthesized images, we can learn a new type of deep re-id features free of the inﬂuence of pose variations. We show that these features are complementary to features learned with the original images. Importantly, a more realistic unsupervised learning setting is considered in this work, and our model is shown to have the potential to be generalizable to a new re-id dataset without any ﬁne-tuning. The codes will be released at https://github.com/naiq/PN\_GAN.},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Qian, Xuelin and Fu, Yanwei and Xiang, Tao and Wang, Wenxuan and Qiu, Jie and Wu, Yang and Jiang, Yu-Gang and Xue, Xiangyang},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01240-3_40},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {661--678},
	file = {Qian et al. - 2018 - Pose-Normalized Image Generation for Person Re-ide.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\IJL75U2B\\Qian et al. - 2018 - Pose-Normalized Image Generation for Person Re-ide.pdf:application/pdf}
}

@article{chen_frame-guided_2020,
	title = {Frame-{Guided} {Region}-{Aligned} {Representation} for {Video} {Person} {Re}-{Identification}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6632},
	doi = {10.1609/aaai.v34i07.6632},
	language = {en},
	number = {07},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Zengqun and Zhou, Zhiheng and Huang, Junchu and Zhang, Pengyu and Li, Bo},
	month = apr,
	year = {2020},
	note = {Number: 07},
	pages = {10591--10598},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\E5QZ2R6T\\Chen et al. - 2020 - Frame-Guided Region-Aligned Representation for Vid.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\LWQJ6SYC\\6632.html:text/html}
}

@article{huang_domain_2020,
	title = {Domain {Adaptive} {Attention} {Learning} for {Unsupervised} {Person} {Re}-{Identification}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6762},
	doi = {10.1609/aaai.v34i07.6762},
	language = {en},
	number = {07},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Huang, Yangru and Peng, Peixi and Jin, Yi and Li, Yidong and Xing, Junliang},
	month = apr,
	year = {2020},
	note = {Number: 07},
	pages = {11069--11076},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\X8BKZHNX\\Huang et al. - 2020 - Domain Adaptive Attention Learning for Unsupervise.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\82VHWCFZ\\6762.html:text/html}
}

@article{jiang_rethinking_2020,
	title = {Rethinking {Temporal} {Fusion} for {Video}-{Based} {Person} {Re}-{Identification} on {Semantic} and {Time} {Aspect}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6770},
	doi = {10.1609/aaai.v34i07.6770},
	language = {en},
	number = {07},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Jiang, Xinyang and Gong, Yifei and Guo, Xiaowei and Yang, Qize and Huang, Feiyue and Zheng, Wei-Shi and Zheng, Feng and Sun, Xing},
	month = apr,
	year = {2020},
	note = {Number: 07},
	pages = {11133--11140},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\JQQIHIHV\\Jiang et al. - 2020 - Rethinking Temporal Fusion for Video-Based Person .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\RTT53KPC\\6770.html:text/html}
}

@article{jin_semantics-aligned_2020,
	title = {Semantics-{Aligned} {Representation} {Learning} for {Person} {Re}-{Identification}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6775},
	doi = {10.1609/aaai.v34i07.6775},
	language = {en},
	number = {07},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Jin, Xin and Lan, Cuiling and Zeng, Wenjun and Wei, Guoqiang and Chen, Zhibo},
	month = apr,
	year = {2020},
	note = {Number: 07},
	pages = {11173--11180},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\3YT6ISTL\\Jin et al. - 2020 - Semantics-Aligned Representation Learning for Pers.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\69Q9M4N2\\6775.html:text/html}
}

@article{li_appearance_2020,
	title = {Appearance and {Motion} {Enhancement} for {Video}-{Based} {Person} {Re}-{Identification}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6802},
	doi = {10.1609/aaai.v34i07.6802},
	language = {en},
	number = {07},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Li, Shuzhao and Yu, Huimin and Hu, Haoji},
	month = apr,
	year = {2020},
	note = {Number: 07},
	pages = {11394--11401},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\LNWWJRGE\\Li et al. - 2020 - Appearance and Motion Enhancement for Video-Based .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\IP2FWML8\\6802.html:text/html}
}

@article{li_relation-guided_2020,
	title = {Relation-{Guided} {Spatial} {Attention} and {Temporal} {Refinement} for {Video}-{Based} {Person} {Re}-{Identification}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6807},
	doi = {10.1609/aaai.v34i07.6807},
	language = {en},
	number = {07},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Li, Xingze and Zhou, Wengang and Zhou, Yun and Li, Houqiang},
	month = apr,
	year = {2020},
	note = {Number: 07},
	pages = {11434--11441},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\VJ3C8JKU\\Li et al. - 2020 - Relation-Guided Spatial Attention and Temporal Ref.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\DWHRZHBH\\6807.html:text/html}
}

@article{park_relation_2020,
	title = {Relation {Network} for {Person} {Re}-{Identification}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6857},
	doi = {10.1609/aaai.v34i07.6857},
	language = {en},
	number = {07},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Park, Hyunjong and Ham, Bumsub},
	month = apr,
	year = {2020},
	note = {Number: 07},
	pages = {11839--11847},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\4G5ABZV3\\Park and Ham - 2020 - Relation Network for Person Re-Identification.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\PLJG4QIX\\6857.html:text/html}
}

@article{wang_cross-modality_2020,
	title = {Cross-{Modality} {Paired}-{Images} {Generation} for {RGB}-{Infrared} {Person} {Re}-{Identification}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6894},
	doi = {10.1609/aaai.v34i07.6894},
	language = {en},
	number = {07},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wang, Guan-An and Zhang, Tianzhu and Yang, Yang and Cheng, Jian and Chang, Jianlong and Liang, Xu and Hou, Zeng-Guang},
	month = apr,
	year = {2020},
	note = {Number: 07},
	pages = {12144--12151},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\Q9MGYIMC\\Wang et al. - 2020 - Cross-Modality Paired-Images Generation for RGB-In.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\XPVFXFV8\\6894.html:text/html}
}

@article{wu_tracklet_2020,
	title = {Tracklet {Self}-{Supervised} {Learning} for {Unsupervised} {Person} {Re}-{Identification}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6921},
	doi = {10.1609/aaai.v34i07.6921},
	language = {en},
	number = {07},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wu, Guile and Zhu, Xiatian and Gong, Shaogang},
	month = apr,
	year = {2020},
	note = {Number: 07},
	pages = {12362--12369},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\A7CQ9RT7\\Wu et al. - 2020 - Tracklet Self-Supervised Learning for Unsupervised.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\I4YFUS2R\\6921.html:text/html}
}

@article{yang_asymmetric_2020,
	title = {Asymmetric {Co}-{Teaching} for {Unsupervised} {Cross}-{Domain} {Person} {Re}-{Identification}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6950},
	doi = {10.1609/aaai.v34i07.6950},
	language = {en},
	number = {07},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Yang, Fengxiang and Li, Ke and Zhong, Zhun and Luo, Zhiming and Sun, Xing and Cheng, Hao and Guo, Xiaowei and Huang, Feiyue and Ji, Rongrong and Li, Shaozi},
	month = apr,
	year = {2020},
	note = {Number: 07},
	pages = {12597--12604},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\8Q69329V\\Yang et al. - 2020 - Asymmetric Co-Teaching for Unsupervised Cross-Doma.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\5LWSLB4K\\6950.html:text/html}
}

@article{zhang_single_2020,
	title = {Single {Camera} {Training} for {Person} {Re}-{Identification}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6985},
	doi = {10.1609/aaai.v34i07.6985},
	language = {en},
	number = {07},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhang, Tianyu and Xie, Lingxi and Wei, Longhui and Zhang, Yongfei and Li, Bo and Tian, Qi},
	month = apr,
	year = {2020},
	note = {Number: 07},
	pages = {12878--12885},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\PLTN2RWC\\Zhang et al. - 2020 - Single Camera Training for Person Re-Identificatio.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\47HG8R97\\6985.html:text/html}
}

@article{zhu_viewpoint-aware_2020,
	title = {Viewpoint-{Aware} {Loss} with {Angular} {Regularization} for {Person} {Re}-{Identification}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/7014},
	doi = {10.1609/aaai.v34i07.7014},
	language = {en},
	number = {07},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhu, Zhihui and Jiang, Xinyang and Zheng, Feng and Guo, Xiaowei and Huang, Feiyue and Sun, Xing and Zheng, Weishi},
	month = apr,
	year = {2020},
	note = {Number: 07},
	pages = {13114--13121},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\4ET3K6NX\\Zhu et al. - 2020 - Viewpoint-Aware Loss with Angular Regularization f.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\39FGYTHY\\7014.html:text/html}
}

@article{chen_learning_2019,
	title = {Learning {Resolution}-{Invariant} {Deep} {Representations} for {Person} {Re}-{Identification}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4832},
	doi = {10.1609/aaai.v33i01.33018215},
	language = {en},
	number = {01},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Yun-Chun and Li, Yu-Jhe and Du, Xiaofei and Wang, Yu-Chiang Frank},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {8215--8222},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\CLVZCZVS\\Chen et al. - 2019 - Learning Resolution-Invariant Deep Representations.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\LG52FKS7\\4832.html:text/html}
}

@article{fu_sta_2019,
	title = {{STA}: {Spatial}-{Temporal} {Attention} for {Large}-{Scale} {Video}-{Based} {Person} {Re}-{Identification}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{STA}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4841},
	doi = {10.1609/aaai.v33i01.33018287},
	language = {en},
	number = {01},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Fu, Yang and Wang, Xiaoyang and Wei, Yunchao and Huang, Thomas},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {8287--8294},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\RBIZ9X4J\\Fu et al. - 2019 - STA Spatial-Temporal Attention for Large-Scale Vi.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\TD8NPRIB\\4841.html:text/html}
}

@article{fu_horizontal_2019,
	title = {Horizontal {Pyramid} {Matching} for {Person} {Re}-{Identification}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4842},
	doi = {10.1609/aaai.v33i01.33018295},
	language = {en},
	number = {01},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Fu, Yang and Wei, Yunchao and Zhou, Yuqian and Shi, Honghui and Huang, Gao and Wang, Xinchao and Yao, Zhiqiang and Huang, Thomas},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {8295--8302},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\Y6YIKQ8D\\Fu et al. - 2019 - Horizontal Pyramid Matching for Person Re-Identifi.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\2E8NEJT5\\4842.html:text/html}
}

@article{hao_hsme_2019,
	title = {{HSME}: {Hypersphere} {Manifold} {Embedding} for {Visible} {Thermal} {Person} {Re}-{Identification}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{HSME}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4853},
	doi = {10.1609/aaai.v33i01.33018385},
	language = {en},
	number = {01},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Hao, Yi and Wang, Nannan and Li, Jie and Gao, Xinbo},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {8385--8392},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\5EZ2APPD\\Hao et al. - 2019 - HSME Hypersphere Manifold Embedding for Visible T.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\BZU2ZSV5\\4853.html:text/html}
}

@article{li_multi-scale_2019,
	title = {Multi-{Scale} {3D} {Convolution} {Network} for {Video} {Based} {Person} {Re}-{Identification}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4882},
	doi = {10.1609/aaai.v33i01.33018618},
	language = {en},
	number = {01},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Li, Jianing and Zhang, Shiliang and Huang, Tiejun},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {8618--8625},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\9U68TT55\\Li et al. - 2019 - Multi-Scale 3D Convolution Network for Video Based.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\NMP8N3QU\\4882.html:text/html}
}

@article{lin_bottom-up_2019,
	title = {A {Bottom}-{Up} {Clustering} {Approach} to {Unsupervised} {Person} {Re}-{Identification}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4898},
	doi = {10.1609/aaai.v33i01.33018738},
	language = {en},
	number = {01},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Lin, Yutian and Dong, Xuanyi and Zheng, Liang and Yan, Yan and Yang, Yi},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {8738--8745},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\IFFCEWDY\\Lin et al. - 2019 - A Bottom-Up Clustering Approach to Unsupervised Pe.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\ERCJN8KC\\4898.html:text/html}
}

@article{liu_spatial_2019,
	title = {Spatial and {Temporal} {Mutual} {Promotion} for {Video}-{Based} {Person} {Re}-{Identification}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4904},
	doi = {10.1609/aaai.v33i01.33018786},
	language = {en},
	number = {01},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Liu, Yiheng and Yuan, Zhenxun and Zhou, Wengang and Li, Houqiang},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {8786--8793},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\FB5LYPDN\\Liu et al. - 2019 - Spatial and Temporal Mutual Promotion for Video-Ba.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\R5SBCKJR\\4904.html:text/html}
}

@article{ro_backbone_2019,
	title = {Backbone {Cannot} {Be} {Trained} at {Once}: {Rolling} {Back} to {Pre}-{Trained} {Network} for {Person} {Re}-{Identification}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Backbone {Cannot} {Be} {Trained} at {Once}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4913},
	doi = {10.1609/aaai.v33i01.33018859},
	language = {en},
	number = {01},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ro, Youngmin and Choi, Jongwon and Jo, Dae Ung and Heo, Byeongho and Lim, Jongin and Choi, Jin Young},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {8859--8867},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\KRJWCVLN\\Ro et al. - 2019 - Backbone Cannot Be Trained at Once Rolling Back t.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\MK9NFE8H\\4913.html:text/html}
}

@article{wang_spatial-temporal_2019,
	title = {Spatial-{Temporal} {Person} {Re}-{Identification}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4921},
	doi = {10.1609/aaai.v33i01.33018933},
	language = {en},
	number = {01},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wang, Guangcong and Lai, Jianhuang and Huang, Peigen and Xie, Xiaohua},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {8933--8940},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\ZY4D54L6\\Wang et al. - 2019 - Spatial-Temporal Person Re-Identification.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\XL6PFM4I\\4921.html:text/html}
}

@article{zhang_learning_2019,
	title = {Learning a {Key}-{Value} {Memory} {Co}-{Attention} {Matching} {Network} for {Person} {Re}-{Identification}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4959},
	doi = {10.1609/aaai.v33i01.33019235},
	language = {en},
	number = {01},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhang, Yaqing and Li, Xi and Zhang, Zhongfei},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {9235--9242},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\J3VYNDCU\\Zhang et al. - 2019 - Learning a Key-Value Memory Co-Attention Matching .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\F7556TNP\\4959.html:text/html}
}

@article{zhang_learning_2019-1,
	title = {Learning {Incremental} {Triplet} {Margin} for {Person} {Re}-{Identification}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4960},
	doi = {10.1609/aaai.v33i01.33019243},
	language = {en},
	number = {01},
	urldate = {2021-04-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhang, Yingying and Zhong, Qiaoyong and Ma, Liang and Xie, Di and Pu, Shiliang},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {9243--9250},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\9I62I9AE\\Zhang et al. - 2019 - Learning Incremental Triplet Margin for Person Re-.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\DNM6DREU\\4960.html:text/html}
}

@article{huang_video-based_2018,
	title = {Video-{Based} {Person} {Re}-{Identification} via {Self} {Paced} {Weighting}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11857},
	language = {en},
	number = {1},
	urldate = {2021-05-22},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Huang, Wenjun and Liang, Chao and Yu, Yi and Wang, Zheng and Ruan, Weijian and Hu, Ruimin},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {Self-Paced Learning},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\UPQE26R4\\Huang et al. - 2018 - Video-Based Person Re-Identification via Self Pace.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\3UVXPHFD\\11857.html:text/html}
}


@article{song_region-based_2018,
	title = {Region-{Based} {Quality} {Estimation} {Network} for {Large}-{Scale} {Person} {Re}-{Identification}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/12305},
	language = {en},
	number = {1},
	urldate = {2021-05-22},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Song, Guanglu and Leng, Biao and Liu, Yu and Hetang, Congrui and Cai, Shaofan},
	month = apr,
	year = {2018},
	note = {Number: 1},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\TYDMBWW4\\Song et al. - 2018 - Region-Based Quality Estimation Network for Large-.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\VIYYSU2Q\\12305.html:text/html}
}



@article{yang_unsupervised_2017,
	title = {Unsupervised {Learning} of {Multi}-{Level} {Descriptors} for {Person} {Re}-{Identification}},
	volume = {31},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11224},
	language = {en},
	number = {1},
	urldate = {2021-05-23},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Yang, Yang and Wen, Longyin and Lyu, Siwei and Li, Stan},
	month = feb,
	year = {2017},
	note = {Number: 1},
	keywords = {person re-identification},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\TTY8BNS7\\Yang et al. - 2017 - Unsupervised Learning of Multi-Level Descriptors f.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\WX4TZNP6\\11224.html:text/html}
}


@article{yang_joint_2021,
	title = {Joint {Noise}-{Tolerant} {Learning} and {Meta} {Camera} {Shift} {Adaptation} for {Unsupervised} {Person} {Re}-{Identification}},
	url = {http://arxiv.org/abs/2103.04618},
	abstract = {This paper considers the problem of unsupervised person re-identification (re-ID), which aims to learn discriminative models with unlabeled data. One popular method is to obtain pseudo-label by clustering and use them to optimize the model. Although this kind of approach has shown promising accuracy, it is hampered by 1) noisy labels produced by clustering and 2) feature variations caused by camera shift. The former will lead to incorrect optimization and thus hinders the model accuracy. The latter will result in assigning the intra-class samples of different cameras to different pseudo-label, making the model sensitive to camera variations. In this paper, we propose a unified framework to solve both problems. Concretely, we propose a Dynamic and Symmetric Cross-Entropy loss (DSCE) to deal with noisy samples and a camera-aware meta-learning algorithm (MetaCam) to adapt camera shift. DSCE can alleviate the negative effects of noisy samples and accommodate the change of clusters after each clustering step. MetaCam simulates cross-camera constraint by splitting the training data into meta-train and meta-test based on camera IDs. With the interacted gradient from meta-train and meta-test, the model is enforced to learn camera-invariant features. Extensive experiments on three re-ID benchmarks show the effectiveness and the complementary of the proposed DSCE and MetaCam. Our method outperforms the state-of-the-art methods on both fully unsupervised re-ID and unsupervised domain adaptive re-ID.},
	urldate = {2021-04-26},
	journal = {arXiv:2103.04618 [cs]},
	author = {Yang, Fengxiang and Zhong, Zhun and Luo, Zhiming and Cai, Yuanzheng and Lin, Yaojin and Li, Shaozi and Sebe, Nicu},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.04618},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\BYU8JSZ2\\Yang et al. - 2021 - Joint Noise-Tolerant Learning and Meta Camera Shif.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\462AHH3C\\2103.html:text/html}
}

@article{xuan_intra-inter_2021,
	title = {Intra-{Inter} {Camera} {Similarity} for {Unsupervised} {Person} {Re}-{Identification}},
	url = {http://arxiv.org/abs/2103.11658},
	abstract = {Most of unsupervised person Re-Identification (Re-ID) works produce pseudo-labels by measuring the feature similarity without considering the distribution discrepancy among cameras, leading to degraded accuracy in label computation across cameras. This paper targets to address this challenge by studying a novel intra-inter camera similarity for pseudo-label generation. We decompose the sample similarity computation into two stage, i.e., the intra-camera and inter-camera computations, respectively. The intra-camera computation directly leverages the CNN features for similarity computation within each camera. Pseudo-labels generated on different cameras train the re-id model in a multi-branch network. The second stage considers the classification scores of each sample on different cameras as a new feature vector. This new feature effectively alleviates the distribution discrepancy among cameras and generates more reliable pseudo-labels. We hence train our re-id model in two stages with intra-camera and inter-camera pseudo-labels, respectively. This simple intra-inter camera similarity produces surprisingly good performance on multiple datasets, e.g., achieves rank-1 accuracy of 89.5\% on the Market1501 dataset, outperforming the recent unsupervised works by 9+\%, and is comparable with the latest transfer learning works that leverage extra annotations.},
	urldate = {2021-04-26},
	journal = {arXiv:2103.11658 [cs]},
	author = {Xuan, Shiyu and Zhang, Shiliang},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.11658},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\US7576NG\\Xuan and Zhang - 2021 - Intra-Inter Camera Similarity for Unsupervised Per.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\ZF42Q7NA\\2103.html:text/html}
}

@article{yan_anchor-free_2021,
	title = {Anchor-{Free} {Person} {Search}},
	url = {http://arxiv.org/abs/2103.11617},
	abstract = {Person search aims to simultaneously localize and identify a query person from realistic, uncropped images, which can be regarded as the unified task of pedestrian detection and person re-identification (re-id). Most existing works employ two-stage detectors like Faster-RCNN, yielding encouraging accuracy but with high computational overhead. In this work, we present the Feature-Aligned Person Search Network (AlignPS), the first anchor-free framework to efficiently tackle this challenging task. AlignPS explicitly addresses the major challenges, which we summarize as the misalignment issues in different levels (i.e., scale, region, and task), when accommodating an anchor-free detector for this task. More specifically, we propose an aligned feature aggregation module to generate more discriminative and robust feature embeddings by following a "re-id first" principle. Such a simple design directly improves the baseline anchor-free model on CUHK-SYSU by more than 20\% in mAP. Moreover, AlignPS outperforms state-of-the-art two-stage methods, with a higher speed. Code is available at https://github.com/daodaofr/AlignPS},
	urldate = {2021-04-26},
	journal = {arXiv:2103.11617 [cs]},
	author = {Yan, Yichao and Li, Jingpeng and Qin, Jie and Bai, Song and Liao, Shengcai and Liu, Li and Zhu, Fan and Shao, Ling},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.11617},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\7Y6JLLYD\\Yan et al. - 2021 - Anchor-Free Person Search.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\KM3V53S9\\2103.html:text/html}
}

@article{pu_lifelong_2021,
	title = {Lifelong {Person} {Re}-{Identification} via {Adaptive} {Knowledge} {Accumulation}},
	url = {http://arxiv.org/abs/2103.12462},
	abstract = {Person ReID methods always learn through a stationary domain that is fixed by the choice of a given dataset. In many contexts (e.g., lifelong learning), those methods are ineffective because the domain is continually changing in which case incremental learning over multiple domains is required potentially. In this work we explore a new and challenging ReID task, namely lifelong person re-identification (LReID), which enables to learn continuously across multiple domains and even generalise on new and unseen domains. Following the cognitive processes in the human brain, we design an Adaptive Knowledge Accumulation (AKA) framework that is endowed with two crucial abilities: knowledge representation and knowledge operation. Our method alleviates catastrophic forgetting on seen domains and demonstrates the ability to generalize to unseen domains. Correspondingly, we also provide a new and large-scale benchmark for LReID. Extensive experiments demonstrate our method outperforms other competitors by a margin of 5.8\% mAP in generalising evaluation.},
	urldate = {2021-04-26},
	journal = {arXiv:2103.12462 [cs]},
	author = {Pu, Nan and Chen, Wei and Liu, Yu and Bakker, Erwin M. and Lew, Michael S.},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.12462},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\MDL4UHCB\\Pu et al. - 2021 - Lifelong Person Re-Identification via Adaptive Kno.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\59RQCPEE\\2103.html:text/html}
}

@article{zheng_group-aware_2021,
	title = {Group-aware {Label} {Transfer} for {Domain} {Adaptive} {Person} {Re}-identification},
	url = {http://arxiv.org/abs/2103.12366},
	abstract = {Unsupervised Domain Adaptive (UDA) person re-identification (ReID) aims at adapting the model trained on a labeled source-domain dataset to a target-domain dataset without any further annotations. Most successful UDA-ReID approaches combine clustering-based pseudo-label prediction with representation learning and perform the two steps in an alternating fashion. However, offline interaction between these two steps may allow noisy pseudo labels to substantially hinder the capability of the model. In this paper, we propose a Group-aware Label Transfer (GLT) algorithm, which enables the online interaction and mutual promotion of pseudo-label prediction and representation learning. Specifically, a label transfer algorithm simultaneously uses pseudo labels to train the data while refining the pseudo labels as an online clustering algorithm. It treats the online label refinery problem as an optimal transport problem, which explores the minimum cost for assigning M samples to N pseudo labels. More importantly, we introduce a group-aware strategy to assign implicit attribute group IDs to samples. The combination of the online label refining algorithm and the group-aware strategy can better correct the noisy pseudo label in an online fashion and narrow down the search space of the target identity. The effectiveness of the proposed GLT is demonstrated by the experimental results (Rank-1 accuracy) for Market1501\${\textbackslash}to\$DukeMTMC (82.0{\textbackslash}\%) and DukeMTMC\${\textbackslash}to\$Market1501 (92.2{\textbackslash}\%), remarkably closing the gap between unsupervised and supervised performance on person re-identification.},
	urldate = {2021-04-26},
	journal = {arXiv:2103.12366 [cs]},
	author = {Zheng, Kecheng and Liu, Wu and He, Lingxiao and Mei, Tao and Luo, Jiebo and Zha, Zheng-Jun},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.12366},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\4KEK9795\\Zheng et al. - 2021 - Group-aware Label Transfer for Domain Adaptive Per.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\UNP5GRQ6\\2103.html:text/html}
}

@article{chen_neural_2021,
	title = {Neural {Feature} {Search} for {RGB}-{Infrared} {Person} {Re}-{Identification}},
	url = {http://arxiv.org/abs/2104.02366},
	abstract = {RGB-Infrared person re-identification (RGB-IR ReID) is a challenging cross-modality retrieval problem, which aims at matching the person-of-interest over visible and infrared camera views. Most existing works achieve performance gains through manually-designed feature selection modules, which often require significant domain knowledge and rich experience. In this paper, we study a general paradigm, termed Neural Feature Search (NFS), to automate the process of feature selection. Specifically, NFS combines a dual-level feature search space and a differentiable search strategy to jointly select identity-related cues in coarse-grained channels and fine-grained spatial pixels. This combination allows NFS to adaptively filter background noises and concentrate on informative parts of human bodies in a data-driven manner. Moreover, a cross-modality contrastive optimization scheme further guides NFS to search features that can minimize modality discrepancy whilst maximizing inter-class distance. Extensive experiments on mainstream benchmarks demonstrate that our method outperforms state-of-the-arts, especially achieving better performance on the RegDB dataset with significant improvement of 11.20\% and 8.64\% in Rank-1 and mAP, respectively.},
	urldate = {2021-04-26},
	journal = {arXiv:2104.02366 [cs]},
	author = {Chen, Yehansen and Wan, Lin and Li, Zhihang and Jing, Qianyan and Sun, Zongyuan},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.02366},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\96E2E7N4\\Chen et al. - 2021 - Neural Feature Search for RGB-Infrared Person Re-I.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\W9R5SZQ5\\2104.html:text/html}
}

@article{li_combined_2021,
	title = {Combined {Depth} {Space} based {Architecture} {Search} {For} {Person} {Re}-identification},
	url = {http://arxiv.org/abs/2104.04163},
	abstract = {Most works on person re-identification (ReID) take advantage of large backbone networks such as ResNet, which are designed for image classification instead of ReID, for feature extraction. However, these backbones may not be computationally efficient or the most suitable architectures for ReID. In this work, we aim to design a lightweight and suitable network for ReID. We propose a novel search space called Combined Depth Space (CDS), based on which we search for an efficient network architecture, which we call CDNet, via a differentiable architecture search algorithm. Through the use of the combined basic building blocks in CDS, CDNet tends to focus on combined pattern information that is typically found in images of pedestrians. We then propose a low-cost search strategy named the Top-k Sample Search strategy to make full use of the search space and avoid trapping in local optimal result. Furthermore, an effective Fine-grained Balance Neck (FBLNeck), which is removable at the inference time, is presented to balance the effects of triplet loss and softmax loss during the training process. Extensive experiments show that our CDNet ({\textasciitilde}1.8M parameters) has comparable performance with state-of-the-art lightweight networks.},
	urldate = {2021-04-26},
	journal = {arXiv:2104.04163 [cs]},
	author = {Li, Hanjun and Wu, Gaojie and Zheng, Wei-Shi},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.04163},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\GRLEBPGQ\\Li et al. - 2021 - Combined Depth Space based Architecture Search For.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\QA2FQE9M\\2104.html:text/html}
}

@article{choi_meta_2021,
	title = {Meta {Batch}-{Instance} {Normalization} for {Generalizable} {Person} {Re}-{Identification}},
	url = {http://arxiv.org/abs/2011.14670},
	abstract = {Although supervised person re-identification (Re-ID) methods have shown impressive performance, they suffer from a poor generalization capability on unseen domains. Therefore, generalizable Re-ID has recently attracted growing attention. Many existing methods have employed an instance normalization technique to reduce style variations, but the loss of discriminative information could not be avoided. In this paper, we propose a novel generalizable Re-ID framework, named Meta Batch-Instance Normalization (MetaBIN). Our main idea is to generalize normalization layers by simulating unsuccessful generalization scenarios beforehand in the meta-learning pipeline. To this end, we combine learnable batch-instance normalization layers with meta-learning and investigate the challenging cases caused by both batch and instance normalization layers. Moreover, we diversify the virtual simulations via our meta-train loss accompanied by a cyclic inner-updating manner to boost generalization capability. After all, the MetaBIN framework prevents our model from overfitting to the given source styles and improves the generalization capability to unseen domains without additional data augmentation or complicated network design. Extensive experimental results show that our model outperforms the state-of-the-art methods on the large-scale domain generalization Re-ID benchmark and the cross-domain Re-ID problem. The source code is available at: https://github.com/bismex/MetaBIN.},
	urldate = {2021-04-26},
	journal = {arXiv:2011.14670 [cs]},
	author = {Choi, Seokeon and Kim, Taekyung and Jeong, Minki and Park, Hyoungseob and Kim, Changick},
	month = mar,
	year = {2021},
	note = {arXiv: 2011.14670},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\JFLXYFRX\\Choi et al. - 2021 - Meta Batch-Instance Normalization for Generalizabl.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\5LY39BYQ\\2011.html:text/html}
}

@inproceedings{zhang_query_2012,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Query {Specific} {Fusion} for {Image} {Retrieval}},
	isbn = {978-3-642-33709-3},
	doi = {10.1007/978-3-642-33709-3_47},
	abstract = {Recent image retrieval algorithms based on local features indexed by a vocabulary tree and holistic features indexed by compact hashing codes both demonstrate excellent scalability. However, their retrieval precision may vary dramatically among queries. This motivates us to investigate how to fuse the ordered retrieval sets given by multiple retrieval methods, to further enhance the retrieval precision. Thus, we propose a graph-based query specific fusion approach where multiple retrieval sets are merged and reranked by conducting a link analysis on a fused graph. The retrieval quality of an individual method is measured by the consistency of the top candidates’ nearest neighborhoods. Hence, the proposed method is capable of adaptively integrating the strengths of the retrieval methods using local or holistic features for different queries without any supervision. Extensive experiments demonstrate competitive performance on 4 public datasets, i.e., the UKbench, Corel-5K, Holidays and San Francisco Landmarks datasets.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2012},
	publisher = {Springer},
	author = {Zhang, Shaoting and Yang, Ming and Cour, Timothee and Yu, Kai and Metaxas, Dimitris N.},
	editor = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
	year = {2012},
	keywords = {Image Retrieval, Query Image, Rank Aggregation, Retrieval Method, Retrieval Result},
	pages = {660--673},
	file = {Springer Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\C5278SYD\\Zhang et al. - 2012 - Query Specific Fusion for Image Retrieval.pdf:application/pdf}
}


@article{hinton_improving_2012,
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	url = {http://arxiv.org/abs/1207.0580},
	abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
	urldate = {2021-04-27},
	journal = {arXiv:1207.0580 [cs]},
	author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	month = jul,
	year = {2012},
	note = {arXiv: 1207.0580},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\F4CHCHRC\\Hinton et al. - 2012 - Improving neural networks by preventing co-adaptat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\UTV68ZVX\\1207.html:text/html}
}


@inproceedings{kostinger_large_2012,
	title = {Large scale metric learning from equivalence constraints},
	doi = {10.1109/CVPR.2012.6247939},
	abstract = {In this paper, we raise important issues on scalability and the required degree of supervision of existing Mahalanobis metric learning methods. Often rather tedious optimization procedures are applied that become computationally intractable on a large scale. Further, if one considers the constantly growing amount of data it is often infeasible to specify fully supervised labels for all data points. Instead, it is easier to specify labels in form of equivalence constraints. We introduce a simple though effective strategy to learn a distance metric from equivalence constraints, based on a statistical inference perspective. In contrast to existing methods we do not rely on complex optimization problems requiring computationally expensive iterations. Hence, our method is orders of magnitudes faster than comparable methods. Results on a variety of challenging benchmarks with rather diverse nature demonstrate the power of our method. These include faces in unconstrained environments, matching before unseen object instances and person re-identification across spatially disjoint cameras. In the latter two benchmarks we clearly outperform the state-of-the-art.},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Köstinger, Martin and Hirzer, Martin and Wohlhart, Paul and Roth, Peter M. and Bischof, Horst},
	month = jun,
	year = {2012},
	note = {ISSN: 1063-6919},
	keywords = {Training, Benchmark testing, Measurement, Optimization, Databases, Support vector machines, Scalability},
	pages = {2288--2295},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\RZJSKWGC\\6247939.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\9X7XWK8R\\Köstinger et al. - 2012 - Large scale metric learning from equivalence const.pdf:application/pdf}
}

@inproceedings{wang_semi-coupled_2012,
	title = {Semi-coupled dictionary learning with applications to image super-resolution and photo-sketch synthesis},
	doi = {10.1109/CVPR.2012.6247930},
	abstract = {In various computer vision applications, often we need to convert an image in one style into another style for better visualization, interpretation and recognition; for examples, up-convert a low resolution image to a high resolution one, and convert a face sketch into a photo for matching, etc. A semi-coupled dictionary learning (SCDL) model is proposed in this paper to solve such cross-style image synthesis problems. Under SCDL, a pair of dictionaries and a mapping function will be simultaneously learned. The dictionary pair can well characterize the structural domains of the two styles of images, while the mapping function can reveal the intrinsic relationship between the two styles' domains. In SCDL, the two dictionaries will not be fully coupled, and hence much flexibility can be given to the mapping function for an accurate conversion across styles. Moreover, clustering and image nonlocal redundancy are introduced to enhance the robustness of SCDL. The proposed SCDL model is applied to image super-resolution and photo-sketch synthesis, and the experimental results validated its generality and effectiveness in cross-style image synthesis.},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Wang, Shenlong and Zhang, Lei and Liang, Yan and Pan, Quan},
	month = jun,
	year = {2012},
	note = {ISSN: 1063-6919},
	keywords = {Clustering algorithms, Dictionaries, Encoding, Face, Image generation, Image reconstruction, Image resolution},
	pages = {2216--2223},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\CZ2TIZ9N\\6247930.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\ZXPDMJLM\\Wang et al. - 2012 - Semi-coupled dictionary learning with applications.pdf:application/pdf}
}

@article{roweis_nonlinear_2000,
	title = {Nonlinear {Dimensionality} {Reduction} by {Locally} {Linear} {Embedding}},
	volume = {290},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/290/5500/2323},
	doi = {10.1126/science.290.5500.2323},
	abstract = {Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.},
	language = {en},
	number = {5500},
	urldate = {2021-04-27},
	journal = {Science},
	author = {Roweis, Sam T. and Saul, Lawrence K.},
	month = dec,
	year = {2000},
	pmid = {11125150},
	note = {Publisher: American Association for the Advancement of Science
Section: Report},
	pages = {2323--2326},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\3IICISNT\\Roweis and Saul - 2000 - Nonlinear Dimensionality Reduction by Locally Line.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\TTJW6A2R\\2323.html:text/html}
}

@incollection{zheng_manifold_2009,
	address = {London},
	series = {Advances in {Pattern} {Recognition}},
	title = {Manifold {Learning}},
	isbn = {978-1-84882-312-9},
	url = {https://doi.org/10.1007/978-1-84882-312-9_4},
	abstract = {Manifold learning methods are one of the most exciting developments in machine learning in recent years. The central idea underlying these methods is that although natural data is typically represented in very high-dimensional spaces, the process generating the data is often thought to have relatively few degrees of freedom. A natural mathematical characterization of this intuition is to model the data as lying on or near a low-dimensional manifoldRecently, manifold learning has also been applied in utilizing both labeled and unlabeled data for classification, that is, semi-supervised learning. For example, once the manifold is estimated, then the Laplace–Beltrami operator may be used to provide a basis for maps intrinsically defined on this manifold and then the appropriate classifier (map) is estimated on the basis of the labeled examples.In this chapter, we will discuss the manifold perspective of visual pattern representation, dimensionality reduction, and classification problems, as well as a survey that includes manifold learning concepts, technical mechanisms, and algorithms.},
	language = {en},
	urldate = {2021-04-27},
	booktitle = {Statistical {Learning} and {Pattern} {Analysis} for {Image} and {Video} {Processing}},
	publisher = {Springer},
	author = {Zheng, Nanning and Xue, Jianru},
	editor = {Zheng, Nanning and Xue, Jianru},
	year = {2009},
	doi = {10.1007/978-1-84882-312-9_4},
	keywords = {Geodesic Distance, Kernel Principal Component Analysis, Locally Linear Embedding, Neighborhood Graph, Neighborhood Size},
	pages = {87--119},
	file = {Springer Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\ANXAVR3B\\Zheng and Xue - 2009 - Manifold Learning.pdf:application/pdf}
}

@article{izenman_introduction_2012,
	title = {Introduction to manifold learning},
	volume = {4},
	issn = {1939-0068},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.1222},
	doi = {https://doi.org/10.1002/wics.1222},
	abstract = {A popular research area today in statistics and machine learning is that of manifold learning, which is related to the algorithmic techniques of dimensionality reduction. Manifold learning can be divided into linear and nonlinear methods. Linear methods, which have long been part of the statistician's toolbox for analyzing multivariate data, include principal component analysis (PCA) and multidimensional scaling (MDS). Recently, there has been a flurry of research activity on nonlinear manifold learning, which includes Isomap, local linear embedding, Laplacian eigenmaps, Hessian eigenmaps, and diffusion maps. Some of these techniques are nonlinear generalizations of the linear methods. The algorithmic process of most of these techniques consists of three steps: a nearest-neighbor search, a definition of distances or affinities between points (a key ingredient for the success of these methods), and an eigenproblem for embedding high-dimensional points into a lower dimensional space. This article gives us a brief survey of these new methods and indicates their strengths and weaknesses. WIREs Comput Stat 2012 doi: 10.1002/wics.1222 This article is categorized under: Statistical and Graphical Methods of Data Analysis {\textgreater} Dimension Reduction Statistical Learning and Exploratory Methods of the Data Sciences {\textgreater} Manifold Learning Statistical and Graphical Methods of Data Analysis {\textgreater} Multivariate Analysis},
	language = {en},
	number = {5},
	urldate = {2021-04-27},
	journal = {WIREs Computational Statistics},
	author = {Izenman, Alan Julian},
	year = {2012},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1222},
	keywords = {diffusion maps, dimensionality reduction, Isomap, Laplacian eigenmaps, local linear embedding},
	pages = {439--446},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\WMX64UYU\\Izenman - 2012 - Introduction to manifold learning.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\FK6IC3UZ\\wics.html:text/html}
}

@article{radford_unsupervised_2016,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	urldate = {2021-04-27},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = jan,
	year = {2016},
	note = {arXiv: 1511.06434},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\DXVMUWLL\\Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\884XP3BN\\1511.html:text/html}
}

@inproceedings{ge_mutual_2020,
	title = {Mutual {Mean}-{Teaching}: {Pseudo} {Label} {Refinery} for {Unsupervised} {Domain} {Adaptation} on {Person} {Re}-identification},
	shorttitle = {Mutual {Mean}-{Teaching}},
	url = {https://iclr.cc/virtual_2020/poster_rJlnOhVYPS.html},
	abstract = {Person re-identification (re-ID) aims at identifying the same persons' images across different cameras. However, domain diversities between different datasets pose an evident challenge for adapting the re-ID model trained on one dataset to another one. State-of-the-art unsupervised domain adaptation methods for person re-ID transferred the learned knowledge from the source domain by optimizing with pseudo labels created by clustering algorithms on the target domain. Although they achieved state-of-the-art performances, the inevitable label noise caused by the clustering procedure was ignored. Such noisy pseudo labels substantially hinders the model's capability on further improving feature representations on the target domain. In order to mitigate the effects of noisy pseudo labels, we propose to softly refine the pseudo labels in the target domain by proposing an unsupervised framework, Mutual Mean-Teaching (MMT), to learn better features from the target domain via off-line refined hard pseudo labels and on-line refined soft pseudo labels in an alternative training manner. In addition, the common practice is to adopt both the classification loss and the triplet loss jointly for achieving optimal performances in person re-ID models. However, conventional triplet loss cannot work with softly refined labels. To solve this problem, a novel soft softmax-triplet loss is proposed to support learning with soft pseudo triplet labels for achieving the optimal domain adaptation performance. The proposed MMT framework achieves considerable improvements of 14.4\%, 18.2\%, 13.1\% and 16.4\% mAP on Market-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT unsupervised domain adaptation tasks.},
	language = {en},
	urldate = {2021-04-27},
	author = {Ge, Yixiao and Chen, Dapeng and Li, Hongsheng},
	month = apr,
	year = {2020},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\AXWFNRDN\\Ge et al. - 2020 - Mutual Mean-Teaching Pseudo Label Refinery for Un.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\WA45NWGY\\poster_rJlnOhVYPS.html:text/html}
}

@article{lv_dilemma_2020,
	title = {The {Dilemma} of {TriHard} {Loss} and an {Element}-{Weighted} {TriHard} {Loss} for {Person} {Re}-{Identification}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/c96c08f8bb7960e11a1239352a479053-Abstract.html},
	language = {en},
	urldate = {2021-04-27},
	journal = {Advances in Neural Information Processing Systems},
	author = {Lv, Yihao and Gu, Youzhi and Xinggao, Liu},
	year = {2020},
	pages = {17391--17402},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\I8ZGLJ4Y\\Lv et al. - 2020 - The Dilemma of TriHard Loss and an Element-Weighte.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\88ICAEXT\\c96c08f8bb7960e11a1239352a479053-Abstract.html:text/html}
}


@inproceedings{pedagadi_local_2013,
	title = {Local {Fisher} {Discriminant} {Analysis} for {Pedestrian} {Re}-identification},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2013/html/Pedagadi_Local_Fisher_Discriminant_2013_CVPR_paper.html},
	urldate = {2021-04-29},
	author = {Pedagadi, Sateesh and Orwell, James and Velastin, Sergio and Boghossian, Boghos},
	year = {2013},
	pages = {3318--3325},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\NVCBE33M\\Pedagadi et al. - 2013 - Local Fisher Discriminant Analysis for Pedestrian .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\CNI9XHQQ\\Pedagadi_Local_Fisher_Discriminant_2013_CVPR_paper.html:text/html}
}

@inproceedings{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://proceedings.mlr.press/v37/ioffe15.html},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the t...},
	language = {en},
	urldate = {2021-04-30},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {448--456},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\55TK4YMA\\Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\DJ4MFKV4\\ioffe15.html:text/html}
}


@article{wu_deep_2019,
	title = {Deep learning-based methods for person re-identification: {A} comprehensive review},
	volume = {337},
	issn = {0925-2312},
	shorttitle = {Deep learning-based methods for person re-identification},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231219301225},
	doi = {10.1016/j.neucom.2019.01.079},
	abstract = {In recent years, person re-identification (ReID) has received much attention since it is a fundamental task in intelligent surveillance systems and has widespread application prospects in numerous fields. Given an image of a pedestrian captured from one camera, the task is to identify this pedestrian from the gallery set captured by other multiple cameras. It is a challenging issue since the appearance of a pedestrian may suffer great changes across different cameras. The task has been greatly boosted by deep learning technology. There are mainly six types of deep learning-based methods designed for this issue, i.e. identification deep model, verification deep model, distance metric-based deep model, part-based deep model, video-based deep model and data augmentation-based deep model. In this paper, we first give a comprehensive review of current six types of deep learning methods. Second, we present the detailed descriptions of existing person ReID datasets. Then, some state-of-the-art performances of methods over recent years on several representative ReID datasets are summarized. Finally, we conclude this paper and discuss the future directions of the person ReID.},
	language = {en},
	urldate = {2021-05-21},
	journal = {Neurocomputing},
	author = {Wu, Di and Zheng, Si-Jia and Zhang, Xiao-Ping and Yuan, Chang-An and Cheng, Fei and Zhao, Yang and Lin, Yong-Jun and Zhao, Zhong-Qiu and Jiang, Yong-Li and Huang, De-Shuang},
	month = apr,
	year = {2019},
	keywords = {Deep learning, Literature review, Person re-identification},
	pages = {354--371},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\HYNF2WYJ\\Wu et al. - 2019 - Deep learning-based methods for person re-identifi.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\76QPFZJU\\S0925231219301225.html:text/html}
}	
@inproceedings{chahar_study_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Study} on {Deep} {Convolutional} {Neural} {Network} {Based} {Approaches} for {Person} {Re}-identification},
	isbn = {978-3-319-69900-4},
	doi = {10.1007/978-3-319-69900-4_69},
	abstract = {Person re-identification is a process to identify the same person again viewed by disjoint field of view of cameras. It is a challenging problem due to visual ambiguity in a person’s appearance across different camera views. These difficulties are often compounded by low resolution surveillance images, occlusion, background clutter and varying lighting conditions. In recent years, person re-identification community obtained large size of annotated datasets and deep learning architecture based approaches have obtained significant improvement in the accuracy over the years as compared to hand-crafted approaches. In this survey paper, we have classified deep learning based approaches into two categories, i.e., image-based and video-based person re-identification. We have also presented the currently ongoing under developing works, issues and future directions for person re-identification.},
	language = {en},
	booktitle = {Pattern {Recognition} and {Machine} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Chahar, Harendra and Nain, Neeta},
	editor = {Shankar, B. Uma and Ghosh, Kuntal and Mandal, Deba Prasad and Ray, Shubhra Sankar and Zhang, David and Pal, Sankar K.},
	year = {2017},
	keywords = {Convolutional neural network, Person re-identification},
	pages = {543--548},
	file = {Springer Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\3CVCM6IT\\Chahar and Nain - 2017 - A Study on Deep Convolutional Neural Network Based.pdf:application/pdf}
}	
@article{vezzani_people_2013,
	title = {People reidentification in surveillance and forensics: {A} survey},
	volume = {46},
	issn = {0360-0300},
	shorttitle = {People reidentification in surveillance and forensics},
	url = {https://doi.org/10.1145/2543581.2543596},
	doi = {10.1145/2543581.2543596},
	abstract = {The field of surveillance and forensics research is currently shifting focus and is now showing an ever increasing interest in the task of people reidentification. This is the task of assigning the same identifier to all instances of a particular individual captured in a series of images or videos, even after the occurrence of significant gaps over time or space. People reidentification can be a useful tool for people analysis in security as a data association method for long-term tracking in surveillance. However, current identification techniques being utilized present many difficulties and shortcomings. For instance, they rely solely on the exploitation of visual cues such as color, texture, and the object’s shape. Despite the many advances in this field, reidentification is still an open problem. This survey aims to tackle all the issues and challenging aspects of people reidentification while simultaneously describing the previously proposed solutions for the encountered problems. This begins with the first attempts of holistic descriptors and progresses to the more recently adopted 2D and 3D model-based approaches. The survey also includes an exhaustive treatise of all the aspects of people reidentification, including available datasets, evaluation metrics, and benchmarking.},
	number = {2},
	urldate = {2021-05-21},
	journal = {ACM Computing Surveys},
	author = {Vezzani, Roberto and Baltieri, Davide and Cucchiara, Rita},
	month = dec,
	year = {2013},
	keywords = {computer vision, multimedia surveillance, People reidentification},
	pages = {29:1--29:37},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\CKICHLNF\\Vezzani et al. - 2013 - People reidentification in surveillance and forens.pdf:application/pdf}
}


@article{nambiar_gait-based_2019,
	title = {Gait-based {Person} {Re}-identification: {A} {Survey}},
	volume = {52},
	issn = {0360-0300},
	shorttitle = {Gait-based {Person} {Re}-identification},
	url = {https://doi.org/10.1145/3243043},
	doi = {10.1145/3243043},
	abstract = {The way people walk is a strong correlate of their identity. Several studies have shown that both humans and machines can recognize individuals just by their gait, given that proper measurements of the observed motion patterns are available. For surveillance applications, gait is also attractive, because it does not require active collaboration from users and is hard to fake. However, the acquisition of good-quality measures of a person’s motion patterns in unconstrained environments, (e.g., in person re-identification applications) has proved very challenging in practice. Existing technology (video cameras) suffer from changes in viewpoint, daylight, clothing, accessories, and other variations in the person’s appearance. Novel three-dimensional sensors are bringing new promises to the field, but still many research issues are open. This article presents a survey of the work done in gait analysis for re-identification in the past decade, looking at the main approaches, datasets, and evaluation methodologies. We identify several relevant dimensions of the problem and provide a taxonomic analysis of the current state of the art. Finally, we discuss the levels of performance achievable with the current technology and give a perspective of the most challenging and promising directions of research for the future.},
	number = {2},
	urldate = {2021-05-22},
	journal = {ACM Computing Surveys},
	author = {Nambiar, Athira and Bernardino, Alexandre and Nascimento, Jacinto C.},
	month = apr,
	year = {2019},
	keywords = {person re-identification, Video surveillance, computer vision, biometrics, gait analysis, machine learning},
	pages = {33:1--33:34}
}

@inproceedings{zheng_mars_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{MARS}: {A} {Video} {Benchmark} for {Large}-{Scale} {Person} {Re}-{Identification}},
	isbn = {978-3-319-46466-4},
	shorttitle = {{MARS}},
	doi = {10.1007/978-3-319-46466-4_52},
	abstract = {This paper considers person re-identification (re-id) in videos. We introduce a new video re-id dataset, named Motion Analysis and Re-identification Set (MARS), a video extension of the Market-1501 dataset. To our knowledge, MARS is the largest video re-id dataset to date. Containing 1,261 IDs and around 20,000 tracklets, it provides rich visual information compared to image-based datasets. Meanwhile, MARS reaches a step closer to practice. The tracklets are automatically generated by the Deformable Part Model (DPM) as pedestrian detector and the GMMCP tracker. A number of false detection/tracking results are also included as distractors which would exist predominantly in practical video databases. Extensive evaluation of the state-of-the-art methods including the space-time descriptors and CNN is presented. We show that CNN in classification mode can be trained from scratch using the consecutive bounding boxes of each identity. The learned CNN embedding outperforms other competing methods considerably and has good generalization ability on other video re-id datasets upon fine-tuning.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Zheng, Liang and Bie, Zhi and Sun, Yifan and Wang, Jingdong and Su, Chi and Wang, Shengjin and Tian, Qi},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {CNN, Motion features, Video person re-identification},
	pages = {868--884},
	file = {Springer Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\2EA6BG38\\Zheng et al. - 2016 - MARS A Video Benchmark for Large-Scale Person Re-.pdf:application/pdf}
}

@inproceedings{ristani_performance_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Performance {Measures} and a {Data} {Set} for {Multi}-target, {Multi}-camera {Tracking}},
	isbn = {978-3-319-48881-3},
	doi = {10.1007/978-3-319-48881-3_2},
	abstract = {To help accelerate progress in multi-target, multi-camera tracking systems, we present (i) a new pair of precision-recall measures of performance that treats errors of all types uniformly and emphasizes correct identification over sources of error; (ii) the largest fully-annotated and calibrated data set to date with more than 2 million frames of 1080 p, 60 fps video taken by 8 cameras observing more than 2,700 identities over 85 min; and (iii) a reference software system as a comparison baseline. We show that (i) our measures properly account for bottom-line identity match performance in the multi-camera setting; (ii) our data set poses realistic challenges to current trackers; and (iii) the performance of our system is comparable to the state of the art.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Ristani, Ergys and Solera, Francesco and Zou, Roger and Cucchiara, Rita and Tomasi, Carlo},
	editor = {Hua, Gang and Jégou, Hervé},
	year = {2016},
	keywords = {Identity management, Large scale data set, Multi camera data set, Multi camera tracking, Performance evaluation},
	pages = {17--35},
	file = {Springer Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\W8AQWYDG\\Ristani et al. - 2016 - Performance Measures and a Data Set for Multi-targ.pdf:application/pdf}
}


@inproceedings{ess_depth_2007,
	title = {Depth and {Appearance} for {Mobile} {Scene} {Analysis}},
	doi = {10.1109/ICCV.2007.4409092},
	abstract = {In this paper, we address the challenging problem of simultaneous pedestrian detection and ground-plane estimation from video while walking through a busy pedestrian zone. Our proposed system integrates robust stereo depth cues, ground-plane estimation, and appearance-based object detection in a principled fashion using a graphical model. Object-object occlusions lead to complex interactions in this model that make an exact solution computationally intractable. We therefore propose a novel iterative approach that first infers scene geometry using belief propagation and then resolves interactions between objects using a global optimization procedure. This approach leads to a robust solution in few iterations, while allowing object detection to benefit from geometry estimation and vice versa. We quantitatively evaluate the performance of our proposed approach on several challenging test sequences showing strolls through busy shopping streets. Comparisons to various baseline systems show that it outperforms both a system using no scene geometry and one just relying on structure-from-motion without dense stereo.},
	booktitle = {2007 {IEEE} 11th {International} {Conference} on {Computer} {Vision}},
	author = {Ess, Andreas and Leibe, Bastian and Van Gool, Luc},
	month = oct,
	year = {2007},
	note = {ISSN: 2380-7504},
	keywords = {Belief propagation, Geometry, Graphical models, Image analysis, Iterative methods, Layout, Legged locomotion, Object detection, Robustness, Testing},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\GR25GGSN\\4409092.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\JIWNV7JB\\Ess et al. - 2007 - Depth and Appearance for Mobile Scene Analysis.pdf:application/pdf}
}

@inproceedings{zhuo_occluded_2018,
	title = {Occluded {Person} {Re}-{Identification}},
	doi = {10.1109/ICME.2018.8486568},
	abstract = {Person re-identification (re-id) suffers from a serious occlusion problem when applied to crowded public places. In this paper, we propose to retrieve a full-body person image by using a person image with occlusions. This differs significantly from the conventional person re-id problem where it is assumed that person images are detected without any occlusion. We thus call this new problem the occluded person re-identitification. To address this new problem, we propose a novel Attention Framework of Person Body (AFPB) based on deep learning, consisting of 1) an Occlusion Simulator (OS) which automatically generates artificial occlusions for full-body person images, and 2) multi-task losses that force the neural network not only to discriminate a person's identity but also to determine whether a sample is from the occluded data distribution or the full-body data distribution. Experiments on a new occluded person re-id dataset and three existing benchmarks modified to include full-body person images and occluded person images show the superiority of the proposed method.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	author = {Zhuo, Jiaxuan and Chen, Zeyu and Lai, Jianhuang and Wang, Guangcong},
	month = jul,
	year = {2018},
	note = {ISSN: 1945-788X},
	keywords = {Attention Framework of Person Body, Multi-task Losses, Occluded Person Re-identification, Occlusion Simulator},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\RRML72VY\\8486568.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\YJY7U4EZ\\Zhuo et al. - 2018 - Occluded Person Re-Identification.pdf:application/pdf}
}

@inproceedings{he_mask_2017,
	title = {Mask {R}-{CNN}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/He_Mask_R-CNN_ICCV_2017_paper.html},
	urldate = {2021-05-22},
	author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
	year = {2017},
	pages = {2961--2969},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\MDSV9CJQ\\He et al. - 2017 - Mask R-CNN.pdf:application/pdf}
}


@article{ma_orientation_2016,
	title = {Orientation {Driven} {Bag} of {Appearances} for {Person} {Re}-identification},
	url = {http://arxiv.org/abs/1605.02464},
	abstract = {Person re-identification (re-id) consists of associating individual across camera network, which is valuable for intelligent video surveillance and has drawn wide attention. Although person re-identification research is making progress, it still faces some challenges such as varying poses, illumination and viewpoints. For feature representation in re-identification, existing works usually use low-level descriptors which do not take full advantage of body structure information, resulting in low representation ability. \%discrimination. To solve this problem, this paper proposes the mid-level body-structure based feature representation (BSFR) which introduces body structure pyramid for codebook learning and feature pooling in the vertical direction of human body. Besides, varying viewpoints in the horizontal direction of human body usually causes the data missing problem, \$i.e.\$, the appearances obtained in different orientations of the identical person could vary significantly. To address this problem, the orientation driven bag of appearances (ODBoA) is proposed to utilize person orientation information extracted by orientation estimation technic. To properly evaluate the proposed approach, we introduce a new re-identification dataset (Market-1203) based on the Market-1501 dataset and propose a new re-identification dataset (PKU-Reid). Both datasets contain multiple images captured in different body orientations for each person. Experimental results on three public datasets and two proposed datasets demonstrate the superiority of the proposed approach, indicating the effectiveness of body structure and orientation information for improving re-identification performance.},
	urldate = {2021-05-22},
	journal = {arXiv:1605.02464 [cs]},
	author = {Ma, Liqian and Liu, Hong and Hu, Liang and Wang, Can and Sun, Qianru},
	month = may,
	year = {2016},
	note = {arXiv: 1605.02464},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\ZIF8KB38\\Ma et al. - 2016 - Orientation Driven Bag of Appearances for Person R.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\AJFWPUKG\\1605.html:text/html}
}

@article{lin_improving_2019,
	title = {Improving person re-identification by attribute and identity learning},
	volume = {95},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320319302377},
	doi = {10.1016/j.patcog.2019.06.006},
	abstract = {Person re-identification (re-ID) and attribute recognition share a common target at learning pedestrian descriptions. Their difference consists in the granularity. Most existing re-ID methods only take identity labels of pedestrians into consideration. However, we find the attributes, containing detailed local descriptions, are beneficial in allowing the re-ID model to learn more discriminative feature representations. In this paper, based on the complementarity of attribute labels and ID labels, we propose an attribute-person recognition (APR) network, a multi-task network which learns a re-ID embedding and at the same time predicts pedestrian attributes. We manually annotate attribute labels for two large-scale re-ID datasets, and systematically investigate how person re-ID and attribute recognition benefit from each other. In addition, we re-weight the attribute predictions considering the dependencies and correlations among the attributes. The experimental results on two large-scale re-ID benchmarks demonstrate that by learning a more discriminative representation, APR achieves competitive re-ID performance compared with the state-of-the-art methods. We use APR to speed up the retrieval process by ten times with a minor accuracy drop of 2.92\% on Market-1501. Besides, we also apply APR on the attribute recognition task and demonstrate improvement over the baselines.},
	language = {en},
	urldate = {2021-05-22},
	journal = {Pattern Recognition},
	author = {Lin, Yutian and Zheng, Liang and Zheng, Zhedong and Wu, Yu and Hu, Zhilan and Yan, Chenggang and Yang, Yi},
	month = nov,
	year = {2019},
	keywords = {Attribute recognition, Person re-identification},
	pages = {151--161},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\9NFLIYLZ\\Lin et al. - 2019 - Improving person re-identification by attribute an.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\55EQ2P89\\S0031320319302377.html:text/html}
}

@article{wang_weakly_2020,
	title = {Weakly {Supervised} {Person} {Re}-{ID}: {Differentiable} {Graphical} {Learning} and {A} {New} {Benchmark}},
	shorttitle = {Weakly {Supervised} {Person} {Re}-{ID}},
	url = {http://arxiv.org/abs/1904.03845},
	abstract = {Person re-identification (Re-ID) benefits greatly from the accurate annotations of existing datasets (e.g., CUHK03 [1] and Market-1501 [2]), which are quite expensive because each image in these datasets has to be assigned with a proper label. In this work, we ease the annotation of Re-ID by replacing the accurate annotation with inaccurate annotation, i.e., we group the images into bags in terms of time and assign a bag-level label for each bag. This greatly reduces the annotation effort and leads to the creation of a large-scale Re-ID benchmark called SYSU-30\$k\$. The new benchmark contains \$30k\$ individuals, which is about \$20\$ times larger than CUHK03 (\$1.3k\$ individuals) and Market-1501 (\$1.5k\$ individuals), and \$30\$ times larger than ImageNet (\$1k\$ categories). It sums up to 29,606,918 images. Learning a Re-ID model with bag-level annotation is called the weakly supervised Re-ID problem. To solve this problem, we introduce a differentiable graphical model to capture the dependencies from all images in a bag and generate a reliable pseudo label for each person image. The pseudo label is further used to supervise the learning of the Re-ID model. When compared with the fully supervised Re-ID models, our method achieves state-of-the-art performance on SYSU-30\$k\$ and other datasets. The code, dataset, and pretrained model will be available at {\textbackslash}url\{https://github.com/wanggrun/SYSU-30k\}.},
	urldate = {2021-05-22},
	journal = {arXiv:1904.03845 [cs]},
	author = {Wang, Guangrun and Wang, Guangcong and Zhang, Xujie and Lai, Jianhuang and Yu, Zhengtao and Lin, Liang},
	month = jul,
	year = {2020},
	note = {arXiv: 1904.03845},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\N8TIMDZL\\Wang et al. - 2020 - Weakly Supervised Person Re-ID Differentiable Gra.pdf:application/pdf}
}
@article{liao_open-set_2014,
	title = {Open-set {Person} {Re}-identification},
	url = {http://arxiv.org/abs/1408.0872},
	abstract = {Person re-identification is becoming a hot research for developing both machine learning algorithms and video surveillance applications. The task of person re-identification is to determine which person in a gallery has the same identity to a probe image. This task basically assumes that the subject of the probe image belongs to the gallery, that is, the gallery contains this person. However, in practical applications such as searching a suspect in a video, this assumption is usually not true. In this paper, we consider the open-set person re-identification problem, which includes two sub-tasks, detection and identification. The detection sub-task is to determine the presence of the probe subject in the gallery, and the identification sub-task is to determine which person in the gallery has the same identity as the accepted probe. We present a database collected from a video surveillance setting of 6 cameras, with 200 persons and 7,413 images segmented. Based on this database, we develop a benchmark protocol for evaluating the performance under the open-set person re-identification scenario. Several popular metric learning algorithms for person re-identification have been evaluated as baselines. From the baseline performance, we observe that the open-set person re-identification problem is still largely unresolved, thus further attention and effort is needed.},
	urldate = {2021-05-22},
	journal = {arXiv:1408.0872 [cs]},
	author = {Liao, Shengcai and Mo, Zhipeng and Zhu, Jianqing and Hu, Yang and Li, Stan Z.},
	month = oct,
	year = {2014},
	note = {arXiv: 1408.0872},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\3HD2HEXS\\Liao et al. - 2014 - Open-set Person Re-identification.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\U5D8YLCP\\1408.html:text/html}
}
@article{kumar_p-destre_2021,
	title = {The {P}-{DESTRE}: {A} {Fully} {Annotated} {Dataset} for {Pedestrian} {Detection}, {Tracking}, and {Short}/{Long}-{Term} {Re}-{Identification} {From} {Aerial} {Devices}},
	volume = {16},
	issn = {1556-6021},
	shorttitle = {The {P}-{DESTRE}},
	doi = {10.1109/TIFS.2020.3040881},
	abstract = {Over the years, unmanned aerial vehicles (UAVs) have been regarded as a potential solution to surveil public spaces, providing a cheap way for data collection, while covering large and difficult-to-reach areas. This kind of solutions can be particularly useful to detect, track and identify subjects of interest in crowds, for security/safety purposes. In this context, various datasets are publicly available, yet most of them are only suitable for evaluating detection, tracking and short-term re-identification techniques. This paper announces the free availability of the P-DESTRE dataset, the first of its kind to provide video/UAV-based data for pedestrian long-term re-identification research, with ID annotations consistent across data collected in different days. As a secondary contribution, we provide the results attained by the state-of-the-art pedestrian detection, tracking, short/long term re-identification techniques in well-known surveillance datasets, used as baselines for the corresponding effectiveness observed in the P-DESTRE data. This comparison highlights the discriminating characteristics of P-DESTRE with respect to similar sets. Finally, we identify the most problematic data degradation factors and co-variates for UAV-based automated data analysis, which should be considered in subsequent technologic/conceptual advances in this field. The dataset and the full specification of the empirical evaluation carried out are freely available at http://p-destre.di.ubi.pt/.},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Kumar, S. V. Aruna and Yaghoubi, Ehsan and Das, Abhijit and Harish, B. S. and Proença, Hugo},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Information Forensics and Security},
	keywords = {aerial data, Cameras, Drones, Head, Object detection, object tracking, pedestrian detection, pedestrian re-identification, pedestrian search, Search problems, Surveillance, TV, Visual surveillance},
	pages = {1696--1708},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\ISGNSQHV\\9272305.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\6V4HG8FC\\Kumar et al. - 2021 - The P-DESTRE A Fully Annotated Dataset for Pedest.pdf:application/pdf}
}
@inproceedings{xiao_joint_2017,
	address = {Honolulu, HI},
	title = {Joint {Detection} and {Identification} {Feature} {Learning} for {Person} {Search}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099843/},
	doi = {10.1109/CVPR.2017.360},
	abstract = {Existing person re-identiﬁcation benchmarks and methods mainly focus on matching cropped pedestrian images between queries and candidates. However, it is different from real-world scenarios where the annotations of pedestrian bounding boxes are unavailable and the target person needs to be searched from a gallery of whole scene images. To close the gap, we propose a new deep learning framework for person search. Instead of breaking it down into two separate tasks—pedestrian detection and person re-identiﬁcation, we jointly handle both aspects in a single convolutional neural network. An Online Instance Matching (OIM) loss function is proposed to train the network effectively, which is scalable to datasets with numerous identities. To validate our approach, we collect and annotate a large-scale benchmark dataset for person search. It contains 18, 184 images, 8, 432 identities, and 96, 143 pedestrian bounding boxes. Experiments show that our framework outperforms other separate approaches, and the proposed OIM loss function converges much faster and better than the conventional Softmax loss.},
	language = {en},
	urldate = {2021-05-23},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Xiao, Tong and Li, Shuang and Wang, Bochao and Lin, Liang and Wang, Xiaogang},
	month = jul,
	year = {2017},
	pages = {3376--3385},
	file = {Xiao et al. - 2017 - Joint Detection and Identification Feature Learnin.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\UQC3WVW9\\Xiao et al. - 2017 - Joint Detection and Identification Feature Learnin.pdf:application/pdf}
}

@article{kviatkovsky_color_2013,
	title = {Color {Invariants} for {Person} {Reidentification}},
	volume = {35},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2012.246},
	abstract = {We revisit the problem of specific object recognition using color distributions. In some applications-such as specific person identification-it is highly likely that the color distributions will be multimodal and hence contain a special structure. Although the color distribution changes under different lighting conditions, some aspects of its structure turn out to be invariants. We refer to this structure as an intradistribution structure, and show that it is invariant under a wide range of imaging conditions while being discriminative enough to be practical. Our signature uses shape context descriptors to represent the intradistribution structure. Assuming the widely used diagonal model, we validate that our signature is invariant under certain illumination changes. Experimentally, we use color information as the only cue to obtain good recognition performance on publicly available databases covering both indoor and outdoor conditions. Combining our approach with the complementary covariance descriptor, we demonstrate results exceeding the state-of-the-art performance on the challenging VIPeR and CAVIAR4REID databases.},
	number = {7},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kviatkovsky, Igor and Adam, Amit and Rivlin, Ehud},
	month = jul,
	year = {2013},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Cameras, Color, color invariant signatures, Context, Image color analysis, Lighting, person reidentification, Shape, Surveillance applications},
	pages = {1622--1634},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\I4TDBYZ6\\6357194.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\SGWATFTR\\Kviatkovsky et al. - 2013 - Color Invariants for Person Reidentification.pdf:application/pdf}
}


@article{chen_multi-task_2017,
	title = {A {Multi}-{Task} {Deep} {Network} for {Person} {Re}-{Identification}},
	volume = {31},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11201},
	language = {en},
	number = {1},
	urldate = {2021-05-29},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Weihua and Chen, Xiaotang and Zhang, Jianguo and Huang, Kaiqi},
	month = feb,
	year = {2017},
	note = {Number: 1},
	keywords = {Deep learning},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\AA62IFJE\\Chen et al. - 2017 - A Multi-Task Deep Network for Person Re-Identifica.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\9BSZS4QH\\11201.html:text/html}
}

@article{gray_evaluating_2007,
	title = {Evaluating {Appearance} {Models} for {Recognition}, {Reacquisition}, and {Tracking}},
	abstract = {Traditionally, appearance models for recognition, reacquisition and tracking problems have been evaluated independently using metrics applied to a complete system. It is shown that appearance models for these three problems can be evaluated using a cumulative matching curve on a standardized dataset, and that this one curve can be converted to a synthetic disambiguation rate for single camera tracking or a synthetic reacquisition rate for cross camera tracking. A challenging new dataset for viewpoint invariant pedestrian recognition (VIPeR) is provided as an example. This dataset contains 632 pedestrian image pairs from arbitrary viewpoints. Several baseline methods are tested on this dataset and the results are presented as a benchmark for future appearance models and matching methods.},
	language = {en},
	author = {Gray, Doug and Brennan, Shane and Tao, Hai},
	year = {2007},
	pages = {7},
	file = {Gray et al. - Evaluating Appearance Models for Recognition, Reac.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\ER8QXVGM\\Gray et al. - Evaluating Appearance Models for Recognition, Reac.pdf:application/pdf}
}

@inproceedings{han_prediction_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Prediction and {Recovery} for {Adaptive} {Low}-{Resolution} {Person} {Re}-{Identification}},
	isbn = {978-3-030-58574-7},
	doi = {10.1007/978-3-030-58574-7_12},
	abstract = {Low-resolution person re-identification (LR re-id) is a challenging task with low-resolution probes and high-resolution gallery images. To address the resolution mismatch, existing methods typically recover missing details for low-resolution probes by super-resolution. However, they usually pre-specify fixed scale factors for all images, and ignore the fact that choosing a preferable scale factor for certain image content probably greatly benefits the identification. In this paper, we propose a novel Prediction, Recovery and Identification (PRI) model for LR re-id, which adaptively recovers missing details by predicting a preferable scale factor based on the image content. To deal with the lack of ground-truth optimal scale factors, our model contains a self-supervised scale factor metric that automatically generates dynamic soft labels. The generated labels indicate probabilities that each scale factor is optimal, which are used as guidance to enhance the content-aware scale factor prediction. Consequently, our model can more accurately predict and recover the content-aware details, and achieve state-of-the-art performances on four LR re-id datasets.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Han, Ke and Huang, Yan and Chen, Zerui and Wang, Liang and Tan, Tieniu},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {Adaptive scale factor prediction, Dynamic soft label, Low-resolution person re-identification},
	pages = {193--209},
	file = {Springer Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\EY8DQKBU\\Han et al. - 2020 - Prediction and Recovery for Adaptive Low-Resolutio.pdf:application/pdf}
}

@article{jiao_deep_2018,
	title = {Deep {Low}-{Resolution} {Person} {Re}-{Identification}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/12284},
	language = {en},
	number = {1},
	urldate = {2021-05-29},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Jiao, Jiening and Zheng, Wei-Shi and Wu, Ancong and Zhu, Xiatian and Gong, Shaogang},
	month = apr,
	year = {2018},
	note = {Number: 1},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\A2B2QFHE\\Jiao et al. - 2018 - Deep Low-Resolution Person Re-Identification.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\VSBXYDNI\\12284.html:text/html}
}

@article{li_discriminative_2018,
	title = {Discriminative {Semi}-{Coupled} {Projective} {Dictionary} {Learning} for {Low}-{Resolution} {Person} {Re}-{Identification}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11908},
	language = {en},
	number = {1},
	urldate = {2021-05-29},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Li, Kai and Ding, Zhengming and Li, Sheng and Fu, Yun},
	month = apr,
	year = {2018},
	note = {Number: 1},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\5SGFNGFP\\Li et al. - 2018 - Discriminative Semi-Coupled Projective Dictionary .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\CB7M83XU\\11908.html:text/html}
}

@article{liu_semi-supervised_2018,
	title = {Semi-{Supervised} {Bayesian} {Attribute} {Learning} for {Person} {Re}-{Identification}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/12339},
	language = {en},
	number = {1},
	urldate = {2021-05-29},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Liu, Wenhe and Chang, Xiaojun and Chen, Ling and Yang, Yi},
	month = apr,
	year = {2018},
	note = {Number: 1},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\9WR3JHP5\\Liu et al. - 2018 - Semi-Supervised Bayesian Attribute Learning for Pe.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\SZ9DVQGT\\12339.html:text/html}
}

@article{mao_multi-channel_2018,
	title = {Multi-{Channel} {Pyramid} {Person} {Matching} {Network} for {Person} {Re}-{Identification}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/12225},
	language = {en},
	number = {1},
	urldate = {2021-05-29},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Mao, Chaojie and Li, Yingming and Zhang, Yaqing and Zhang, Zhongfei and Li, Xi},
	month = apr,
	year = {2018},
	note = {Number: 1},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\C37E772Q\\Mao et al. - 2018 - Multi-Channel Pyramid Person Matching Network for .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\NLFZT996\\12225.html:text/html}
}

@inproceedings{prosser_person_2010,
	address = {Aberystwyth},
	title = {Person {Re}-{Identification} by {Support} {Vector} {Ranking}},
	isbn = {978-1-901725-40-7},
	url = {http://www.bmva.org/bmvc/2010/conference/paper21/index.html},
	doi = {10.5244/C.24.21},
	abstract = {Solving the person re-identiﬁcation problem involves matching observations of individuals across disjoint camera views. The problem becomes particularly hard in a busy public scene as the number of possible matches is very high. This is further compounded by signiﬁcant appearance changes due to varying lighting conditions, viewing angles and body poses across camera views. To address this problem, existing approaches focus on extracting or learning discriminative features followed by template matching using a distance measure. The novelty of this work is that we reformulate the person reidentiﬁcation problem as a ranking problem and learn a subspace where the potential true match is given highest ranking rather than any direct distance measure. By doing so, we convert the person re-identiﬁcation problem from an absolute scoring problem to a relative ranking problem. We further develop an novel Ensemble RankSVM to overcome the scalability limitation problem suffered by existing SVM-based ranking methods. This new model reduces signiﬁcantly memory usage therefore is much more scalable, whilst maintaining high-level performance. We present extensive experiments to demonstrate the performance gain of the proposed ranking approach over existing template matching and classiﬁcation models.},
	language = {en},
	urldate = {2021-05-29},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2010},
	publisher = {British Machine Vision Association},
	author = {Prosser, Bryan and Zheng, Wei-Shi and Gong, Shaogang and Xiang, Tao},
	year = {2010},
	pages = {21.1--21.11},
	file = {Prosser et al. - 2010 - Person Re-Identification by Support Vector Ranking.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\R9IVGNS2\\Prosser et al. - 2010 - Person Re-Identification by Support Vector Ranking.pdf:application/pdf}
}

@article{srivastava_dropout_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overﬁtting}},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overﬁtting is a serious problem in such networks. Large networks are also slow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many diﬀerent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of diﬀerent “thinned” networks. At test time, it is easy to approximate the eﬀect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signiﬁcantly reduces overﬁtting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classiﬁcation and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	language = {en},
	author = {Srivastava, Nitish and Hinton, Geoﬀrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {30},
	file = {Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\DVXEIZFF\\Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf}
}

@article{wei_swiss-system_2015,
	title = {Swiss-{System} {Based} {Cascade} {Ranking} for {Gait}-{Based} {Person} {Re}-{Identification}},
	volume = {29},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/9454},
	language = {en},
	number = {1},
	urldate = {2021-05-29},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wei, Lan and Tian, Yonghong and Wang, Yaowei and Huang, Tiejun},
	month = feb,
	year = {2015},
	note = {Number: 1},
	keywords = {Swiss system},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\ZFBDH9AR\\Wei et al. - 2015 - Swiss-System Based Cascade Ranking for Gait-Based .pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\8IMCMVHY\\9454.html:text/html}
}

@article{wu_temporal-enhanced_2018,
	title = {Temporal-{Enhanced} {Convolutional} {Network} for {Person} {Re}-{Identification}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/12264},
	language = {en},
	number = {1},
	urldate = {2021-05-29},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wu, Yang and Qiu, Jie and Takamatsu, Jun and Ogasawara, Tsukasa},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {Person Re-Identification},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\JFW7NAQG\\Wu et al. - 2018 - Temporal-Enhanced Convolutional Network for Person.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\XIPVV28P\\12264.html:text/html}
}

@article{yang_metric_2016,
	title = {Metric {Embedded} {Discriminative} {Vocabulary} {Learning} for {High}-{Level} {Person} {Representation}},
	volume = {30},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/10461},
	language = {en},
	number = {1},
	urldate = {2021-05-29},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Yang, Yang and Lei, Zhen and Zhang, Shifeng and Shi, Hailin and Li, Stan},
	month = mar,
	year = {2016},
	note = {Number: 1},
	keywords = {High-Level features},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\B7X7NZQG\\Yang et al. - 2016 - Metric Embedded Discriminative Vocabulary Learning.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\NQAWCBIL\\10461.html:text/html}
}

@article{ye_hierarchical_2018,
	title = {Hierarchical {Discriminative} {Learning} for {Visible} {Thermal} {Person} {Re}-{Identification}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/12293},
	language = {en},
	number = {1},
	urldate = {2021-05-29},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ye, Mang and Lan, Xiangyuan and Li, Jiawei and Yuen, Pong},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {Cross Modality},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\3PRR6KDN\\Ye et al. - 2018 - Hierarchical Discriminative Learning for Visible T.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\37HEY7EI\\12293.html:text/html}
}

@inproceedings{zheng_associating_2009,
	address = {London},
	title = {Associating {Groups} of {People}},
	isbn = {978-1-901725-39-1},
	url = {http://www.bmva.org/bmvc/2009/Papers/Paper167/Paper167.html},
	doi = {10.5244/C.23.23},
	language = {en},
	urldate = {2021-05-29},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2009},
	publisher = {British Machine Vision Association},
	author = {Zheng, Wei-Shi and Gong, Shaogang and Xiang, Tao},
	year = {2009},
	pages = {23.1--23.11},
	file = {Zheng et al. - 2009 - Associating Groups of People.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\GZB3TAEC\\Zheng et al. - 2009 - Associating Groups of People.pdf:application/pdf}
}

@inproceedings{zhou_attention-based_2017,
	address = {Honolulu, HI, USA},
	title = {Attention-{Based} {Natural} {Language} {Person} {Retrieval}},
	isbn = {978-1-5386-0733-6},
	url = {http://ieeexplore.ieee.org/document/8014744/},
	doi = {10.1109/CVPRW.2017.10},
	abstract = {Following the recent progress in image classiﬁcation and captioning using deep learning, we develop a novel natural language person retrieval system based on an attention mechanism. More speciﬁcally, given the description of a person, the goal is to localize the person in an image. To this end, we ﬁrst construct a benchmark dataset for natural language person retrieval. To do so, we generate bounding boxes for persons in a public image dataset from the segmentation masks, which are then annotated with descriptions and attributes using the Amazon Mechanical Turk. We then adopt a region proposal network in Faster R-CNN as a candidate region generator. The cropped images based on the region proposals as well as the whole images with attention weights are fed into Convolutional Neural Networks for visual feature extraction, while the natural language expression and attributes are input to Bidirectional Long ShortTerm Memory (BLSTM) models for text feature extraction. The visual and text features are integrated to score region proposals, and the one with the highest score is retrieved as the output of our system. The experimental results show signiﬁcant improvement over the state-of-the-art method for generic object retrieval and this line of research promises to beneﬁt search in surveillance video footage.},
	language = {en},
	urldate = {2021-05-29},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Zhou, Tao and Chen, Muhao and Yu, Jie and Terzopoulos, Demetri},
	month = jul,
	year = {2017},
	pages = {27--34},
	file = {Zhou et al. - 2017 - Attention-Based Natural Language Person Retrieval.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\GZ3R5XEZ\\Zhou et al. - 2017 - Attention-Based Natural Language Person Retrieval.pdf:application/pdf}
}

@article{zhou_graph_2018,
	title = {Graph {Correspondence} {Transfer} for {Person} {Re}-{Identification}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/12241},
	language = {en},
	number = {1},
	urldate = {2021-05-29},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhou, Qin and Fan, Heng and Zheng, Shibao and Su, Hang and Li, Xinzhe and Wu, Shuang and Ling, Haibin},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {graph matching},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\2JAJ5UMQ\\Zhou et al. - 2018 - Graph Correspondence Transfer for Person Re-Identi.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\3SYT23WV\\12241.html:text/html}
}

@article{fan_unsupervised_2018,
	title = {Unsupervised {Person} {Re}-identification: {Clustering} and {Fine}-tuning},
	volume = {14},
	issn = {1551-6857},
	shorttitle = {Unsupervised {Person} {Re}-identification},
	url = {https://doi.org/10.1145/3243316},
	doi = {10.1145/3243316},
	abstract = {The superiority of deeply learned pedestrian representations has been reported in very recent literature of person re-identification (re-ID). In this article, we consider the more pragmatic issue of learning a deep feature with no or only a few labels. We propose a progressive unsupervised learning (PUL) method to transfer pretrained deep representations to unseen domains. Our method is easy to implement and can be viewed as an effective baseline for unsupervised re-ID feature learning. Specifically, PUL iterates between (1) pedestrian clustering and (2) fine-tuning of the convolutional neural network (CNN) to improve the initialization model trained on the irrelevant labeled dataset. Since the clustering results can be very noisy, we add a selection operation between the clustering and fine-tuning. At the beginning, when the model is weak, CNN is fine-tuned on a small amount of reliable examples that locate near to cluster centroids in the feature space. As the model becomes stronger, in subsequent iterations, more images are being adaptively selected as CNN training samples. Progressively, pedestrian clustering and the CNN model are improved simultaneously until algorithm convergence. This process is naturally formulated as self-paced learning. We then point out promising directions that may lead to further improvement. Extensive experiments on three large-scale re-ID datasets demonstrate that PUL outputs discriminative features that improve the re-ID accuracy. Our code has been released at https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning.},
	number = {4},
	urldate = {2021-06-24},
	journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},
	author = {Fan, Hehe and Zheng, Liang and Yan, Chenggang and Yang, Yi},
	month = oct,
	year = {2018},
	keywords = {clustering, convolutional neural network, Large-scale person re-identification, unsupervised learning},
	pages = {83:1--83:18},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\MUUIIFH3\\Fan et al. - 2018 - Unsupervised Person Re-identification Clustering .pdf:application/pdf}
}

@article{lin_multi-task_2018,
	title = {Multi-task {Mid}-level {Feature} {Alignment} {Network} for {Unsupervised} {Cross}-{Dataset} {Person} {Re}-{Identiﬁcation}},
	abstract = {Most existing person re-identiﬁcation (Re-ID) approaches follow a supervised learning framework, in which a large number of labelled matching pairs are required for training. Such a setting severely limits their scalability in real-world applications where no labelled samples are available during the training phase. To overcome this limitation, we develop a novel unsupervised Multi-task Mid-level Feature Alignment (MMFA) network for the unsupervised cross-dataset person re-identiﬁcation task. Under the assumption that the source and target datasets share the same set of mid-level semantic attributes, our proposed model can be jointly optimised under the person’s identity classiﬁcation and the attribute learning task with a cross-dataset mid-level feature alignment regularisation term. In this way, the learned feature representation can be better generalised from one dataset to another which further improve the person re-identiﬁcation accuracy. Experimental results on four benchmark datasets demonstrate that our proposed method outperforms the state-of-the-art baselines.},
	language = {en},
	author = {Lin, Shan},
	year = {2018},
	pages = {13},
	file = {Lin - Multi-task Mid-level Feature Alignment Network for.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\KQW35XEB\\Lin - Multi-task Mid-level Feature Alignment Network for.pdf:application/pdf}
}
@article{chen_deep_2018,
	title = {Deep {Association} {Learning} for {Unsupervised} {Video} {Person} {Re}-identification},
	url = {http://arxiv.org/abs/1808.07301},
	abstract = {Deep learning methods have started to dominate the research progress of video-based person re-identification (re-id). However, existing methods mostly consider supervised learning, which requires exhaustive manual efforts for labelling cross-view pairwise data. Therefore, they severely lack scalability and practicality in real-world video surveillance applications. In this work, to address the video person re-id task, we formulate a novel Deep Association Learning (DAL) scheme, the first end-to-end deep learning method using none of the identity labels in model initialisation and training. DAL learns a deep re-id matching model by jointly optimising two margin-based association losses in an end-to-end manner, which effectively constrains the association of each frame to the best-matched intra-camera representation and cross-camera representation. Existing standard CNNs can be readily employed within our DAL scheme. Experiment results demonstrate that our proposed DAL significantly outperforms current state-of-the-art unsupervised video person re-id methods on three benchmarks: PRID 2011, iLIDS-VID and MARS.},
	urldate = {2021-06-27},
	journal = {arXiv:1808.07301 [cs]},
	author = {Chen, Yanbei and Zhu, Xiatian and Gong, Shaogang},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.07301},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\WKZ2QRVN\\Chen et al. - 2018 - Deep Association Learning for Unsupervised Video P.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\S5EJ4C83\\1808.html:text/html}
}

@article{dosovitskiy_discriminative_nodate,
	title = {Discriminative {Unsupervised} {Feature} {Learning} with {Convolutional} {Neural} {Networks}},
	abstract = {Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled ’seed’ image patch. We ﬁnd that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classiﬁcation results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101).},
	language = {en},
	author = {Dosovitskiy, Alexey and Springenberg, Jost Tobias and Riedmiller, Martin and Brox, Thomas},
	pages = {9},
	file = {Dosovitskiy et al. - Discriminative Unsupervised Feature Learning with .pdf:C\:\\Users\\lxt76\\Zotero\\storage\\KPCAGNAF\\Dosovitskiy et al. - Discriminative Unsupervised Feature Learning with .pdf:application/pdf},
}

@inproceedings{mathieu_disentangling_2016,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'16},
	title = {Disentangling factors of variation in deep representations using adversarial training},
	isbn = {978-1-5108-3881-9},
	abstract = {We introduce a conditional generative model for learning to disentangle the hidden factors of variation within a set of labeled observations, and separate them into complementary codes. One code summarizes the specified factors of variation associated with the labels. The other summarizes the remaining unspecified variability. During training, the only available source of supervision comes from our ability to distinguish among different observations belonging to the same class. Examples of such observations include images of a set of labeled objects captured at different viewpoints, or recordings of set of speakers dictating multiple phrases. In both instances, the intra-class diversity is the source of the unspecified factors of variation: each object is observed at multiple viewpoints, and each speaker dictates multiple phrases. Learning to disentangle the specified factors from the unspecified ones becomes easier when strong supervision is possible. Suppose that during training, we have access to pairs of images, where each pair shows two different objects captured from the same viewpoint. This source of alignment allows us to solve our task using existing methods. However, labels for the unspecified factors are usually unavailable in realistic scenarios where data acquisition is not strictly controlled. We address the problem of disentaglement in this more general setting by combining deep convolutional autoencoders with a form of adversarial training. Both factors of variation are implicitly captured in the organization of the learned embedding space, and can be used for solving single-image analogies. Experimental results on synthetic and real datasets show that the proposed method is capable of generalizing to unseen classes and intra-class variabilities.},
	urldate = {2021-07-07},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Mathieu, Michael and Zhao, Junbo and Sprechmann, Pablo and Ramesh, Aditya and LeCun, Yann},
	month = dec,
	year = {2016},
	pages = {5047--5055},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\DLW6CQPS\\Mathieu et al. - 2016 - Disentangling factors of variation in deep represe.pdf:application/pdf},
}

@article{hoffman_cycada_2018,
	title = {{CyCADA}: {Cycle}-{Consistent} {Adversarial} {Domain} {Adaptation}},
	abstract = {Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models have shown tremendous progress towards adapting to new environments by focusing either on discovering domain invariant representations or by mapping between unpaired image domains. While feature space methods are difﬁcult to interpret and sometimes fail to capture pixel-level and low-level domain shifts, image space methods sometimes fail to incorporate high level semantic knowledge relevant for the end task. We propose a model which adapts between domains using both generative image space alignment and latent representation space alignment. Our approach, Cycle-Consistent Adversarial Domain Adaptation (CyCADA), guides transfer between domains according to a speciﬁc discriminatively trained task and avoids divergence by enforcing consistency of the relevant semantics before and after adaptation. We evaluate our method on a variety of visual recognition and prediction settings, including digit classiﬁcation and semantic segmentation of road scenes, advancing state-of-the-art performance for unsupervised adaptation from synthetic to real world driving domains.},
	language = {en},
	author = {Hoffman, Judy and Tzeng, Eric and Park, Taesung and Zhu, Jun-Yan and Isola, Phillip and Saenko, Kate and Efros, Alexei A and Darrell, Trevor},
	pages = {10},
	year = {2018},
	file = {Hoffman et al. - CyCADA Cycle-Consistent Adversarial Domain Adapta.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\INR393T2\\Hoffman et al. - CyCADA Cycle-Consistent Adversarial Domain Adapta.pdf:application/pdf},
}

@article{song_unsupervised_2018,
	title = {Unsupervised {Domain} {Adaptive} {Re}-{Identification}: {Theory} and {Practice}},
	shorttitle = {Unsupervised {Domain} {Adaptive} {Re}-{Identification}},
	url = {http://arxiv.org/abs/1807.11334},
	abstract = {We study the problem of unsupervised domain adaptive re-identification (re-ID) which is an active topic in computer vision but lacks a theoretical foundation. We first extend existing unsupervised domain adaptive classification theories to re-ID tasks. Concretely, we introduce some assumptions on the extracted feature space and then derive several loss functions guided by these assumptions. To optimize them, a novel self-training scheme for unsupervised domain adaptive re-ID tasks is proposed. It iteratively makes guesses for unlabeled target data based on an encoder and trains the encoder based on the guessed labels. Extensive experiments on unsupervised domain adaptive person re-ID and vehicle re-ID tasks with comparisons to the state-of-the-arts confirm the effectiveness of the proposed theories and self-training framework. Our code is available at {\textbackslash}url\{https://github.com/LcDog/DomainAdaptiveReID\}.},
	urldate = {2021-07-08},
	journal = {arXiv:1807.11334 [cs]},
	author = {Song, Liangchen and Wang, Cheng and Zhang, Lefei and Du, Bo and Zhang, Qian and Huang, Chang and Wang, Xinggang},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.11334},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\SQBKQ44J\\Song et al. - 2018 - Unsupervised Domain Adaptive Re-Identification Th.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\ZJRUISA2\\1807.html:text/html},
}

@inproceedings{choi_stargan_2018,
	address = {Salt Lake City, UT},
	title = {{StarGAN}: {Unified} {Generative} {Adversarial} {Networks} for {Multi}-domain {Image}-to-{Image} {Translation}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{StarGAN}},
	url = {https://ieeexplore.ieee.org/document/8579014/},
	doi = {10.1109/CVPR.2018.00916},
	language = {en},
	urldate = {2021-07-09},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
	month = jun,
	year = {2018},
	pages = {8789--8797},
	file = {Choi et al. - 2018 - StarGAN Unified Generative Adversarial Networks f.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\E9TEPJ9R\\Choi et al. - 2018 - StarGAN Unified Generative Adversarial Networks f.pdf:application/pdf},
}

@inproceedings{zhang_mixup_2018,
	title = {mixup: {Beyond} {Empirical} {Risk} {Minimization}},
	shorttitle = {mixup},
	url = {https://openreview.net/forum?id=r1Ddp1-Rb},
	abstract = {Training on convex combinations between random training examples and their labels improves generalization in deep neural networks},
	language = {en},
	urldate = {2021-07-09},
	author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
	month = feb,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\XEC4FRCZ\\Zhang et al. - 2018 - mixup Beyond Empirical Risk Minimization.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\BYE83GS5\\forum.html:text/html},
}

@inproceedings{kodirov_dictionary_2015,
	address = {Swansea},
	title = {Dictionary {Learning} with {Iterative} {Laplacian} {Regularisation} for {Unsupervised} {Person} {Re}-identification},
	isbn = {978-1-901725-53-7},
	url = {http://www.bmva.org/bmvc/2015/papers/paper044/index.html},
	doi = {10.5244/C.29.44},
	abstract = {Many existing approaches to person re-identiﬁcation (Re-ID) are based on supervised learning, which requires hundreds of matching pairs to be labelled for each pair of cameras. This severely limits their scalability for real-world applications. This work aims to overcome this limitation by developing a novel unsupervised Re-ID approach. The approach is based on a new dictionary learning for sparse coding formulation with a graph Laplacian regularisation term whose value is set iteratively. As an unsupervised model, the dictionary learning model is well-suited to the unsupervised task, whilst the regularisation term enables the exploitation of cross-view identity-discriminative information ignored by existing unsupervised Re-ID methods. Importantly this model is also ﬂexible in utilising any labelled data if available. Experiments on two benchmark datasets demonstrate that the proposed approach signiﬁcantly outperforms the state-of-the-arts.},
	language = {en},
	urldate = {2021-07-09},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2015},
	publisher = {British Machine Vision Association},
	author = {Kodirov, Elyor and Xiang, Tao and Gong, Shaogang},
	year = {2015},
	pages = {44.1--44.12},
	file = {Kodirov et al. - 2015 - Dictionary Learning with Iterative Laplacian Regul.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\NFQQZQ4C\\Kodirov et al. - 2015 - Dictionary Learning with Iterative Laplacian Regul.pdf:application/pdf},
}

@article{li_adaptation_2018,
	title = {Adaptation and {Re}-{Identification} {Network}: {An} {Unsupervised} {Deep} {Transfer} {Learning} {Approach} to {Person} {Re}-{Identification}},
	shorttitle = {Adaptation and {Re}-{Identification} {Network}},
	url = {http://arxiv.org/abs/1804.09347},
	abstract = {Person re-identification (Re-ID) aims at recognizing the same person from images taken across different cameras. To address this task, one typically requires a large amount labeled data for training an effective Re-ID model, which might not be practical for real-world applications. To alleviate this limitation, we choose to exploit a sufficient amount of pre-existing labeled data from a different (auxiliary) dataset. By jointly considering such an auxiliary dataset and the dataset of interest (but without label information), our proposed adaptation and re-identification network (ARN) performs unsupervised domain adaptation, which leverages information across datasets and derives domain-invariant features for Re-ID purposes. In our experiments, we verify that our network performs favorably against state-of-the-art unsupervised Re-ID approaches, and even outperforms a number of baseline Re-ID methods which require fully supervised data for training.},
	urldate = {2021-07-09},
	journal = {arXiv:1804.09347 [cs]},
	author = {Li, Yu-Jhe and Yang, Fu-En and Liu, Yen-Cheng and Yeh, Yu-Ying and Du, Xiaofei and Wang, Yu-Chiang Frank},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.09347},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\XSW46FD4\\Li et al. - 2018 - Adaptation and Re-Identification Network An Unsup.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\AGVWJLHU\\1804.html:text/html},
}

@article{zhong_camstyle_2019,
	title = {{CamStyle}: {A} {Novel} {Data} {Augmentation} {Method} for {Person} {Re}-{Identification}},
	volume = {28},
	issn = {1941-0042},
	shorttitle = {{CamStyle}},
	doi = {10.1109/TIP.2018.2874313},
	abstract = {Person re-identification (re-ID) is a cross-camera retrieval task that suffers from image style variations caused by different cameras. The art implicitly addresses this problem by learning a camera-invariant descriptor subspace. In this paper, we explicitly consider this challenge by introducing camera style (CamStyle). CamStyle can serve as a data augmentation approach that reduces the risk of deep network overfitting and that smooths the CamStyle disparities. Specifically, with a style transfer model, labeled training images can be style transferred to each camera, and along with the original training samples, form the augmented training set. This method, while increasing data diversity against overfitting, also incurs a considerable level of noise. In the effort to alleviate the impact of noise, the label smooth regularization (LSR) is adopted. The vanilla version of our method (without LSR) performs reasonably well on few camera systems in which overfitting often occurs. With LSR, we demonstrate consistent improvement in all systems regardless of the extent of overfitting. We also report competitive accuracy compared with the state of the art on Market-1501 and DukeMTMC-re-ID. Importantly, CamStyle can be employed to the challenging problems of one view learning and unsupervised domain adaptation (UDA) in person re-identification (re-ID), both of which have critical research and application significance. The former only has labeled data in one camera view and the latter only has labeled data in the source domain. Experimental results show that CamStyle significantly improves the performance of the baseline in the two problems. Specially, for UDA, CamStyle achieves state-of-the-art accuracy based on a baseline deep re-ID model on Market-1501 and DukeMTMC-reID. Our code is available at: https://github.com/zhunzhong07/CamStyle.},
	number = {3},
	journal = {IEEE Transactions on Image Processing},
	author = {Zhong, Zhun and Zheng, Liang and Zheng, Zhedong and Li, Shaozi and Yang, Yi},
	month = mar,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Adaptation models, Australia, Cameras, CamStyle, Data models, Machine learning, one-view learning, Person re-identification, Task analysis, Training, unsupervised domain adaptation},
	pages = {1176--1190},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\7AUANFMU\\8485427.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\U9ESUUHH\\Zhong et al. - 2019 - CamStyle A Novel Data Augmentation Method for Per.pdf:application/pdf},
}

@article{ding_towards_2019,
	title = {Towards better {Validity}: {Dispersion} based {Clustering} for {Unsupervised} {Person} {Re}-identification},
	shorttitle = {Towards better {Validity}},
	url = {http://arxiv.org/abs/1906.01308},
	abstract = {Person re-identiﬁcation aims to establish the correct identity correspondences of a person moving through a nonoverlapping multi-camera installation. Recent advances based on deep learning models for this task mainly focus on supervised learning scenarios where accurate annotations are assumed to be available for each setup. Annotating large scale datasets for person re-identiﬁcation is demanding and burdensome, which renders the deployment of such supervised approaches to realworld applications infeasible. Therefore, it is necessary to train models without explicit supervision in an autonomous manner.},
	language = {en},
	urldate = {2021-07-10},
	journal = {arXiv:1906.01308 [cs]},
	author = {Ding, Guodong and Khan, Salman and Tang, Zhenmin and Zhang, Jian and Porikli, Fatih},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.01308},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Ding et al. - 2019 - Towards better Validity Dispersion based Clusteri.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\C3WWZZ4S\\Ding et al. - 2019 - Towards better Validity Dispersion based Clusteri.pdf:application/pdf},
}


@article{zhou_large_2017,
	title = {Large {Margin} {Learning} in {Set} to {Set} {Similarity} {Comparison} for {Person} {Re}-identification},
	url = {http://arxiv.org/abs/1708.05512},
	abstract = {Person re-identification (Re-ID) aims at matching images of the same person across disjoint camera views, which is a challenging problem in multimedia analysis, multimedia editing and content-based media retrieval communities. The major challenge lies in how to preserve similarity of the same person across video footages with large appearance variations, while discriminating different individuals. To address this problem, conventional methods usually consider the pairwise similarity between persons by only measuring the point to point (P2P) distance. In this paper, we propose to use deep learning technique to model a novel set to set (S2S) distance, in which the underline objective focuses on preserving the compactness of intra-class samples for each camera view, while maximizing the margin between the intra-class set and inter-class set. The S2S distance metric is consisted of three terms, namely the class-identity term, the relative distance term and the regularization term. The class-identity term keeps the intra-class samples within each camera view gathering together, the relative distance term maximizes the distance between the intra-class class set and inter-class set across different camera views, and the regularization term smoothness the parameters of deep convolutional neural network (CNN). As a result, the final learned deep model can effectively find out the matched target to the probe object among various candidates in the video gallery by learning discriminative and stable feature representations. Using the CUHK01, CUHK03, PRID2011 and Market1501 benchmark datasets, we extensively conducted comparative evaluations to demonstrate the advantages of our method over the state-of-the-art approaches.},
	urldate = {2021-07-11},
	journal = {arXiv:1708.05512 [cs, stat]},
	author = {Zhou, Sanping and Wang, Jinjun and Shi, Rui and Hou, Qiqi and Gong, Yihong and Zheng, Nanning},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.05512},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\46SPZWRA\\Zhou et al. - 2017 - Large Margin Learning in Set to Set Similarity Com.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\QTB9VGH4\\1708.html:text/html},
}

@inproceedings{chen_person_2017,
	title = {Person {Re}-identification by {Deep} {Learning} {Multi}-scale {Representations}},
	doi = {10.1109/ICCVW.2017.304},
	abstract = {Existing person re-identification (re-id) methods depend mostly on single-scale appearance information. This not only ignores the potentially useful explicit information of other different scales, but also loses the chance of mining the implicit correlated complementary advantages across scales. In this work, we demonstrate the benefits of learning multi-scale person appearance features using Convolutional Neural Networks (CNN) by aiming to jointly learn discriminative scale-specific features and maximise multiscale feature fusion selections in image pyramid inputs. Specifically, we formulate a novel Deep Pyramid Feature Learning (DPFL) CNN architecture for multi-scale appearance feature fusion optimised simultaneously by concurrent per-scale re-id losses and interactive cross-scale consensus regularisation in a closed-loop design. Extensive comparative evaluations demonstrate the re-id advantages of the proposed DPFL model over a wide range of state-of-the-art re-id methods on three benchmarks Market-1501, CUHK03, and DukeMTMC-reID.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} {Workshops} ({ICCVW})},
	author = {Chen, Yanbei and Zhu, Xiatian and Gong, Shaogang},
	month = oct,
	year = {2017},
	note = {ISSN: 2473-9944},
	keywords = {Cameras, Data models, Feature extraction, Image resolution, Propagation losses, Training, Visualization},
	pages = {2590--2600},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\2YHZ7MWJ\\8265515.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\DWBIYWD4\\Chen et al. - 2017 - Person Re-identification by Deep Learning Multi-sc.pdf:application/pdf},
}

@article{sener_learning_nodate,
	title = {Learning {Transferrable} {Representations} for {Unsupervised} {Domain} {Adaptation}},
	abstract = {Supervised learning with large scale labelled datasets and deep layered models has caused a paradigm shift in diverse areas in learning and recognition. However, this approach still suffers from generalization issues under the presence of a domain shift between the training and the test data distribution. Since unsupervised domain adaptation algorithms directly address this domain shift problem between a labelled source dataset and an unlabelled target dataset, recent papers [11, 33] have shown promising results by ﬁne-tuning the networks with domain adaptation loss functions which try to align the mismatch between the training and testing data distributions. Nevertheless, these recent deep learning based domain adaptation approaches still suffer from issues such as high sensitivity to the gradient reversal hyperparameters [11] and overﬁtting during the ﬁne-tuning stage. In this paper, we propose a uniﬁed deep learning framework where the representation, cross domain transformation, and target label inference are all jointly optimized in an end-to-end fashion for unsupervised domain adaptation. Our experiments show that the proposed method signiﬁcantly outperforms state-of-the-art algorithms in both object recognition and digit classiﬁcation experiments by a large margin.},
	language = {en},
	author = {Sener, Ozan and Song, Hyun Oh and Saxena, Ashutosh and Savarese, Silvio},
	pages = {9},
	file = {Sener et al. - Learning Transferrable Representations for Unsuper.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\E7LDRE7L\\Sener et al. - Learning Transferrable Representations for Unsuper.pdf:application/pdf},
}

@article{sener_learning_nodate-1,
	title = {Learning {Transferrable} {Representations} for {Unsupervised} {Domain} {Adaptation}},
	abstract = {Supervised learning with large scale labelled datasets and deep layered models has caused a paradigm shift in diverse areas in learning and recognition. However, this approach still suffers from generalization issues under the presence of a domain shift between the training and the test data distribution. Since unsupervised domain adaptation algorithms directly address this domain shift problem between a labelled source dataset and an unlabelled target dataset, recent papers [11, 33] have shown promising results by ﬁne-tuning the networks with domain adaptation loss functions which try to align the mismatch between the training and testing data distributions. Nevertheless, these recent deep learning based domain adaptation approaches still suffer from issues such as high sensitivity to the gradient reversal hyperparameters [11] and overﬁtting during the ﬁne-tuning stage. In this paper, we propose a uniﬁed deep learning framework where the representation, cross domain transformation, and target label inference are all jointly optimized in an end-to-end fashion for unsupervised domain adaptation. Our experiments show that the proposed method signiﬁcantly outperforms state-of-the-art algorithms in both object recognition and digit classiﬁcation experiments by a large margin.},
	language = {en},
	author = {Sener, Ozan and Song, Hyun Oh and Saxena, Ashutosh and Savarese, Silvio},
	pages = {9},
	file = {Sener et al. - Learning Transferrable Representations for Unsuper.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\9CTWY98R\\Sener et al. - Learning Transferrable Representations for Unsuper.pdf:application/pdf},
}

@inproceedings{zhong_re-ranking_2017,
	address = {Honolulu, HI},
	title = {Re-ranking {Person} {Re}-identification with k-{Reciprocal} {Encoding}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099872/},
	doi = {10.1109/CVPR.2017.389},
	abstract = {When considering person re-identiﬁcation (re-ID) as a retrieval process, re-ranking is a critical step to improve its accuracy. Yet in the re-ID community, limited effort has been devoted to re-ranking, especially those fully automatic, unsupervised solutions..},
	language = {en},
	urldate = {2021-07-11},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhong, Zhun and Zheng, Liang and Cao, Donglin and Li, Shaozi},
	month = jul,
	year = {2017},
	pages = {3652--3661},
	file = {Zhong et al. - 2017 - Re-ranking Person Re-identification with k-Recipro.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\NRXUF9Z9\\Zhong et al. - 2017 - Re-ranking Person Re-identification with k-Recipro.pdf:application/pdf},
}

@article{ma_cross-domain_2015,
	title = {Cross-{Domain} {Person} {Reidentification} {Using} {Domain} {Adaptation} {Ranking} {SVMs}},
	url = {https://www.semanticscholar.org/paper/Cross-Domain-Person-Reidentification-Using-Domain-Ma-Li/16e577820999e584c787ec611f55746cf9147518},
	abstract = {This paper addresses a new person reidentification problem without label information of persons under nonoverlapping target cameras. Given the matched (positive) and unmatched (negative) image pairs from source domain cameras, as well as unmatched (negative) and unlabeled image pairs from target domain cameras, we propose an adaptive ranking support vector machines (AdaRSVMs) method for reidentification under target domain cameras without person labels. To overcome the problems introduced due to the absence of matched (positive) image pairs in the target domain, we relax the discriminative constraint to a necessary condition only relying on the positive mean in the target domain. To estimate the target positive mean, we make use of all the available data from source and target domains as well as constraints in person reidentification. Inspired by adaptive learning methods, a new discriminative model with high confidence in target positive mean and low confidence in target negative image pairs is developed by refining the distance model learnt from the source domain. Experimental results show that the proposed AdaRSVM outperforms existing supervised or unsupervised, learning or non-learning reidentification methods without using label information in target cameras. Moreover, our method achieves better reidentification performance than existing domain adaptation methods derived under equal conditional probability assumption.},
	language = {en},
	urldate = {2021-06-27},
	journal = {undefined},
	author = {Ma, A. J. and Li, Jiawei and Yuen, P. and Li, Ping},
	year = {2015},
	file = {Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\2G3U5KYB\\16e577820999e584c787ec611f55746cf9147518.html:text/html},
}


@article{wang_learning_2018,
	title = {Learning {Discriminative} {Features} with {Multiple} {Granularities} for {Person} {Re}-{Identification}},
	url = {http://arxiv.org/abs/1804.01438},
	doi = {10.1145/3240508.3240552},
	abstract = {The combination of global and partial features has been an essential solution to improve discriminative performances in person re-identification (Re-ID) tasks. Previous part-based methods mainly focus on locating regions with specific pre-defined semantics to learn local representations, which increases learning difficulty but not efficient or robust to scenarios with large variances. In this paper, we propose an end-to-end feature learning strategy integrating discriminative information with various granularities. We carefully design the Multiple Granularity Network (MGN), a multi-branch deep network architecture consisting of one branch for global feature representations and two branches for local feature representations. Instead of learning on semantic regions, we uniformly partition the images into several stripes, and vary the number of parts in different local branches to obtain local feature representations with multiple granularities. Comprehensive experiments implemented on the mainstream evaluation datasets including Market-1501, DukeMTMC-reid and CUHK03 indicate that our method has robustly achieved state-of-the-art performances and outperformed any existing approaches by a large margin. For example, on Market-1501 dataset in single query mode, we achieve a state-of-the-art result of Rank-1/mAP=96.6\%/94.2\% after re-ranking.},
	urldate = {2021-07-11},
	journal = {Proceedings of the 26th ACM international conference on Multimedia},
	author = {Wang, Guanshuo and Yuan, Yufeng and Chen, Xiong and Li, Jiwei and Zhou, Xi},
	month = oct,
	year = {2018},
	note = {arXiv: 1804.01438},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {274--282},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\CLWK76DD\\Wang et al. - 2018 - Learning Discriminative Features with Multiple Gra.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\Q4PANS5A\\1804.html:text/html},
}

@article{yang_distance_2006,
	title = {Distance {Metric} {Learning}: {A} {Comprehensive} {Survey}},
	language = {en},
	author = {Yang, Liu and Jin, Rong},
	year = {2006},
	pages = {51},
	file = {Yang and Jin - Distance Metric Learning A Comprehensive Survey.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\ND6LW33C\\Yang and Jin - Distance Metric Learning A Comprehensive Survey.pdf:application/pdf},
}

@article{lisanti_person_2015,
	title = {Person {Re}-{Identification} by {Iterative} {Re}-{Weighted} {Sparse} {Ranking}},
	volume = {37},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/6951486/},
	doi = {10.1109/TPAMI.2014.2369055},
	abstract = {In this paper we introduce a method for person re-identiﬁcation based on discriminative, sparse basis expansions of targets in terms of a labeled gallery of known individuals. We propose an iterative extension to sparse discriminative classiﬁers capable of ranking many candidate targets. The approach makes use of soft- and hard- re-weighting to redistribute energy among the most relevant contributing elements and to ensure that the best candidates are ranked at each iteration. Our approach also leverages a novel visual descriptor which we show to be discriminative while remaining robust to pose and illumination variations. An extensive comparative evaluation is given demonstrating that our approach achieves state-of-the-art performance on single- and multi-shot person re-identiﬁcation scenarios on the VIPeR, i-LIDS, ETHZ, and CAVIAR4REID datasets. The combination of our descriptor and iterative sparse basis expansion improves state-of-the-art rank-1 performance by 6 percentage points on VIPeR and by 20 on CAVIAR4REID compared to other methods with a single gallery image per person. With multiple gallery and probe images per person our approach improves by 17 percentage points the state-of-the-art on i-LIDS and by 72 on CAVIAR4REID at rank-1. The approach is also quite efﬁcient, capable of single-shot person re-identiﬁcation over galleries containing hundreds of individuals at about 30 re-identiﬁcations per second.},
	language = {en},
	number = {8},
	urldate = {2021-07-12},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Lisanti, Giuseppe and Masi, Iacopo and Bagdanov, Andrew D. and Bimbo, Alberto Del},
	month = aug,
	year = {2015},
	pages = {1629--1642},
	file = {Lisanti et al. - 2015 - Person Re-Identification by Iterative Re-Weighted .pdf:C\:\\Users\\lxt76\\Zotero\\storage\\B5QIE3VG\\Lisanti et al. - 2015 - Person Re-Identification by Iterative Re-Weighted .pdf:application/pdf},
}

@inproceedings{ma_bicov_2012,
	address = {Surrey},
	title = {{BiCov}: a novel image representation for person re-identification and face verification},
	isbn = {978-1-901725-46-9},
	shorttitle = {{BiCov}},
	url = {http://www.bmva.org/bmvc/2012/BMVC/paper057/index.html},
	doi = {10.5244/C.26.57},
	abstract = {This paper proposes a novel image representation which can properly handle both background and illumination variations. It is therefore adapted to the person/face reidentiﬁcation tasks, avoiding the use of any additional pre-processing steps such as foreground-background separation or face and body part segmentation. This novel representation relies on the combination of Biologically Inspired Features (BIF) and covariance descriptors used to compute the similarity of the BIF features at neighboring scales. Hence, we will refer to it as the BiCov representation. To show the effectiveness of BiCov, this paper conducts experiments on two person re-identiﬁcation tasks (VIPeR and ETHZ) and one face veriﬁcation task (LFW), on which it improves the current state-ofthe-art performance.},
	language = {en},
	urldate = {2021-07-12},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2012},
	publisher = {British Machine Vision Association},
	author = {Ma, Bingpeng and Su, Yu and Jurie, Frederic},
	year = {2012},
	pages = {57.1--57.11},
	file = {Ma et al. - 2012 - BiCov a novel image representation for person re-.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\UK3E46N3\\Ma et al. - 2012 - BiCov a novel image representation for person re-.pdf:application/pdf},
}

@inproceedings{wang_unsupervised_2014,
	address = {Nottingham},
	title = {Unsupervised {Learning} of {Generative} {Topic} {Saliency} for {Person} {Re}-identification},
	isbn = {978-1-901725-52-0},
	url = {http://www.bmva.org/bmvc/2014/papers/paper019/index.html},
	doi = {10.5244/C.28.48},
	language = {en},
	urldate = {2021-07-12},
	booktitle = {Proceedings of the {British} {Machine} {Vision} {Conference} 2014},
	publisher = {British Machine Vision Association},
	author = {Wang, Hanxiao and Gong, Shaogang and Xiang, Tao},
	year = {2014},
	pages = {48.1--48.11},
	file = {Submitted Version:C\:\\Users\\lxt76\\Zotero\\storage\\XK533P8V\\Wang et al. - 2014 - Unsupervised Learning of Generative Topic Saliency.pdf:application/pdf},
}


@article{ye_dynamic_2017,
	title = {Dynamic {Label} {Graph} {Matching} for {Unsupervised} {Video} {Re}-{Identification}},
	doi = {10.1109/ICCV.2017.550},
	abstract = {Label estimation is an important component in an unsupervised person re-identification (re-ID) system. This paper focuses on cross-camera label estimation, which can be subsequently used in feature learning to learn robust re-ID models. Specifically, we propose to construct a graph for samples in each camera, and then graph matching scheme is introduced for cross-camera labeling association. While labels directly output from existing graph matching methods may be noisy and inaccurate due to significant cross-camera variations, this paper proposes a dynamic graph matching (DGM) method. DGM iteratively updates the image graph and the label estimation process by learning a better feature space with intermediate estimated labels. DGM is advantageous in two aspects: 1) the accuracy of estimated labels is improved significantly with the iterations; 2) DGM is robust to noisy initial training data. Extensive experiments conducted on three benchmarks including the large-scale MARS dataset show that DGM yields competitive performance to fully supervised baselines, and outperforms competing unsupervised learning methods.},
	author = {Ye, Mang and Ma, Andy and Zheng, Liang and Li, Jiawei and Yuen, P C},
	month = sep,
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\8QTUNF7U\\Ye et al. - 2017 - Dynamic Label Graph Matching for Unsupervised Vide.pdf:application/pdf},
}

@inproceedings{li_person_2017,
	address = {Melbourne, Australia},
	title = {Person {Re}-{Identification} by {Deep} {Joint} {Learning} of {Multi}-{Loss} {Classification}},
	isbn = {978-0-9992411-0-3},
	url = {https://www.ijcai.org/proceedings/2017/305},
	doi = {10.24963/ijcai.2017/305},
	abstract = {Existing person re-identiﬁcation (re-id) methods rely mostly on either localised or global feature representation alone. This ignores their joint beneﬁt and mutual complementary effects. In this work, we show the advantages of jointly learning local and global features in a Convolutional Neural Network (CNN) by aiming to discover correlated local and global features in different context. Speciﬁcally, we formulate a method for joint learning of local and global feature selection losses designed to optimise person re-id when using only generic matching metrics such as the L2 distance. We design a novel CNN architecture for Jointly Learning Multi-Loss (JLML). Extensive comparative evaluations demonstrate the advantages of this new JLML model for person re-id over a wide range of state-of-the-art re-id methods on four benchmarks (VIPeR, GRID, CUHK03, Market-1501).},
	language = {en},
	urldate = {2021-07-12},
	booktitle = {Proceedings of the {Twenty}-{Sixth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Li, Wei and Zhu, Xiatian and Gong, Shaogang},
	month = aug,
	year = {2017},
	pages = {2194--2200},
	file = {Li et al. - 2017 - Person Re-Identification by Deep Joint Learning of.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\DQ6T898G\\Li et al. - 2017 - Person Re-Identification by Deep Joint Learning of.pdf:application/pdf},
}
@inproceedings{liu_multi-scale_2016,
	address = {New York, NY, USA},
	series = {{MM} '16},
	title = {Multi-{Scale} {Triplet} {CNN} for {Person} {Re}-{Identification}},
	isbn = {978-1-4503-3603-1},
	url = {https://doi.org/10.1145/2964284.2967209},
	doi = {10.1145/2964284.2967209},
	abstract = {Person re-identification aims at identifying a certain person across non-overlapping multi-camera networks. It is a fundamental and challenging task in automated video surveillance. Most existing researches mainly rely on hand-crafted features, resulting in unsatisfactory performance. In this paper, we propose a multi-scale triplet convolutional neural network which captures visual appearance of a person at various scales. We propose to optimize the network parameters by a comparative similarity loss on massive sample triplets, addressing the problem of small training set in person re-identification. In particular, we design a unified multi-scale network architecture consisting of both deep and shallow neural networks, towards learning robust and effective features for person re-identification under complex conditions. Extensive evaluation on the real-world Market-1501 dataset have demonstrated the effectiveness of the proposed approach.},
	urldate = {2021-07-11},
	booktitle = {Proceedings of the 24th {ACM} international conference on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Jiawei and Zha, Zheng-Jun and Tian, QI and Liu, Dong and Yao, Ting and Ling, Qiang and Mei, Tao},
	month = oct,
	year = {2016},
	keywords = {deep CNN, person re-identification},
	pages = {192--196},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\QR2R974M\\Liu et al. - 2016 - Multi-Scale Triplet CNN for Person Re-Identificati.pdf:application/pdf},
}
@article{ganin_unsupervised_2015,
	title = {Unsupervised {Domain} {Adaptation} by {Backpropagation}},
	abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled targetdomain data is necessary).},
	language = {en},
	author = {Ganin, Yaroslav and Lempitsky, Victor},
	year = {2015},
	pages = {10},
	file = {Ganin and Lempitsky - Unsupervised Domain Adaptation by Backpropagation.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\B7J2ZCIQ\\Ganin and Lempitsky - Unsupervised Domain Adaptation by Backpropagation.pdf:application/pdf},
}

@inproceedings{tzeng_adversarial_2017,
	address = {Honolulu, HI},
	title = {Adversarial {Discriminative} {Domain} {Adaptation}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099799/},
	doi = {10.1109/CVPR.2017.316},
	abstract = {Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They can also improve recognition despite the presence of domain shift or dataset bias: recent adversarial approaches to unsupervised domain adaptation reduce the difference between the training and test domain distributions and thus improve generalization performance. However, while generative adversarial networks (GANs) show compelling visualizations, they are not optimal on discriminative tasks and can be limited to smaller shifts. On the other hand, discriminative approaches can handle larger domain shifts, but impose tied weights on the model and do not exploit a GAN-based loss. In this work, we ﬁrst outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and use this generalized view to better relate prior approaches. We then propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domainadversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard domain adaptation tasks as well as a difﬁcult cross-modality object classiﬁcation task.},
	language = {en},
	urldate = {2021-07-12},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},
	month = jul,
	year = {2017},
	pages = {2962--2971},
	file = {Tzeng et al. - 2017 - Adversarial Discriminative Domain Adaptation.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\ZULBPF2C\\Tzeng et al. - 2017 - Adversarial Discriminative Domain Adaptation.pdf:application/pdf},
}
@inproceedings{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Nets}},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and Weinberger, K. Q.},
	year = {2014},
	file = {Goodfellow et al. - Generative Adversarial Nets.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\NUVYSA92\\Goodfellow et al. - Generative Adversarial Nets.pdf:application/pdf},
}

@inproceedings{zhu_unpaired_2017,
	title = {Unpaired {Image}-to-{Image} {Translation} {Using} {Cycle}-{Consistent} {Adversarial} {Networks}},
	doi = {10.1109/ICCV.2017.244},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to push F(G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	month = oct,
	year = {2017},
	note = {ISSN: 2380-7504},
	keywords = {Extraterrestrial measurements, Graphics, Painting, Semantics, Training, Training data},
	pages = {2242--2251},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\AI5T7S2J\\8237506.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\UNF9EK2N\\Zhu et al. - 2017 - Unpaired Image-to-Image Translation Using Cycle-Co.pdf:application/pdf},
}
@inproceedings{wu_clustering_2019,
	title = {Clustering and {Dynamic} {Sampling} {Based} {Unsupervised} {Domain} {Adaptation} for {Person} {Re}-{Identification}},
	doi = {10.1109/ICME.2019.00157},
	abstract = {Person Re-Identification (Re-ID) has witnessed great improvements due to the advances of the deep convolutional neural networks (CNN). Despite this, existing methods mainly suffer from the poor generalization ability to unseen scenes because of the different characteristics between different domains. To address this issue, a Clustering and Dynamic Sampling (CDS) method is proposed in this paper, which tries to transfer the useful knowledge of existing labeled source domain to the unlabeled target one. Specifically, to improve the discriminability of CNN model on source domain, we use the commonly shared pedestrian attributes (e.g., gender, hat and clothing color etc.) to enrich the information and resort to the margin-based softmax (e.g., A-Softmax) loss to train the model. For the unlabeled target domain, we iteratively cluster the samples into several centers and dynamically select informative ones from each center to fine-tune the source-domain model. Extensive experiments on DukeMTMC-reID and Market-1501 datasets show that the proposed method greatly improves the state of the arts in unsupervised domain adaptation.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	author = {Wu, Jinlin and Liao, Shengcai and lei, Zhen and Wang, Xiaobo and Yang, Yang and Li, Stan Z.},
	month = jul,
	year = {2019},
	note = {ISSN: 1945-788X},
	keywords = {A-Softmax, Adaptation models, Cameras, Clothing, Clustering, Dynamic Sampling, Heuristic algorithms, Image color analysis, Pedestrian Attributes, Task analysis, Training},
	pages = {886--891},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\PBBB82TD\\8784975.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\2ED9TVP7\\Wu et al. - 2019 - Clustering and Dynamic Sampling Based Unsupervised.pdf:application/pdf},
}

@inproceedings{wang_towards_2016,
	title = {Towards unsupervised open-set person re-identification},
	doi = {10.1109/ICIP.2016.7532461},
	author = {Wang, Hanxiao and Zhu, Xiatian and Xiang, Tao and Gong, Shaogang},
	month = sep,
	year = {2016},
	pages = {769--773},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\YS87PHSI\\Wang et al. - 2016 - Towards unsupervised open-set person re-identifica.pdf:application/pdf},
}


@article{li_unsupervised_2020,
	title = {Unsupervised {Tracklet} {Person} {Re}-{Identification}},
	volume = {42},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2019.2903058},
	abstract = {Most existing person re-identification (re-id) methods rely on supervised model learning on per-camera-pair manually labelled pairwise training data. This leads to poor scalability in a practical re-id deployment, due to the lack of exhaustive identity labelling of positive and negative image pairs for every camera-pair. In this work, we present an unsupervised re-id deep learning approach. It is capable of incrementally discovering and exploiting the underlying re-id discriminative information from automatically generated person tracklet data end-to-end. We formulate an Unsupervised Tracklet Association Learning (UTAL) framework. This is by jointly learning within-camera tracklet discrimination and cross-camera tracklet association in order to maximise the discovery of tracklet identity matching both within and across camera views. Extensive experiments demonstrate the superiority of the proposed model over the state-of-the-art unsupervised learning and domain adaptation person re-id methods on eight benchmarking datasets.},
	number = {7},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Li, Minxian and Zhu, Xiatian and Gong, Shaogang},
	month = jul,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Adaptation models, Cameras, Data models, Deep learning, Labeling, multi-task deep learning, Person re-identification, Training data, trajectory fragmentation, Unsupervised learning, unsupervised tracklet association},
	pages = {1770--1782},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lxt76\\Zotero\\storage\\BBUC84ZH\\8658110.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\HJECFTKI\\Li et al. - 2020 - Unsupervised Tracklet Person Re-Identification.pdf:application/pdf},
}


@article{leal-taixe_motchallenge_2015,
	title = {{MOTChallenge} 2015: {Towards} a {Benchmark} for {Multi}-{Target} {Tracking}},
	shorttitle = {{MOTChallenge} 2015},
	url = {http://arxiv.org/abs/1504.01942},
	abstract = {In the recent past, the computer vision community has developed centralized benchmarks for the performance evaluation of a variety of tasks, including generic object and pedestrian detection, 3D reconstruction, optical flow, single-object short-term tracking, and stereo estimation. Despite potential pitfalls of such benchmarks, they have proved to be extremely helpful to advance the state of the art in the respective area. Interestingly, there has been rather limited work on the standardization of quantitative benchmarks for multiple target tracking. One of the few exceptions is the well-known PETS dataset, targeted primarily at surveillance applications. Despite being widely used, it is often applied inconsistently, for example involving using different subsets of the available data, different ways of training the models, or differing evaluation scripts. This paper describes our work toward a novel multiple object tracking benchmark aimed to address such issues. We discuss the challenges of creating such a framework, collecting existing and new data, gathering state-of-the-art methods to be tested on the datasets, and finally creating a unified evaluation system. With MOTChallenge we aim to pave the way toward a unified evaluation framework for a more meaningful quantification of multi-target tracking.},
	urldate = {2021-07-12},
	journal = {arXiv:1504.01942 [cs]},
	author = {Leal-Taixé, Laura and Milan, Anton and Reid, Ian and Roth, Stefan and Schindler, Konrad},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.01942},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\4BHDKQRQ\\Leal-Taixé et al. - 2015 - MOTChallenge 2015 Towards a Benchmark for Multi-T.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\UQJXK7S9\\1504.html:text/html},
}

@inproceedings{gao_graph_2019,
	address = {Long Beach, CA, USA},
	title = {Graph {Convolutional} {Tracking}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953448/},
	doi = {10.1109/CVPR.2019.00478},
	abstract = {Tracking by siamese networks has achieved favorable performance in recent years. However, most of existing siamese methods do not take full advantage of spatialtemporal target appearance modeling under different contextual situations. In fact, the spatial-temporal information can provide diverse features to enhance the target representation, and the context information is important for online adaption of target localization. To comprehensively leverage the spatial-temporal structure of historical target exemplars and get beneﬁt from the context information, in this work, we present a novel Graph Convolutional Tracking (GCT) method for high-performance visual tracking. Speciﬁcally, the GCT jointly incorporates two types of Graph Convolutional Networks (GCNs) into a siamese framework for target appearance modeling. Here, we adopt a spatial-temporal GCN to model the structured representation of historical target exemplars. Furthermore, a context GCN is designed to utilize the context of the current frame to learn adaptive features for target localization. Extensive results on 4 challenging benchmarks show that our GCT method performs favorably against state-of-the-art trackers while running around 50 frames per second.},
	language = {en},
	urldate = {2021-07-12},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Gao, Junyu and Zhang, Tianzhu and Xu, Changsheng},
	month = jun,
	year = {2019},
	pages = {4644--4654},
	file = {Gao et al. - 2019 - Graph Convolutional Tracking.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\2WCQL98R\\Gao et al. - 2019 - Graph Convolutional Tracking.pdf:application/pdf},
}

@article{gao_i_2019,
	title = {I {Know} the {Relationships}: {Zero}-{Shot} {Action} {Recognition} via {Two}-{Stream} {Graph} {Convolutional} {Networks} and {Knowledge} {Graphs}.},
	shorttitle = {I {Know} the {Relationships}},
	doi = {10.1609/aaai.v33i01.33018303},
	author = {Gao, Junyu},
	year = {2019},
	pages = {8303--8311},
	file = {dblp\: I Know the Relationships\: Zero-Shot Action Recognition via Two-Stream Graph Convolutional Networks and Knowledge Graphs.:C\:\\Users\\lxt76\\Zotero\\storage\\YRFSJPIQ\\GaoZX19.html:text/html;Full Text:C\:\\Users\\lxt76\\Zotero\\storage\\ZZHFMQ2U\\Gao - 2019 - I Know the Relationships Zero-Shot Action Recogni.pdf:application/pdf},
}

@article{zhang_deeper_2019,
	title = {Deeper and {Wider} {Siamese} {Networks} for {Real}-{Time} {Visual} {Tracking}},
	url = {http://arxiv.org/abs/1901.01660},
	abstract = {Siamese networks have drawn great attention in visual tracking because of their balanced accuracy and speed. However, the backbone networks used in Siamese trackers are relatively shallow, such as AlexNet [18], which does not fully take advantage of the capability of modern deep neural networks. In this paper, we investigate how to leverage deeper and wider convolutional neural networks to enhance tracking robustness and accuracy. We observe that direct replacement of backbones with existing powerful architectures, such as ResNet [14] and Inception [33], does not bring improvements. The main reasons are that 1)large increases in the receptive field of neurons lead to reduced feature discriminability and localization precision; and 2) the network padding for convolutions induces a positional bias in learning. To address these issues, we propose new residual modules to eliminate the negative impact of padding, and further design new architectures using these modules with controlled receptive field size and network stride. The designed architectures are lightweight and guarantee real-time tracking speed when applied to SiamFC [2] and SiamRPN [20]. Experiments show that solely due to the proposed network architectures, our SiamFC+ and SiamRPN+ obtain up to 9.8\%/5.7\% (AUC), 23.3\%/8.8\% (EAO) and 24.4\%/25.0\% (EAO) relative improvements over the original versions [2, 20] on the OTB-15, VOT-16 and VOT-17 datasets, respectively.},
	urldate = {2021-07-12},
	journal = {arXiv:1901.01660 [cs]},
	author = {Zhang, Zhipeng and Peng, Houwen},
	month = mar,
	year = {2019},
	note = {arXiv: 1901.01660},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\RDBZLU7C\\Zhang and Peng - 2019 - Deeper and Wider Siamese Networks for Real-Time Vi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\B7IDVB3N\\1901.html:text/html},
}

@article{girshick_deformable_2014,
	title = {Deformable {Part} {Models} are {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1409.5403},
	abstract = {Deformable part models (DPMs) and convolutional neural networks (CNNs) are two widely used tools for visual recognition. They are typically viewed as distinct approaches: DPMs are graphical models (Markov random fields), while CNNs are "black-box" non-linear classifiers. In this paper, we show that a DPM can be formulated as a CNN, thus providing a novel synthesis of the two ideas. Our construction involves unrolling the DPM inference algorithm and mapping each step to an equivalent (and at times novel) CNN layer. From this perspective, it becomes natural to replace the standard image features used in DPM with a learned feature extractor. We call the resulting model DeepPyramid DPM and experimentally validate it on PASCAL VOC. DeepPyramid DPM significantly outperforms DPMs based on histograms of oriented gradients features (HOG) and slightly outperforms a comparable version of the recently introduced R-CNN detection system, while running an order of magnitude faster.},
	urldate = {2021-07-12},
	journal = {arXiv:1409.5403 [cs]},
	author = {Girshick, Ross and Iandola, Forrest and Darrell, Trevor and Malik, Jitendra},
	month = oct,
	year = {2014},
	note = {arXiv: 1409.5403},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\MDZVIUY3\\Girshick et al. - 2014 - Deformable Part Models are Convolutional Neural Ne.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\DGQ8VVXW\\1409.html:text/html},
}

@inproceedings{hirzer_person_2011,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Person {Re}-identification by {Descriptive} and {Discriminative} {Classification}},
	isbn = {978-3-642-21227-7},
	doi = {10.1007/978-3-642-21227-7_9},
	abstract = {Person re-identification, i.e., recognizing a single person across spatially disjoint cameras, is an important task in visual surveillance. Existing approaches either try to find a suitable description of the appearance or learn a discriminative model. Since these different representational strategies capture a large extent of complementary information we propose to combine both approaches. First, given a specific query, we rank all samples according to a feature-based similarity, where appearance is modeled by a set of region covariance descriptors. Next, a discriminative model is learned using boosting for feature selection, which provides a more specific classifier. The proposed approach is demonstrated on two datasets, where we show that the combination of a generic descriptive statistical model and a discriminatively learned feature-based model attains considerably better results than the individual models alone. In addition, we give a comparison to the state-of-the-art on a publicly available benchmark dataset.},
	language = {en},
	booktitle = {Image {Analysis}},
	publisher = {Springer},
	author = {Hirzer, Martin and Beleznai, Csaba and Roth, Peter M. and Bischof, Horst},
	editor = {Heyden, Anders and Kahl, Fredrik},
	year = {2011},
	keywords = {Covariance Feature, Discriminative Model, Feature Selection, Image Pair, Probe Image},
	pages = {91--102},
	file = {Springer Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\246DHD85\\Hirzer et al. - 2011 - Person Re-identification by Descriptive and Discri.pdf:application/pdf},
}

@inproceedings{wang_person_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Person {Re}-identification by {Video} {Ranking}},
	isbn = {978-3-319-10593-2},
	doi = {10.1007/978-3-319-10593-2_45},
	abstract = {Current person re-identification (re-id) methods typically rely on single-frame imagery features, and ignore space-time information from image sequences. Single-frame (single-shot) visual appearance matching is inherently limited for person re-id in public spaces due to visual ambiguity arising from non-overlapping camera views where viewpoint and lighting changes can cause significant appearance variation. In this work, we present a novel model to automatically select the most discriminative video fragments from noisy image sequences of people where more reliable space-time features can be extracted, whilst simultaneously to learn a video ranking function for person re-id. Also, we introduce a new image sequence re-id dataset (iLIDS-VID) based on the i-LIDS MCT benchmark data. Using the iLIDS-VID and PRID 2011 sequence re-id datasets, we extensively conducted comparative evaluations to demonstrate the advantages of the proposed model over contemporary gait recognition, holistic image sequence matching and state-of-the-art single-shot/multi-shot based re-id methods.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Wang, Taiqing and Gong, Shaogang and Zhu, Xiatian and Wang, Shengjin},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {Action Recognition, Dynamic Time Warping, Gait Recognition, Image Sequence, Video Fragment},
	pages = {688--703},
	file = {Springer Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\U7QJB226\\Wang et al. - 2014 - Person Re-identification by Video Ranking.pdf:application/pdf},
}

@inproceedings{chen_domain_2018,
	address = {Salt Lake City, UT, USA},
	title = {Domain {Adaptive} {Faster} {R}-{CNN} for {Object} {Detection} in the {Wild}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578450/},
	doi = {10.1109/CVPR.2018.00352},
	abstract = {Object detection typically assumes that training and test data are drawn from an identical distribution, which, however, does not always hold in practice. Such a distribution mismatch will lead to a signiﬁcant performance drop. In this work, we aim to improve the cross-domain robustness of object detection. We tackle the domain shift on two levels: 1) the image-level shift, such as image style, illumination, etc., and 2) the instance-level shift, such as object appearance, size, etc. We build our approach based on the recent state-of-the-art Faster R-CNN model, and design two domain adaptation components, on image level and instance level, to reduce the domain discrepancy. The two domain adaptation components are based on H-divergence theory, and are implemented by learning a domain classiﬁer in adversarial training manner. The domain classiﬁers on different levels are further reinforced with a consistency regularization to learn a domain-invariant region proposal network (RPN) in the Faster R-CNN model. We evaluate our newly proposed approach using multiple datasets including Cityscapes, KITTI, SIM10K, etc. The results demonstrate the effectiveness of our proposed approach for robust object detection in various domain shift scenarios.},
	language = {en},
	urldate = {2021-07-12},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Chen, Yuhua and Li, Wen and Sakaridis, Christos and Dai, Dengxin and Van Gool, Luc},
	month = jun,
	year = {2018},
	pages = {3339--3348},
	file = {Chen et al. - 2018 - Domain Adaptive Faster R-CNN for Object Detection .pdf:C\:\\Users\\lxt76\\Zotero\\storage\\AZ9JN847\\Chen et al. - 2018 - Domain Adaptive Faster R-CNN for Object Detection .pdf:application/pdf},
}

@inproceedings{chen_road_2018,
	address = {Salt Lake City, UT},
	title = {{ROAD}: {Reality} {Oriented} {Adaptation} for {Semantic} {Segmentation} of {Urban} {Scenes}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{ROAD}},
	url = {https://ieeexplore.ieee.org/document/8578921/},
	doi = {10.1109/CVPR.2018.00823},
	abstract = {Exploiting synthetic data to learn deep models has attracted increasing attention in recent years. However, the intrinsic domain difference between synthetic and real images usually causes a signiﬁcant performance drop when applying the learned model to real world scenarios. This is mainly due to two reasons: 1) the model overﬁts to synthetic images, making the convolutional ﬁlters incompetent to extract informative representation for real images; 2) there is a distribution difference between synthetic and real data, which is also known as the domain adaptation problem. To this end, we propose a new reality oriented adaptation approach for urban scene semantic segmentation by learning from synthetic data. First, we propose a target guided distillation approach to learn the real image style, which is achieved by training the segmentation model to imitate a pretrained real style model using real images. Second, we further take advantage of the intrinsic spatial structure presented in urban scene images, and propose a spatialaware adaptation scheme to effectively align the distribution of two domains. These two modules can be readily integrated with existing state-of-the-art semantic segmentation networks to improve their generalizability when adapting from synthetic to real urban scenes. We evaluate the proposed method on Cityscapes dataset by adapting from GTAV and SYNTHIA datasets, where the results demonstrate the effectiveness of our method.},
	language = {en},
	urldate = {2021-07-12},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Chen, Yuhua and Li, Wen and Gool, Luc Van},
	month = jun,
	year = {2018},
	pages = {7892--7901},
	file = {Chen et al. - 2018 - ROAD Reality Oriented Adaptation for Semantic Seg.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\YSQQB2EA\\Chen et al. - 2018 - ROAD Reality Oriented Adaptation for Semantic Seg.pdf:application/pdf},
}

@inproceedings{motiian_unified_2017,
	address = {Venice},
	title = {Unified {Deep} {Supervised} {Domain} {Adaptation} and {Generalization}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237871/},
	doi = {10.1109/ICCV.2017.609},
	abstract = {This work provides a uniﬁed framework for addressing the problem of visual supervised domain adaptation and generalization with deep models. The main idea is to exploit the Siamese architecture to learn an embedding subspace that is discriminative, and where mapped visual domains are semantically aligned and yet maximally separated. The supervised setting becomes attractive especially when only few target data samples need to be labeled. In this scenario, alignment and separation of semantic probability distributions is difﬁcult because of the lack of data. We found that by reverting to point-wise surrogates of distribution distances and similarities provides an effective solution. In addition, the approach has a high “speed” of adaptation, which requires an extremely low number of labeled target training samples, even one per category can be effective. The approach is extended to domain generalization. For both applications the experiments show very promising results.},
	language = {en},
	urldate = {2021-07-12},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Motiian, Saeid and Piccirilli, Marco and Adjeroh, Donald A. and Doretto, Gianfranco},
	month = oct,
	year = {2017},
	pages = {5716--5726},
	file = {Motiian et al. - 2017 - Unified Deep Supervised Domain Adaptation and Gene.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\KL7IF8A7\\Motiian et al. - 2017 - Unified Deep Supervised Domain Adaptation and Gene.pdf:application/pdf},
}

@article{ge_self-paced_2020,
	title = {Self-paced {Contrastive} {Learning} with {Hybrid} {Memory} for {Domain} {Adaptive} {Object} {Re}-{ID}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/821fa74b50ba3f7cba1e6c53e8fa6845-Abstract.html},
	language = {en},
	urldate = {2021-08-03},
	journal = {Advances in Neural Information Processing Systems},
	author = {Ge, Yixiao and Zhu, Feng and Chen, Dapeng and Zhao, Rui and Li, Hongsheng},
	year = {2020},
	pages = {11309--11321},
	file = {Full Text PDF:C\:\\Users\\lxt76\\Zotero\\storage\\U8MBZW3V\\Ge et al. - 2020 - Self-paced Contrastive Learning with Hybrid Memory.pdf:application/pdf;Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\7LGDTZ69\\821fa74b50ba3f7cba1e6c53e8fa6845-Abstract.html:text/html},
}
@inproceedings{xu_person_2014,
	address = {New York, NY, USA},
	series = {{MM} '14},
	title = {Person {Search} in a {Scene} by {Jointly} {Modeling} {People} {Commonness} and {Person} {Uniqueness}},
	isbn = {978-1-4503-3063-3},
	url = {https://doi.org/10.1145/2647868.2654965},
	doi = {10.1145/2647868.2654965},
	abstract = {This paper presents a novel framework for a multimedia search task: searching a person in a scene using human body appearance. Existing works mostly focus on two independent problems related to this task, i.e., people detection and person re-identification. However, a sequential combination of these two components does not solve the person search problem seamlessly for two reasons: 1) the errors in people detection are carried into person re-identification unavoidably; 2) the setting of person re-identification is different from that of person search which is essentially a verification problem. To bridge this gap, we propose a unified framework which jointly models the commonness of people (for detection) and the uniqueness of a person (for identification). We demonstrate superior performance of our approach on public benchmarks compared with the sequential combination of the state-of-the-art detection and identification algorithms.},
	urldate = {2021-01-29},
	booktitle = {Proceedings of the 22nd {ACM} international conference on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Xu, Yuanlu and Ma, Bingpeng and Huang, Rui and Lin, Liang},
	month = nov,
	year = {2014},
	keywords = {person search, fisher vector, generative model, gmm},
	pages = {937--940},
	file = {Full Text PDF:C\:\\Users\\Xiangtan\\Zotero\\storage\\MR2H29N6\\Xu et al. - 2014 - Person Search in a Scene by Jointly Modeling Peopl.pdf:application/pdf}
}

@inproceedings{ijcai2021-613,
  title     = {Person Search Challenges and Solutions: A Survey},
  author    = {Lin, Xiangtan and Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Hauptmann, Alex},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on
               Artificial Intelligence, {IJCAI-21}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Zhi-Hua Zhou},
  pages     = {4500--4507},
  year      = {2021},
  month     = {8},
  note      = {Survey Track},
  doi       = {10.24963/ijcai.2021/613},
  url       = {https://doi.org/10.24963/ijcai.2021/613},
}

@inproceedings{isola_image--image_2017,
	address = {Honolulu, HI},
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8100115/},
	doi = {10.1109/CVPR.2017.632},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.},
	language = {en},
	urldate = {2021-08-28},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = jul,
	year = {2017},
	pages = {5967--5976},
	file = {Isola et al. - 2017 - Image-to-Image Translation with Conditional Advers.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\G9RFP7CK\\Isola et al. - 2017 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf},
}
@inproceedings{li_person_2017b,
	address = {Honolulu, HI},
	title = {Person {Search} with {Natural} {Language} {Description}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8100034/},
	doi = {10.1109/CVPR.2017.551},
	abstract = {Searching persons in large-scale image databases with the query of natural language description has important applications in video surveillance. Existing methods mainly focused on searching persons with image-based or attribute-based queries, which have major limitations for a practical usage. In this paper, we study the problem of person search with natural language description. Given the textual description of a person, the algorithm of the person search is required to rank all the samples in the person database then retrieve the most relevant sample corresponding to the queried description. Since there is no person dataset or benchmark with textual description available, we collect a large-scale person description dataset with detailed natural language annotations and person samples from various sources, termed as CUHK Person Description Dataset (CUHK-PEDES). A wide range of possible models and baselines have been evaluated and compared on the person search benchmark. An Recurrent Neural Network with Gated Neural Attention mechanism (GNARNN) is proposed to establish the state-of-the art performance on person search.},
	language = {en},
	urldate = {2021-08-30},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Li, Shuang and Xiao, Tong and Li, Hongsheng and Zhou, Bolei and Yue, Dayu and Wang, Xiaogang},
	month = jul,
	year = {2017},
	pages = {5187--5196},
	file = {Li et al. - 2017 - Person Search with Natural Language Description.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\5VVC7IVR\\Li et al. - 2017 - Person Search with Natural Language Description.pdf:application/pdf},
}


@inproceedings{dehghan_gmmcp_2015,
	address = {Boston, MA, USA},
	title = {{GMMCP} tracker: {Globally} optimal {Generalized} {Maximum} {Multi} {Clique} problem for multiple object tracking},
	isbn = {978-1-4673-6964-0},
	shorttitle = {{GMMCP} tracker},
	url = {http://ieeexplore.ieee.org/document/7299036/},
	doi = {10.1109/CVPR.2015.7299036},
	abstract = {Data association is the backbone to many multiple object tracking (MOT) methods. In this paper we formulate data association as a Generalized Maximum Multi Clique problem (GMMCP). We show that this is the ideal case of modeling tracking in real world scenario where all the pairwise relationships between targets in a batch of frames are taken into account. Previous works assume simpliﬁed version of our tracker either in problem formulation or problem optimization. However, we propose a solution using GMMCP where no simpliﬁcation is assumed in either steps. We show that the NP hard problem of GMMCP can be formulated through Binary-Integer Program where for small and medium size MOT problems the solution can be found efﬁciently. We further propose a speed-up method, employing Aggregated Dummy Nodes for modeling occlusion and miss-detection, which reduces the size of the input graph without using any heuristics. We show that, using the speedup method, our tracker lends itself to real-time implementation which is plausible in many applications. We evaluated our tracker on six challenging sequences of Town Center, TUD-Crossing, TUD-Stadtmitte, Parking-lot 1, Parking-lot 2 and Parking-lot pizza and show favorable improvement against state of art.},
	language = {en},
	urldate = {2021-09-03},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Dehghan, Afshin and Assari, Shayan Modiri and Shah, Mubarak},
	month = jun,
	year = {2015},
	pages = {4091--4099},
	file = {Dehghan et al. - 2015 - GMMCP tracker Globally optimal Generalized Maximu.pdf:C\:\\Users\\lxt76\\Zotero\\storage\\PNW2QMQT\\Dehghan et al. - 2015 - GMMCP tracker Globally optimal Generalized Maximu.pdf:application/pdf},
}


@article{hermans_defense_2017,
	title = {In {Defense} of the {Triplet} {Loss} for {Person} {Re}-{Identification}},
	url = {http://arxiv.org/abs/1703.07737},
	abstract = {In the past few years, the field of computer vision has gone through a revolution fueled mainly by the advent of large datasets and the adoption of deep convolutional neural networks for end-to-end learning. The person re-identification subfield is no exception to this. Unfortunately, a prevailing belief in the community seems to be that the triplet loss is inferior to using surrogate losses (classification, verification) followed by a separate metric learning step. We show that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms most other published methods by a large margin.},
	urldate = {2021-09-10},
	journal = {arXiv:1703.07737 [cs]},
	author = {Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},
	month = nov,
	year = {2017},
	note = {arXiv: 1703.07737},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\JC3XY67W\\Hermans et al. - 2017 - In Defense of the Triplet Loss for Person Re-Ident.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\AF7RQ8IL\\1703.html:text/html},
}

@article{khan_transformers_2021,
	title = {Transformers in {Vision}: {A} {Survey}},
	shorttitle = {Transformers in {Vision}},
	url = {http://arxiv.org/abs/2101.01169},
	abstract = {Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.},
	urldate = {2021-09-10},
	journal = {arXiv:2101.01169 [cs]},
	author = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
	month = sep,
	year = {2021},
	note = {arXiv: 2101.01169},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\lxt76\\Zotero\\storage\\QFP4NQDE\\Khan et al. - 2021 - Transformers in Vision A Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lxt76\\Zotero\\storage\\TBTWDZWK\\2101.html:text/html},
}

@InProceedings{10.1007/978-3-030-58452-8_13,
author="Carion, Nicolas
and Massa, Francisco
and Synnaeve, Gabriel
and Usunier, Nicolas
and Kirillov, Alexander
and Zagoruyko, Sergey",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="End-to-End Object Detection with Transformers",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="213--229",
abstract="We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.",
isbn="978-3-030-58452-8"
}

@inproceedings{nguyen_clusformer_2021,
	title = {Clusformer: {A} {Transformer} {Based} {Clustering} {Approach} to {Unsupervised} {Large}-{Scale} {Face} and {Visual} {Landmark} {Recognition}},
	shorttitle = {Clusformer},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Nguyen_Clusformer_A_Transformer_Based_Clustering_Approach_to_Unsupervised_Large-Scale_Face_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-09-12},
	author = {Nguyen, Xuan-Bac and Bui, Duc Toan and Duong, Chi Nhan and Bui, Tien D. and Luu, Khoa},
	year = {2021},
	pages = {10847--10856},
	file = {Full Text PDF:H\:\\Zotero\\storage\\SF5BB7FF\\Nguyen et al. - 2021 - Clusformer A Transformer Based Clustering Approac.pdf:application/pdf;Snapshot:H\:\\Zotero\\storage\\5S72EU48\\Nguyen_Clusformer_A_Transformer_Based_Clustering_Approach_to_Unsupervised_Large-Scale_Face_CVPR.html:text/html},
}
@article{liu_swin_2021,
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
	shorttitle = {Swin {Transformer}},
	url = {http://arxiv.org/abs/2103.14030},
	abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with {\textbackslash}textbf\{S\}hifted {\textbackslash}textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
	urldate = {2021-09-12},
	journal = {arXiv:2103.14030 [cs]},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	month = aug,
	year = {2021},
	note = {arXiv: 2103.14030},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:H\:\\Zotero\\storage\\CM4MNUQK\\Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf:application/pdf},
}
@article{LiuCCPZYH20,
  author    = {Wenhe Liu and
               Xiaojun Chang and
               Ling Chen and
               Dinh Phung and
               Xiaoqin Zhang and
               Yi Yang and
               Alexander G. Hauptmann},
  title     = {Pair-based Uncertainty and Diversity Promoting Early Active Learning
               for Person Re-identification},
  journal   = {{ACM} Trans. Intell. Syst. Technol.},
  volume    = {11},
  number    = {2},
  pages     = {21:1--21:15},
  year      = {2020}
}

@inproceedings{LiLCYPZ19,
  author    = {Zhihui Li and
               Wenhe Liu and
               Xiaojun Chang and
               Lina Yao and
               Mahesh Prakash and
               Huaxiang Zhang},
  editor    = {Jianxin Li and
               Sen Wang and
               Shaowen Qin and
               Xue Li and
               Shuliang Wang},
  title     = {Domain-Aware Unsupervised Cross-dataset Person Re-identification},
  booktitle = {Advanced Data Mining and Applications - 15th International Conference,
               {ADMA} 2019, Dalian, China, November 21-23, 2019, Proceedings},
  series    = {Lecture Notes in Computer Science},
  volume    = {11888},
  pages     = {406--420},
  publisher = {Springer},
  year      = {2019}
}

@article{ChengGCSHZ18,
  author    = {De Cheng and
               Yihong Gong and
               Xiaojun Chang and
               Weiwei Shi and
               Alexander G. Hauptmann and
               Nanning Zheng},
  title     = {Deep feature learning via structured graph Laplacian embedding for
               person re-identification},
  journal   = {Pattern Recognit.},
  volume    = {82},
  pages     = {94--104},
  year      = {2018}
}

@inproceedings{LiuCCY18,
  author    = {Wenhe Liu and
               Xiaojun Chang and
               Ling Chen and
               Yi Yang},
  title     = {Semi-Supervised Bayesian Attribute Learning for Person Re-Identification},
  booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence,
               (AAAI-18), the 30th innovative Applications of Artificial Intelligence
               (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in
               Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February
               2-7, 2018},
  pages     = {7162--7169},
  publisher = {{AAAI} Press},
  year      = {2018}
}

@inproceedings{ChengCLHGZ17,
  author    = {De Cheng and
               Xiaojun Chang and
               Li Liu and
               Alexander G. Hauptmann and
               Yihong Gong and
               Nanning Zheng},
  title     = {Discriminative Dictionary Learning With Ranking Metric Embedded for
               Person Re-Identification},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
               Artificial Intelligence, {IJCAI} 2017, Melbourne, Australia, August
               19-25, 2017},
  pages     = {964--970},
  year      = {2017}
}

@inproceedings{LiuC0Y17,
  author    = {Wenhe Liu and
               Xiaojun Chang and
               Ling Chen and
               Yi Yang},
  title     = {Early Active Learning with Pairwise Constraint for Person Re-identification},
  booktitle = {Machine Learning and Knowledge Discovery in Databases - European Conference,
               {ECML} {PKDD} 2017, Skopje, Macedonia, September 18-22, 2017, Proceedings,
               Part {I}},
  series    = {Lecture Notes in Computer Science},
  volume    = {10534},
  pages     = {103--118},
  publisher = {Springer},
  year      = {2017}
}