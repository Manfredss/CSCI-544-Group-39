\section{Related Works}
\paragraph{Multitraverse driving} 
A vehicle generally operates within the same geographical area, leading to multiple traversals of the same location. This repetition enriches the vehicle's memory of specific places, enhancing its capabilities in perception and localization~\cite{you2024better,you2023enhancing,xiong2023neural,yuan2024presight}. Regarding perception, the Hindsight framework~\cite{you2021hindsight} utilizes past LiDAR point clouds to learn memory features that are easy to query, thereby addressing the challenges of point sparsity and boosting 3D detection performance. Other studies have leveraged the persistence prior score~\cite{you2022learning,you2022unsupervised}, which quantifies the consistency of a single LiDAR point across multiple traversals, for self-training of detectors and domain adaptation. In localization, a significant number of works focus on either metric~\cite{maddern20171,toft2020long} or topological~\cite{lowry2015visual,li2023collaborative} localization, aiming to match a query image with a set of reference images collected from different traversals under varying seasonal or lighting conditions. Closely related to our work is~\cite{barnes2018driven}, which employs multiple traversals to map out ephemeral regions, enhancing monocular visual odometry in dense traffic conditions. However, this approach also depends on the consistency of LiDAR point clouds across traversals, remarking an unexplored gap in leveraging consensus in the 2D image space.

\paragraph{NeRF and 3DGS} NeRF has recently revolutionized novel-view synthesis and scene reconstruction with image or video input, boasting a wide range of applications in graphics, vision, and robotics. NeRF employs a volumetric representation and trains neural networks to model density and color. The success of NeRF has sparked a surge in follow-up methods aiming to enhance quality~\cite{barron2021mip,niemeyer2022regnerf,barron2023zip} and increase speed~\cite{xu2022point,fridovich2022plenoxels, muller2022instant}. The recent 3D Gaussian Splatting (3DGS)~\cite{kerbl20233d} uses an explicit Gaussian-based representation and splatting-based rasterization~\cite{zwicker2002ewa} to project anisotropic 3D Gaussians onto a 2D screen. It determines the pixel's color by performing depth sorting and $\alpha$-blending on the projected 2D Gaussians, thus avoiding the complex sampling strategy of ray marching and achieving real-time rendering. Subsequent works have applied 3DGS to scene editing~\cite{chen2023gaussianeditor}, dynamic scene modeling~\cite{luiten2023dynamic,yang2023gs4d}, sparse view reconstruction~\cite{fan2024instantsplat}, mesh reconstruction~\cite{guedon2023sugar}, semantic understanding~\cite{zhou2023feature,qin2023langsplat}, and indoor SLAM~\cite{Matsuki:Murai:etal:CVPR2024}.

\paragraph{NeRF and 3DGS for self-driving} 
Beyond their use in object-centric scenarios and bounded indoor environments, NeRF and 3DGS have also been explored in unbounded driving scenes~\cite{chen2023periodic,zhao2024tclc}. Several works address the implicit surface reconstruction of static scenes~\cite{rematas2022urban,wang2023neural,guo2023streetsurf}. A large body of research focuses on dynamic scene reconstruction from a single driving log. Most works use a compositional method and rely on bounding annotations/trained detectors to model dynamic objects~\cite{ost2021neural,xie2022s,yang2023unisim,tonderski2023neurad,zhou2023drivinggaussian,yan2024street,zhou2024hugs}. EmerNeRF~\cite{yang2023emernerf} is the first self-supervised method to learn 4D neural representations of driving scenes from LiDAR-camera recordings. It couples static, dynamic, and flow fields~\cite{muller2022instant} and leverages the flow field to aggregate multi-frame information to enhance the feature representation of dynamic objects. Another line of research investigates the scalability of the neural representation to model large-scale scenes~\cite{tancik2022block,xiangli2022bungeenerf,turki2022mega,turki2023suds,li2024nerfxl,lin2024vastgaussian}. Block-NeRF~\cite{tancik2022block} segments the scene into separately trained NeRF models, processing camera images from multiple drives, and applies a semantic segmentation model~\cite{cheng2020panoptic} to exclude common movable objects. SUDS takes the input of multitraverse driving logs, leveraging RGB images, LiDAR point clouds, DINO~\cite{caron2021emerging}, and 2D optical flow~\cite{teed2020raft} for dynamic scene decomposition. In this work, we create an environment map represented by 3DGS without requiring LiDARs, leveraging the multitraverse consensus for self-supervised object removal.


\paragraph{Scene decomposition} Traditional background subtraction approaches~\cite{piccardi2004background,brutzer2011evaluation} distinguish moving objects from static scenes by comparing successive video frames and identifying significant differences as foreground elements. Representative works include low-rank decomposition, which treats moving objects in the scene as pixel-wise sparse outliers~\cite{zhou2012moving,liu2015background}. These methods are typically used in surveillance applications and are limited to static cameras. Follow-up works~\cite{hayman2003statistical,sheikh2009background} investigate background subtraction for mobile robotics, yet suffering from low performance. NeRF has recently emerged as a popular scene representation and has been applied to the self-supervised dynamic-static decomposition of indoor scenes by modeling \textit{time-varying} and \textit{time-independent} components separately~\cite{yuan2021star,wu2022d}. EmerNeRF~\cite{yang2023emernerf} extends similar intuition to autonomous driving and obtains scene flow for free while achieving dynamic-static decomposition of a single traversal. Yet it still depends on the LiDAR inputs. In this study, we leverage signals of consensus and dissensus across multiple traversals to accomplish \textit{permanence-ephemerality} decomposition using only image inputs.

\paragraph{Vision foundation models} Inspired by the success
of scaling in NLP~\cite{brown2020language}, the field of computer vision intensively studies large-scale self-supervised pre-training with Transformers~\cite{vaswani2017attention}. Vision Transformers (ViTs)~\cite{dosovitskiy2020image}, pre-trained on extensive datasets, achieve excellent image recognition results. DINO~\cite{caron2021emerging} further amplifies feature representation capabilities by harnessing self-supervised learning alongside knowledge distillation. Meanwhile, scene layouts emerge within the attention maps, enabling unsupervised semantic understanding. DINOv2~\cite{oquab2023dinov2} scales up both the data and model size, achieving more robust visual features. Subsequent research focuses on examining noise artifacts to further enhance the performance of self-supervised descriptors, including training-free denoising of ViTs\cite{yang2024denoising} and retraining ViTs with registered tokens\cite{darcet2023vision}. In this work, we leverage denoised DINOv2 features~\cite{oquab2023dinov2,yang2024denoising} to facilitate consensus verification across multiple traversals in pixel space, \textit{as the high-dimensional features prove more resilient to changes in environmental appearance.}



