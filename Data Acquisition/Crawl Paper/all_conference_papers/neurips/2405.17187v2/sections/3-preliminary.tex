\section{3DGS: 3D Gaussian Splatting}
\label{sec:3dgs}
3D Gaussian Splatting~\cite{kerbl20233d} represents the 3D environment with a set of anisotropic 3D Gaussians, denoted by $\mathbf{G}=\{\mathbf{G}_i \mid i = 1, \ldots, N\}$, where $N$ is the total number of Gaussians. Each Gaussian, $\mathbf{G}_i$, is parameterized by its mean vector $\boldsymbol{\mu}_i \in \mathbb{R}^{3}$, indicating the position, and a covariance matrix $\boldsymbol{\Sigma}_i \in \mathbb{R}^{3 \times 3}$, defining its shape. 
To guarantee positive semi-definiteness, the covariance matrix $\boldsymbol{\Sigma}_i$ is further decomposed as $\boldsymbol{\Sigma}_i = \mathbf{R}_i\mathbf{S}_i\mathbf{R}_i^\top$, with $\mathbf{R}_i$ being an orthogonal rotation matrix and $\mathbf{S}_i$ a diagonal scaling matrix. These are stored compactly as a rotation quaternion $\mathbf{q}_i \in \mathbb{R}^4$ and a scaling factor $\mathbf{s}_i \in \mathbb{R}^3$. Each Gaussian also incorporates an opacity value $\alpha_i \in \mathbb{R}$ and a spherical harmonics coefficients $\boldsymbol{\beta}_i$. Therefore, the learnable parameters for the $i$-th Gaussian are $\mathbf{G}_i = [ \boldsymbol{\mu}_i, \mathbf{q}_i, \mathbf{s}_i, \alpha_i, \boldsymbol{\beta}_i ]$. Rendering from a viewpoint computes the color at pixel $\mathbf{p}$ (denoted by $\mathbf{c_p}$) via volumetric rendering, integrating $K$ ordered Gaussians $\{\mathbf{G}_k \mid k = 1, \ldots, K\}$ overlapping pixel $\mathbf{p}$, \textit{i.e.}, $\mathbf{c_p} = \sum_{k=1}^{K} \mathbf{c}_k \alpha_k \prod_{j=1}^{k-1} (1 - \alpha_j)$. Here, $\alpha_k$ is derived by evaluating a 2D Gaussian projection~\cite{zwicker2002ewa} from $\mathbf{G}_k$ onto pixel $\mathbf{p}$, multiplied by the Gaussian's learned opacity, and $\mathbf{c}_k$ is the color obtained by evaluating the spherical harmonics of $\mathbf{G}_k$. The Gaussians are sorted by their depth from the viewpoint. The overall objective is to minimize the rendering loss:
\begin{equation}
\mathcal{L} = \sum_t \mathcal{L}_{rgb} ( \mathbf{I}_t(\boldsymbol{\xi}_t;\mathbf{G}) , \mathbf{I}_t )
\end{equation}
where $\mathbf{I}_t(\boldsymbol{\xi}_t; \mathbf{G}) \in \mathbb{R}^{w\times h\times 3}$ is the RGB image indexed by $t$, with spatial dimensions $w\times h$ and rendered from the pose $\boldsymbol{\xi}_t \in \mathfrak{se}(3)$, given Gaussians $\mathbf{G}$. $\mathbf{I}_t \in \mathbb{R}^{w\times h\times 3}$ is the paired ground truth image. $\mathcal{L}_{rgb}$ is a loss function such as L1 loss. Initialized by COLMAP~\cite{schonberger2016structure}, all attributes of $\mathbf{G}$ are learned by executing this view reconstruction task. Meanwhile, adaptive densification and pruning strategies are proposed to improve the fitting of the 3D scene.


